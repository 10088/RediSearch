{
    "docs": [
        {
            "location": "/", 
            "text": "RediSearch - Redis Powered Search Engine\n\n\nRediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by \nRedis Labs\n. \n\n\n\n\nQuick Links:\n\n\n\n\nSource Code at GitHub\n.\n\n\nLatest Release: 1.0.0\n\n\nDocker Image: redislabs/redisearch\n\n\nQuick Start Guide\n\n\nMailing list / Forum\n\n\n\n\n\n\n\n\nSupported Platforms\n\n\nRediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.\n\n\ni386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently. \n\n\n\n\nOverview\n\n\nRedisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.\n\n\nThis also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional redis search approache\n\n\nClient Libraries\n\n\nOfficial and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee \nClients Page\n\n\nCluster Support and Commercial Version\n\n\nRediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial suppport for RediSearch. See the \nRedis Labs Website\n for more info and contact information. \n\n\nPrimary Features:\n\n\n\n\nFull-Text indexing of multiple fields in documents.\n\n\nIncremental indexing without performance loss.\n\n\nDocument ranking (provided manually by the user at index time).\n\n\nComplex boolean queries with AND, OR, NOT operators between sub-queries.\n\n\nOptional query clauses.\n\n\nPrefix based searches.\n\n\nField weights.\n\n\nAuto-complete suggestions (with fuzzy prefix suggestions)\n\n\nExact Phrase Search, Slop based search.\n\n\nStemming based query expansion in \nmany languages\n (using \nSnowball\n).\n\n\nSupport for custom functions for query expansion and scoring (see \nExtensions\n).\n\n\nLimiting searches to specific document fields (\nup to 32 TEXT fields supported\n).\n\n\nNumeric filters and ranges.\n\n\nGeo filtering using Redis' own Geo-commands. \n\n\nUnicode support (UTF-8 input required)\n\n\nRetrieve full document content or just ids\n\n\nDocument deletion and updating with index garbage collection.\n\n\nPartial document updates.", 
            "title": "Home"
        }, 
        {
            "location": "/#redisearch\"_\"-\"_\"redis\"_\"powered\"_\"search\"_\"engine", 
            "text": "RediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by  Redis Labs .    Quick Links:   Source Code at GitHub .  Latest Release: 1.0.0  Docker Image: redislabs/redisearch  Quick Start Guide  Mailing list / Forum     Supported Platforms  RediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.  i386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently.", 
            "title": "RediSearch - Redis Powered Search Engine"
        }, 
        {
            "location": "/#overview", 
            "text": "Redisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.  This also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional redis search approache", 
            "title": "Overview"
        }, 
        {
            "location": "/#client\"_\"libraries", 
            "text": "Official and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee  Clients Page", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/#cluster\"_\"support\"_\"and\"_\"commercial\"_\"version", 
            "text": "RediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial suppport for RediSearch. See the  Redis Labs Website  for more info and contact information.", 
            "title": "Cluster Support and Commercial Version"
        }, 
        {
            "location": "/#primary\"_\"features", 
            "text": "Full-Text indexing of multiple fields in documents.  Incremental indexing without performance loss.  Document ranking (provided manually by the user at index time).  Complex boolean queries with AND, OR, NOT operators between sub-queries.  Optional query clauses.  Prefix based searches.  Field weights.  Auto-complete suggestions (with fuzzy prefix suggestions)  Exact Phrase Search, Slop based search.  Stemming based query expansion in  many languages  (using  Snowball ).  Support for custom functions for query expansion and scoring (see  Extensions ).  Limiting searches to specific document fields ( up to 32 TEXT fields supported ).  Numeric filters and ranges.  Geo filtering using Redis' own Geo-commands.   Unicode support (UTF-8 input required)  Retrieve full document content or just ids  Document deletion and updating with index garbage collection.  Partial document updates.", 
            "title": "Primary Features:"
        }, 
        {
            "location": "/Quick_Start/", 
            "text": "Quick Start Guide for RediSearch:\n\n\nRunning with Docker\n\n\ndocker run -p \n6379\n:6379 redislabs/redisearch:latest\n\n\n\n\n\nBuilding and running from source:\n\n\ngit clone https://github.com/RedisLabsModules/RediSearch.git\n\ncd\n RediSearch/src\nmake all\n\n\n# Assuming you have a redis build from the unstable branch:\n\n/path/to/redis-server --loadmodule ./redisearch.so\n\n\n\n\n\nCreating an index with fields and weights (default weight is 1.0):\n\n\n127.0.0.1:6379\n FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK \n\n\n\n\n\nAdding documents to the index:\n\n\n127.0.0.1:6379\n FT.ADD myIdx doc1 1.0 FIELDS title \nhello world\n body \nlorem ipsum\n url \nhttp://redis.io\n \nOK\n\n\n\n\n\nSearching the index:\n\n\n127.0.0.1:6379\n FT.SEARCH myIdx \nhello world\n LIMIT 0 10\n1) (integer) 1\n2) \ndoc1\n\n3) 1) \ntitle\n\n   2) \nhello world\n\n   3) \nbody\n\n   4) \nlorem ipsum\n\n   5) \nurl\n\n   6) \nhttp://redis.io\n\n\n\n\n\n\n\n\nNOTE\n: Input is expected to be valid utf-8 or ascii. The engine cannot handle wide character unicode at the moment. \n\n\n\n\nDropping the index:\n\n\n127.0.0.1:6379\n FT.DROP myIdx\nOK\n\n\n\n\n\nAdding and getting Auto-complete suggestions:\n\n\n127.0.0.1:6379\n FT.SUGADD autocomplete \nhello world\n 100\nOK\n\n127.0.0.1:6379\n FT.SUGGET autocomplete \nhe\n\n1) \nhello world", 
            "title": "Quick Start"
        }, 
        {
            "location": "/Quick_Start/#quick\"_\"start\"_\"guide\"_\"for\"_\"redisearch", 
            "text": "", 
            "title": "Quick Start Guide for RediSearch:"
        }, 
        {
            "location": "/Quick_Start/#running\"_\"with\"_\"docker", 
            "text": "docker run -p  6379 :6379 redislabs/redisearch:latest", 
            "title": "Running with Docker"
        }, 
        {
            "location": "/Quick_Start/#building\"_\"and\"_\"running\"_\"from\"_\"source", 
            "text": "git clone https://github.com/RedisLabsModules/RediSearch.git cd  RediSearch/src\nmake all # Assuming you have a redis build from the unstable branch: \n/path/to/redis-server --loadmodule ./redisearch.so", 
            "title": "Building and running from source:"
        }, 
        {
            "location": "/Quick_Start/#creating\"_\"an\"_\"index\"_\"with\"_\"fields\"_\"and\"_\"weights\"_\"default\"_\"weight\"_\"is\"_\"10", 
            "text": "127.0.0.1:6379  FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK", 
            "title": "Creating an index with fields and weights (default weight is 1.0):"
        }, 
        {
            "location": "/Quick_Start/#adding\"_\"documents\"_\"to\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.ADD myIdx doc1 1.0 FIELDS title  hello world  body  lorem ipsum  url  http://redis.io  \nOK", 
            "title": "Adding documents to the index:"
        }, 
        {
            "location": "/Quick_Start/#searching\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.SEARCH myIdx  hello world  LIMIT 0 10\n1) (integer) 1\n2)  doc1 \n3) 1)  title \n   2)  hello world \n   3)  body \n   4)  lorem ipsum \n   5)  url \n   6)  http://redis.io    NOTE : Input is expected to be valid utf-8 or ascii. The engine cannot handle wide character unicode at the moment.", 
            "title": "Searching the index:"
        }, 
        {
            "location": "/Quick_Start/#dropping\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.DROP myIdx\nOK", 
            "title": "Dropping the index:"
        }, 
        {
            "location": "/Quick_Start/#adding\"_\"and\"_\"getting\"_\"auto-complete\"_\"suggestions", 
            "text": "127.0.0.1:6379  FT.SUGADD autocomplete  hello world  100\nOK\n\n127.0.0.1:6379  FT.SUGGET autocomplete  he \n1)  hello world", 
            "title": "Adding and getting Auto-complete suggestions:"
        }, 
        {
            "location": "/Commands/", 
            "text": "RediSeach Full Command Documentation\n\n\nFT.CREATE\n\n\nFormat:\n\n\n  FT.CREATE {index} \n    [NOOFFSETS] [NOFIELDS]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] | NUMERIC | GEO] [SORTABLE] [NOINDEX] ...\n\n\n\n\n\nDescription:\n\n\nCreates an index with the given spec. The index name will be used in all the key names\nso keep it short!\n\n\n\n\nNote on field number limits\nRediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields.\n\n\nOn 32 bit builds, at most 64 fields can be TEXT fields.\n\n\nNotice that the more fields you have, the larger your index will be, as each additional 8 fiedls require one extra byte per index record to encode.\n\n\nYou can always use the NOFIELDS option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields.\n\n\n\n\n\n\nParameters:\n\n\n\n\n\n\nindex\n: the index name to create. If it exists the old spec will be overwritten\n\n\n\n\n\n\nNOOFFSETS\n: If set, we do not store term offsets for documents (saves memory, does not allow exact searches or highlighting).\n  Implies \nNOHL\n\n\n\n\n\n\nNOHL\n: Conserves storage space and memory by disabling highlighting support. If set, we do\n  not store corresponding byte offsets for term positions. \nNOHL\n is also implied by \nNOOFFSETS\n.\n\n\n\n\n\n\nNOFIELDS\n: If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields.\n\n\n\n\n\n\nNOFREQS\n: If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.\n\n\n\n\n\n\nSTOPWORDS\n: If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}. \n\n\nIf not set, we take the default list of stopwords. \n\n\nIf \n{num}\n is set to 0, the index will not have stopwords.\n\n\n\n\n\n\nSCHEMA {field} {options...}\n: After the SCHEMA keyword we define the index fields. \nThey can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0.\n\n\nField Options\n\n\n\n\n\n\nSORTABLE\n\n\nNumeric or text field can have the optional SORTABLE argument that allows the user to later \nsort the results by the value of this field\n (this adds memory overhead so do not declare it on large text fields).\n\n\n\n\n\n\nNOSTEM\n\n\nText fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names.\n\n\n\n\n\n\nNOINDEX\n\n\nFields can have the \nNOINDEX\n option, which means they will not be indexed. \nThis is useful in conjunction with \nSORTABLE\n, to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index.\n\n\n\n\n\n\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns:\n\n\nOK or an error\n\n\n\n\nFT.ADD\n\n\nFormat:\n\n\nFT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE [PARTIAL]]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  FIELDS {field} {value} [{field} {value}...]\n\n\n\n\n\nDescription\n\n\nAdd a documet to the index.\n\n\nParameters:\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id that will be returned from searches. \n  Note that the same docId cannot be added twice to the same index\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nNOSAVE\n: If set to true, we will not save the actual document in the index and only index it.\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists. \n\n\n\n\n\n\nPARTIAL\n (only applicable with REPLACE): If set, you do not have to specify all fields for reindexing. Fields not given to the command will be loaded from the current version of the document. Also, if only non indexable fields, score or payload are set - we do not do a full reindexing of the document, and this will be a lot faster.\n\n\n\n\n\n\nFIELDS\n: Following the FIELDS specifier, we are looking for pairs of  \n{field} {value}\n to be indexed.\n  Each field will be scored based on the index spec given in FT.CREATE. \n  Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set \n\n\n\n\n\n\nPAYLOAD {payload}\n: Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.\n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\"chinese\"\n\n\n\n\n\n\n\n\nIf indexing a chinese language document, you must set the language to \nchinese\n\n  in order for the chinese characters to be tokenized properly.\n\n\nAdding Chinese Documents\n\n\nWhen adding Chinese-language documents, \nLANGUAGE chinese\n should be set in\norder for the indexer to properly tokenize the terms. If the default language\nis used then search terms will be extracted based on punctuation characters and\nwhitespace. The Chinese language tokenizer makes use of a segmentation algorithm\n(via \nFriso\n) which segments texts and\nchecks it against a predefined dictionary. See \nStemming\n for more\ninformation.\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\n\n\nFT.ADD with  REPLACE and PARTIAL\nBy default, FT.ADD does not allow updating the document, and will fail if it already exists in the index.\n\n\nHowever, updating the document is possible with the REPLACE and REPLACE PARTIAL options.\n\n\nREPLACE\n: On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document.\n\n\nREPLACE PARTIAL\n: When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage.\n\n\n\n\n\n\n\n\nFT.ADDHASH\n\n\nFormat\n\n\n \nFT\n.\nADDHASH\n \n{\nindex\n}\n \n{\ndocId\n}\n \n{\nscore\n}\n \n[\nLANGUAGE\n \nlanguage\n]\n \n[\nREPLACE\n]\n\n\n\n\n\n\nDescription\n\n\nAdd a documet to the index from an existing HASH key in Redis.\n\n\nParameters:\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id. This has to be an existing HASH key in redis that will hold the fields \n    the index needs.\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.\n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\n\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\n\n\nFT.INFO\n\n\nFormat\n\n\nFT.INFO {index} \n\n\n\n\n\nDescription\n\n\nReturn information and statistics on the index. Returned values include:\n\n\n\n\nNumber of documents.\n\n\nNumber of distinct terms.\n\n\nAverage bytes per record.\n\n\nSize and capacity of the index buffers.\n\n\n\n\nExample:\n\n\n127.0.0.1:6379\n ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n 5) num_docs\n 6) \n502694\n\n 7) num_terms\n 8) \n439158\n\n 9) num_records\n10) \n8098583\n\n11) inverted_sz_mb\n12) \n45.58\n13) inverted_cap_mb\n14) \n56.61\n15) inverted_cap_ovh\n16) \n0.19\n17) offset_vectors_sz_mb\n18) \n9.27\n19) skip_index_size_mb\n20) \n7.35\n21) score_index_size_mb\n22) \n30.8\n23) records_per_doc_avg\n24) \n16.1\n25) bytes_per_record_avg\n26) \n5.90\n27) offsets_per_term_avg\n28) \n1.20\n29) offset_bits_per_record_avg\n30) \n8.00\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nArray Response. A nested array of keys and values.\n\n\n\n\nFT.SEARCH\n\n\nFormat\n\n\nFT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]]\n  [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]\n\n\n\n\n\nDescription\n\n\nSearch the index with a textual query, returning either documents or just ids.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nquery\n: the text query to search. If it's more than a single word, put it in quotes.\n  See below for documentation on query syntax. \n\n\nNOCONTENT\n: If it appears after the query, we only return the document ids and not \n  the content. This is useful if rediseach is only an index on an external document collection\n\n\nRETURN {num} {field} ...\n: Use this keyword to limit which fields from the document are returned.\n  \nnum\n is the number of fields following the keyword. If \nnum\n is 0, it acts like \nNOCONTENT\n.\n\n\nSUMMARIZE ...\n: Use this option to return only the sections of the field which contain the matched text.\n  See \nHighlighting\n for more detailts\n\n\nHIGHLIGHT ...\n: Use this option to format occurrences of matched text. See \nHighligting\n for more\n  details\n\n\nLIMIT first num\n: If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10\n\n\nINFIELDS {num} {field} ...\n: If set, filter the results to ones appearing only in specific\n  fields of the document, like title or url. num is the number of specified field arguments\n\n\nINKEYS {num} {field} ...\n: If set, we limit the result to a given set of keys specified in the list. \n  the first argument must be the length of the list, and greater than zero.\n  Non existent keys are ignored - unless all the keys are non existent.\n\n\nSLOP {slop}\n: If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0)\n\n\nINORDER\n: If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them. \n\n\nFILTER numeric_field min max\n: If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be \n-inf\n, \n+inf\n and use \n(\n for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.\n\n\nGEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft\n: If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See \nGEORADIUS\n for more details. \n\n\nNOSTOPWORDS\n: If set, we do not filter stopwords from the query. \n\n\nWITHSCORES\n: If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances\n\n\nWITHSORTKEYS\n: Only relevant in conjunction with \nSORTBY\n. Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes.\n\n\nVERBATIM\n: if set, we do not try to use stemming for query expansion but search the query terms verbatim.\n\n\nLANGUAGE {language}\n: If set, we use a stemmer for the supplied langauge during search for query expansion.\n  If querying documents in Chinese, this should be set to \nchinese\n in order to\n  properly tokenize the query terms. \n  Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages.\n\n\nEXPANDER {expander}\n: If set, we will use a custom query expander instead of the stemmer. \nSee Extensions\n.\n\n\nSCORER {scorer}\n: If set, we will use a custom scoring function defined by the user. \nSee Extensions\n.\n\n\nPAYLOAD {payload}\n: Add an arbitrary, binary safe payload that will be exposed to custom scoring functions. \nSee Extensions\n.\n\n\nWITHPAYLOADS\n: If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if \nWITHSCORES\n was set, follow the scores.\n\n\nSORTBY {field} [ASC|DESC]\n: If specified, and field is a \nsortable field\n, the results are ordered by the value of this field. This applies to both text and numeric fields.\n\n\n\n\nComplexity\n\n\nO(n) for single word queries (though for popular words we save a cache of the top 50 results).\n\n\nComplexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.\n\n\nReturns\n\n\nArray reply,\n where the first element is the total number of results, and then pairs of document id, and a nested array of field/value. \n\n\nIf \nNOCONTENT\n was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.\n\n\n\n\nFT.EXPLAIN\n\n\nFormat\n\n\nFT.EXPLAIN {index} {query}\n\n\n\n\n\nDescription\n\n\nReturn the execution plan for a complex query\n\n\nExample:\n\n\n$ redis-cli --raw\n\n\n127\n.0.0.1:6379\n FT.EXPLAIN rd \n(foo bar)|(hello world) @date:[100 200]|@date:[500 +inf]\n\nINTERSECT \n{\n\n  UNION \n{\n\n    INTERSECT \n{\n\n      foo\n      bar\n    \n}\n\n    INTERSECT \n{\n\n      hello\n      world\n    \n}\n\n  \n}\n\n  UNION \n{\n\n    NUMERIC \n{\n100\n.000000 \n=\n x \n=\n \n200\n.000000\n}\n\n    NUMERIC \n{\n500\n.000000 \n=\n x \n=\n inf\n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nquery\n: The query string, as if sent to FT.SEARCH\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nString Response. A string representing the execution plan (see above example). \n\n\nNote\n: You should use \nredis-cli --raw\n to properly read line-breaks in the returned response.\n\n\n\n\nFT.DEL\n\n\nFormat\n\n\nFT.DEL {index} {doc_id}\n\n\n\n\n\nDescription\n\n\nDelete a document from the index. Returns 1 if the document was in the index, or 0 if not. \n\n\nAfter deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.\n\n\nNOTE\n: This does not actually delete the document from the index, just marks it as deleted. \nThus, deleting and re-inserting the same document over and over will inflate the index size with each re-insertion.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\ndoc_id\n: the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed.\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nInteger Reply: 1 if the document was deleted, 0 if not.\n\n\n\n\nFT.GET\n\n\nFormat\n\n\nFT.GET {index} {doc id}\n\n\n\n\n\nDescription\n\n\nReturns the full contents of a document.\n\n\nCurrently it is equivalent to HGETALL, but this is future proof and will allow us to change the internal representation of documents inside redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode.\n\n\nIf the document does not exist or is not a HASH object, we reutrn a NULL reply\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\ndocumentId\n: The id of the document as inserted to the index\n\n\n\n\nReturns\n\n\nArray Reply: Key-value pairs of field names and values of the document\n\n\n\n\nFT.MGET\n\n\nFormat\n\n\nFT.GET {index} {docId} ...\n\n\n\n\n\nDescription\n\n\nReturns the full contents of multiple documents. \nCurrently it is equivalent to calling multiple HGETALL commands, although faster. \nThis command is also future proof, and will allow us to change the internal representation of documents inside redis in the future. \nIn addition, it allows simpler implementation of fetching documents in clustered mode.\n\n\nWe return an array with exactly the same number of elements as the number of keys sent to the command. \n\n\nEach element in turn is an array of key-value pairs representing the document. \n\n\nIf a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\ndocumentIds\n: The ids of the requested documents as inserted to the index\n\n\n\n\nReturns\n\n\nArray Reply: An array with exactly the same number of elements as the number of keys sent to the command.  Each element in it is either an array representing the document, or Null if it was not found.\n\n\n\n\nFT.DROP\n\n\nFormat\n\n\nFT.DROP {index}\n\n\n\n\n\nDescription\n\n\nDeletes all the keys associated with the index. \n\n\nIf no other data is on the redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nReturns\n\n\nStatus Reply: OK on success.\n\n\n\n\nFT.TAGVALS\n\n\nFormat\n\n\nFT.TAGVALS {index} {field_name}\n\n\n\n\n\nDescription\n\n\nReturn the distinct tags indexed in a \nTag field\n. \n\n\nThis is useful if your tag field indexes things like cities, categories, etc.\n\n\n\n\nLimitations\n\n\nThere is no paging or sorting, the tags are not alphabetically sorted. \n\n\nThis command only operates on \nTag fields\n.  \n\n\nThe strings return lower-cased and stripped of whitespaces, but otherwise unchanged.\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nfiled_name\n: The name of a Tag file defined in the schema.\n\n\n\n\nReturns\n\n\nArray Reply: All the distinct tags in the tag index.\n\n\nComplexity\n\n\nO(n), n being the cardinality of the tag field.\n\n\n\n\nFT.SUGADD\n\n\nFormat\n\n\nFT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]\n\n\n\n\n\nDescription\n\n\nAdd a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestino dictionaries to the user.\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the suggestion string we index\n\n\nscore\n: a floating point number of the suggestion string's weight\n\n\nINCR\n: if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time\n\n\nPAYLOAD {payload}\n: If set, we save an extra payload with the suggestion, that can be fetched by adding the \nWITHPAYLOADS\n argument to \nFT.SUGGET\n.\n\n\n\n\nReturns:\n\n\nInteger Reply: the current size of the suggestion dictionary.\n\n\n\n\nFT.SUGGET\n\n\nFormat\n\n\nFT\n.\nSUGGET\n \n{\nkey\n}\n \n{\nprefix\n}\n \n[\nFUZZY\n]\n \n[\nWITHPAYLOADS\n]\n \n[\nMAX\n \nnum\n]\n\n\n\n\n\n\nDescription\n\n\nGet completion suggestions for a prefix\n\n\nParameters:\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nprefix\n: the prefix to complete on\n\n\nFUZZY\n: if set,we do a fuzzy prefix search, including prefixes at levenshtein distance of 1 from the prefix sent\n\n\nMAX num\n: If set, we limit the results to a maximum of \nnum\n. (\nNote\n: The default is 5, and the number cannot be greater than 10).\n\n\nWITHSCORES\n: If set, we also return the score of each suggestion. this can be used to merge results from multiple instances\n\n\nWITHPAYLOADS\n: If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply.\n\n\n\n\nReturns:\n\n\nArray Reply: a list of the top suggestions matching the prefix, optionally with score after each entry\n\n\n\n\nFT.SUGDEL\n\n\nFormat\n\n\nFT.SUGDEL {key} {string}\n\n\n\n\n\nDescription\n\n\nDelete a string from a suggestion index. \n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the string to delete\n\n\n\n\nReturns:\n\n\nInteger Reply: 1 if the string was found and deleted, 0 otherwise.\n\n\n\n\nFT.SUGLEN\n\n\nFormat\n\n\nFT.SUGLEN {key}\n\n\n\n\n\nDescription\n\n\nGet the size of an autoc-complete suggestion dictionary\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\n\n\nReturns:\n\n\nInteger Reply: the current size of the suggestion dictionary.\n\n\n\n\nFT.OPTIMIZE \n DEPRECATED \n\n\nFormat\n\n\nFT.OPTIMIZE {index}\n\n\n\n\n\nDescription\n\n\nThis command is deprecated. Index optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command, and remove it if they haven't already.", 
            "title": "Command Reference"
        }, 
        {
            "location": "/Commands/#rediseach\"_\"full\"_\"command\"_\"documentation", 
            "text": "", 
            "title": "RediSeach Full Command Documentation"
        }, 
        {
            "location": "/Commands/#ftcreate", 
            "text": "", 
            "title": "FT.CREATE"
        }, 
        {
            "location": "/Commands/#format", 
            "text": "FT.CREATE {index} \n    [NOOFFSETS] [NOFIELDS]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] | NUMERIC | GEO] [SORTABLE] [NOINDEX] ...", 
            "title": "Format:"
        }, 
        {
            "location": "/Commands/#description", 
            "text": "Creates an index with the given spec. The index name will be used in all the key names\nso keep it short!   Note on field number limits RediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields.  On 32 bit builds, at most 64 fields can be TEXT fields.  Notice that the more fields you have, the larger your index will be, as each additional 8 fiedls require one extra byte per index record to encode.  You can always use the NOFIELDS option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields.", 
            "title": "Description:"
        }, 
        {
            "location": "/Commands/#parameters", 
            "text": "index : the index name to create. If it exists the old spec will be overwritten    NOOFFSETS : If set, we do not store term offsets for documents (saves memory, does not allow exact searches or highlighting).\n  Implies  NOHL    NOHL : Conserves storage space and memory by disabling highlighting support. If set, we do\n  not store corresponding byte offsets for term positions.  NOHL  is also implied by  NOOFFSETS .    NOFIELDS : If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields.    NOFREQS : If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.    STOPWORDS : If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}.   If not set, we take the default list of stopwords.   If  {num}  is set to 0, the index will not have stopwords.    SCHEMA {field} {options...} : After the SCHEMA keyword we define the index fields. \nThey can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#field\"_\"options", 
            "text": "SORTABLE  Numeric or text field can have the optional SORTABLE argument that allows the user to later  sort the results by the value of this field  (this adds memory overhead so do not declare it on large text fields).    NOSTEM  Text fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names.    NOINDEX  Fields can have the  NOINDEX  option, which means they will not be indexed. \nThis is useful in conjunction with  SORTABLE , to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index.", 
            "title": "Field Options"
        }, 
        {
            "location": "/Commands/#complexity", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns", 
            "text": "OK or an error", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftadd", 
            "text": "", 
            "title": "FT.ADD"
        }, 
        {
            "location": "/Commands/#format_1", 
            "text": "FT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE [PARTIAL]]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  FIELDS {field} {value} [{field} {value}...]", 
            "title": "Format:"
        }, 
        {
            "location": "/Commands/#description_1", 
            "text": "Add a documet to the index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_1", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id that will be returned from searches. \n  Note that the same docId cannot be added twice to the same index    score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    NOSAVE : If set to true, we will not save the actual document in the index and only index it.    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.     PARTIAL  (only applicable with REPLACE): If set, you do not have to specify all fields for reindexing. Fields not given to the command will be loaded from the current version of the document. Also, if only non indexable fields, score or payload are set - we do not do a full reindexing of the document, and this will be a lot faster.    FIELDS : Following the FIELDS specifier, we are looking for pairs of   {field} {value}  to be indexed.\n  Each field will be scored based on the index spec given in FT.CREATE. \n  Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set     PAYLOAD {payload} : Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.    LANGUAGE language : If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:   \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\"chinese\"     If indexing a chinese language document, you must set the language to  chinese \n  in order for the chinese characters to be tokenized properly.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#adding\"_\"chinese\"_\"documents", 
            "text": "When adding Chinese-language documents,  LANGUAGE chinese  should be set in\norder for the indexer to properly tokenize the terms. If the default language\nis used then search terms will be extracted based on punctuation characters and\nwhitespace. The Chinese language tokenizer makes use of a segmentation algorithm\n(via  Friso ) which segments texts and\nchecks it against a predefined dictionary. See  Stemming  for more\ninformation.", 
            "title": "Adding Chinese Documents"
        }, 
        {
            "location": "/Commands/#complexity_1", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_1", 
            "text": "OK on success, or an error if something went wrong.   FT.ADD with  REPLACE and PARTIAL By default, FT.ADD does not allow updating the document, and will fail if it already exists in the index.  However, updating the document is possible with the REPLACE and REPLACE PARTIAL options.  REPLACE : On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document.  REPLACE PARTIAL : When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftaddhash", 
            "text": "", 
            "title": "FT.ADDHASH"
        }, 
        {
            "location": "/Commands/#format_2", 
            "text": "FT . ADDHASH   { index }   { docId }   { score }   [ LANGUAGE   language ]   [ REPLACE ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_2", 
            "text": "Add a documet to the index from an existing HASH key in Redis.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_2", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id. This has to be an existing HASH key in redis that will hold the fields \n    the index needs.    score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.    LANGUAGE language : If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:     \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#complexity_2", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_2", 
            "text": "OK on success, or an error if something went wrong.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftinfo", 
            "text": "", 
            "title": "FT.INFO"
        }, 
        {
            "location": "/Commands/#format_3", 
            "text": "FT.INFO {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_3", 
            "text": "Return information and statistics on the index. Returned values include:   Number of documents.  Number of distinct terms.  Average bytes per record.  Size and capacity of the index buffers.   Example:  127.0.0.1:6379  ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n 5) num_docs\n 6)  502694 \n 7) num_terms\n 8)  439158 \n 9) num_records\n10)  8098583 \n11) inverted_sz_mb\n12)  45.58\n13) inverted_cap_mb\n14)  56.61\n15) inverted_cap_ovh\n16)  0.19\n17) offset_vectors_sz_mb\n18)  9.27\n19) skip_index_size_mb\n20)  7.35\n21) score_index_size_mb\n22)  30.8\n23) records_per_doc_avg\n24)  16.1\n25) bytes_per_record_avg\n26)  5.90\n27) offsets_per_term_avg\n28)  1.20\n29) offset_bits_per_record_avg\n30)  8.00", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_3", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_3", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_3", 
            "text": "Array Response. A nested array of keys and values.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsearch", 
            "text": "", 
            "title": "FT.SEARCH"
        }, 
        {
            "location": "/Commands/#format_4", 
            "text": "FT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]]\n  [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_4", 
            "text": "Search the index with a textual query, returning either documents or just ids.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_4", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  query : the text query to search. If it's more than a single word, put it in quotes.\n  See below for documentation on query syntax.   NOCONTENT : If it appears after the query, we only return the document ids and not \n  the content. This is useful if rediseach is only an index on an external document collection  RETURN {num} {field} ... : Use this keyword to limit which fields from the document are returned.\n   num  is the number of fields following the keyword. If  num  is 0, it acts like  NOCONTENT .  SUMMARIZE ... : Use this option to return only the sections of the field which contain the matched text.\n  See  Highlighting  for more detailts  HIGHLIGHT ... : Use this option to format occurrences of matched text. See  Highligting  for more\n  details  LIMIT first num : If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10  INFIELDS {num} {field} ... : If set, filter the results to ones appearing only in specific\n  fields of the document, like title or url. num is the number of specified field arguments  INKEYS {num} {field} ... : If set, we limit the result to a given set of keys specified in the list. \n  the first argument must be the length of the list, and greater than zero.\n  Non existent keys are ignored - unless all the keys are non existent.  SLOP {slop} : If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0)  INORDER : If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them.   FILTER numeric_field min max : If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be  -inf ,  +inf  and use  (  for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.  GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft : If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See  GEORADIUS  for more details.   NOSTOPWORDS : If set, we do not filter stopwords from the query.   WITHSCORES : If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances  WITHSORTKEYS : Only relevant in conjunction with  SORTBY . Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes.  VERBATIM : if set, we do not try to use stemming for query expansion but search the query terms verbatim.  LANGUAGE {language} : If set, we use a stemmer for the supplied langauge during search for query expansion.\n  If querying documents in Chinese, this should be set to  chinese  in order to\n  properly tokenize the query terms. \n  Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages.  EXPANDER {expander} : If set, we will use a custom query expander instead of the stemmer.  See Extensions .  SCORER {scorer} : If set, we will use a custom scoring function defined by the user.  See Extensions .  PAYLOAD {payload} : Add an arbitrary, binary safe payload that will be exposed to custom scoring functions.  See Extensions .  WITHPAYLOADS : If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if  WITHSCORES  was set, follow the scores.  SORTBY {field} [ASC|DESC] : If specified, and field is a  sortable field , the results are ordered by the value of this field. This applies to both text and numeric fields.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_4", 
            "text": "O(n) for single word queries (though for popular words we save a cache of the top 50 results).  Complexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_4", 
            "text": "Array reply,  where the first element is the total number of results, and then pairs of document id, and a nested array of field/value.   If  NOCONTENT  was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftexplain", 
            "text": "", 
            "title": "FT.EXPLAIN"
        }, 
        {
            "location": "/Commands/#format_5", 
            "text": "FT.EXPLAIN {index} {query}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_5", 
            "text": "Return the execution plan for a complex query  Example:  $ redis-cli --raw 127 .0.0.1:6379  FT.EXPLAIN rd  (foo bar)|(hello world) @date:[100 200]|@date:[500 +inf] \nINTERSECT  { \n  UNION  { \n    INTERSECT  { \n      foo\n      bar\n     } \n    INTERSECT  { \n      hello\n      world\n     } \n   } \n  UNION  { \n    NUMERIC  { 100 .000000  =  x  =   200 .000000 } \n    NUMERIC  { 500 .000000  =  x  =  inf } \n   }  }", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_5", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  query : The query string, as if sent to FT.SEARCH", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_5", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_5", 
            "text": "String Response. A string representing the execution plan (see above example).   Note : You should use  redis-cli --raw  to properly read line-breaks in the returned response.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdel", 
            "text": "", 
            "title": "FT.DEL"
        }, 
        {
            "location": "/Commands/#format_6", 
            "text": "FT.DEL {index} {doc_id}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_6", 
            "text": "Delete a document from the index. Returns 1 if the document was in the index, or 0 if not.   After deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.  NOTE : This does not actually delete the document from the index, just marks it as deleted. \nThus, deleting and re-inserting the same document over and over will inflate the index size with each re-insertion.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_6", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  doc_id : the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_6", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_6", 
            "text": "Integer Reply: 1 if the document was deleted, 0 if not.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftget", 
            "text": "", 
            "title": "FT.GET"
        }, 
        {
            "location": "/Commands/#format_7", 
            "text": "FT.GET {index} {doc id}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_7", 
            "text": "Returns the full contents of a document.  Currently it is equivalent to HGETALL, but this is future proof and will allow us to change the internal representation of documents inside redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode.  If the document does not exist or is not a HASH object, we reutrn a NULL reply", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_7", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  documentId : The id of the document as inserted to the index", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_7", 
            "text": "Array Reply: Key-value pairs of field names and values of the document", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftmget", 
            "text": "", 
            "title": "FT.MGET"
        }, 
        {
            "location": "/Commands/#format_8", 
            "text": "FT.GET {index} {docId} ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_8", 
            "text": "Returns the full contents of multiple documents. \nCurrently it is equivalent to calling multiple HGETALL commands, although faster. \nThis command is also future proof, and will allow us to change the internal representation of documents inside redis in the future. \nIn addition, it allows simpler implementation of fetching documents in clustered mode.  We return an array with exactly the same number of elements as the number of keys sent to the command.   Each element in turn is an array of key-value pairs representing the document.   If a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_8", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  documentIds : The ids of the requested documents as inserted to the index", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_8", 
            "text": "Array Reply: An array with exactly the same number of elements as the number of keys sent to the command.  Each element in it is either an array representing the document, or Null if it was not found.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdrop", 
            "text": "", 
            "title": "FT.DROP"
        }, 
        {
            "location": "/Commands/#format_9", 
            "text": "FT.DROP {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_9", 
            "text": "Deletes all the keys associated with the index.   If no other data is on the redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_9", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_9", 
            "text": "Status Reply: OK on success.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#fttagvals", 
            "text": "", 
            "title": "FT.TAGVALS"
        }, 
        {
            "location": "/Commands/#format_10", 
            "text": "FT.TAGVALS {index} {field_name}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_10", 
            "text": "Return the distinct tags indexed in a  Tag field .   This is useful if your tag field indexes things like cities, categories, etc.   Limitations  There is no paging or sorting, the tags are not alphabetically sorted.   This command only operates on  Tag fields .    The strings return lower-cased and stripped of whitespaces, but otherwise unchanged.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_10", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  filed_name : The name of a Tag file defined in the schema.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_10", 
            "text": "Array Reply: All the distinct tags in the tag index.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#complexity_7", 
            "text": "O(n), n being the cardinality of the tag field.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#ftsugadd", 
            "text": "", 
            "title": "FT.SUGADD"
        }, 
        {
            "location": "/Commands/#format_11", 
            "text": "FT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_11", 
            "text": "Add a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestino dictionaries to the user.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_11", 
            "text": "key : the suggestion dictionary key.  string : the suggestion string we index  score : a floating point number of the suggestion string's weight  INCR : if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time  PAYLOAD {payload} : If set, we save an extra payload with the suggestion, that can be fetched by adding the  WITHPAYLOADS  argument to  FT.SUGGET .", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_11", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsugget", 
            "text": "", 
            "title": "FT.SUGGET"
        }, 
        {
            "location": "/Commands/#format_12", 
            "text": "FT . SUGGET   { key }   { prefix }   [ FUZZY ]   [ WITHPAYLOADS ]   [ MAX   num ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_12", 
            "text": "Get completion suggestions for a prefix", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_12", 
            "text": "key : the suggestion dictionary key.  prefix : the prefix to complete on  FUZZY : if set,we do a fuzzy prefix search, including prefixes at levenshtein distance of 1 from the prefix sent  MAX num : If set, we limit the results to a maximum of  num . ( Note : The default is 5, and the number cannot be greater than 10).  WITHSCORES : If set, we also return the score of each suggestion. this can be used to merge results from multiple instances  WITHPAYLOADS : If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#returns_12", 
            "text": "Array Reply: a list of the top suggestions matching the prefix, optionally with score after each entry", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsugdel", 
            "text": "", 
            "title": "FT.SUGDEL"
        }, 
        {
            "location": "/Commands/#format_13", 
            "text": "FT.SUGDEL {key} {string}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_13", 
            "text": "Delete a string from a suggestion index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_13", 
            "text": "key : the suggestion dictionary key.  string : the string to delete", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_13", 
            "text": "Integer Reply: 1 if the string was found and deleted, 0 otherwise.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsuglen", 
            "text": "Format  FT.SUGLEN {key}", 
            "title": "FT.SUGLEN"
        }, 
        {
            "location": "/Commands/#description_14", 
            "text": "Get the size of an autoc-complete suggestion dictionary", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_14", 
            "text": "key : the suggestion dictionary key.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_14", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftoptimize\"_\"deprecated", 
            "text": "Format  FT.OPTIMIZE {index}  Description  This command is deprecated. Index optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command, and remove it if they haven't already.", 
            "title": "FT.OPTIMIZE  DEPRECATED"
        }, 
        {
            "location": "/Configuring/", 
            "text": "Run-Time Configuration\n\n\nRediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added. \n\n\n\n\nPassing Configuration Options\n\n\nIn general, passing configuration options is done by appending arguments after the \n--loadmodule\n argument in command line, \nloadmodule\n configurtion directive in a redis config file, or \nMODULE LOAD\n when loading modules in command line. For example:\n\n\nIn redis.conf:\n\n\nloadmodule redisearch.so OPT1 OPT2\n\n\nIn redis-cli:\n\n\n127.0.0.6379\n MODULE load redisearch.so OPT1 OPT2\n\n\nIn command-line:\n\n\n$ redis-server --loadmodule ./redisearch.so OPT1 OPT2\n\n\n\n\nRediSearch Configuration Options\n\n\nTIMEOUT\n\n\nThe maximum amount of time \nin Millisecods\n that a search query is allowed to run. If this time is exceeded, we return the top results accumulated so far. \nThe defalt is 500ms. \n\n\nNOTE\n: This works only in concurrent mode, so enabling SAFEMODE disables ths option.\n\n\nDefault:\n\n\n500\n\n\nExample:\n\n\n$ redis-server --loadmodule ./redisearch.so TIMEOUT \n100\n\n\n\n\n\n\nSAFEMODE\n\n\nIf present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.\n\n\nThis is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily incosistent results (i.e. documents that were valid during the the invokation of the query are not returned because they were deleted durin query processing).\n\n\nDefault:\n\n\nOff (not present)\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so SAFEMODE\n\n\n\n\n\n\n\nEXTLOAD {file_name}\n\n\nIf present, we try to load a redisearch extension dynamic library from the specified file path. See \nExtensions\n for details.\n\n\nDefault:\n\n\nNone\n\n\nExample:\n\n\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so\n\n\n\n\n\n\n\nNOGC\n\n\nIf set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.\n\n\nDefault:\n\n\nNot set\n\n\nExample:\n\n\n$ redis-server --loadmodule ./redisearch.so NOGC\n\n\n\n\n\n\n\nMINPREFIX\n\n\nThe minimum number of characters we allow for prefix queries (e.g. \nhel*\n). Setting it to 1 can hurt performance.\n\n\nDefault:\n\n\n2\n\n\nExample:\n\n\n$ redis-server --loadmodule ./redisearch.so MINPREFIX \n3\n\n\n\n\n\n\n\n\nMAXEXPANSIONS\n\n\nThe maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues.\n\n\nDefault:\n\n\n200\n\n\nExample:\n\n\n$ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS \n1000", 
            "title": "Configuration"
        }, 
        {
            "location": "/Configuring/#run-time\"_\"configuration", 
            "text": "RediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added.    Passing Configuration Options  In general, passing configuration options is done by appending arguments after the  --loadmodule  argument in command line,  loadmodule  configurtion directive in a redis config file, or  MODULE LOAD  when loading modules in command line. For example:  In redis.conf:  loadmodule redisearch.so OPT1 OPT2  In redis-cli:  127.0.0.6379  MODULE load redisearch.so OPT1 OPT2  In command-line:  $ redis-server --loadmodule ./redisearch.so OPT1 OPT2", 
            "title": "Run-Time Configuration"
        }, 
        {
            "location": "/Configuring/#redisearch\"_\"configuration\"_\"options", 
            "text": "", 
            "title": "RediSearch Configuration Options"
        }, 
        {
            "location": "/Configuring/#timeout", 
            "text": "The maximum amount of time  in Millisecods  that a search query is allowed to run. If this time is exceeded, we return the top results accumulated so far. \nThe defalt is 500ms.   NOTE : This works only in concurrent mode, so enabling SAFEMODE disables ths option.", 
            "title": "TIMEOUT"
        }, 
        {
            "location": "/Configuring/#default", 
            "text": "500", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example", 
            "text": "$ redis-server --loadmodule ./redisearch.so TIMEOUT  100", 
            "title": "Example:"
        }, 
        {
            "location": "/Configuring/#safemode", 
            "text": "If present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.  This is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily incosistent results (i.e. documents that were valid during the the invokation of the query are not returned because they were deleted durin query processing).", 
            "title": "SAFEMODE"
        }, 
        {
            "location": "/Configuring/#default_1", 
            "text": "Off (not present)", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_1", 
            "text": "$ redis-server --loadmodule ./redisearch.so SAFEMODE", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#extload\"_\"file\"_\"name", 
            "text": "If present, we try to load a redisearch extension dynamic library from the specified file path. See  Extensions  for details.", 
            "title": "EXTLOAD {file_name}"
        }, 
        {
            "location": "/Configuring/#default_2", 
            "text": "None", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_2", 
            "text": "$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so", 
            "title": "Example:"
        }, 
        {
            "location": "/Configuring/#nogc", 
            "text": "If set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.", 
            "title": "NOGC"
        }, 
        {
            "location": "/Configuring/#default_3", 
            "text": "Not set", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_3", 
            "text": "$ redis-server --loadmodule ./redisearch.so NOGC", 
            "title": "Example:"
        }, 
        {
            "location": "/Configuring/#minprefix", 
            "text": "The minimum number of characters we allow for prefix queries (e.g.  hel* ). Setting it to 1 can hurt performance.", 
            "title": "MINPREFIX"
        }, 
        {
            "location": "/Configuring/#default_4", 
            "text": "2", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_4", 
            "text": "$ redis-server --loadmodule ./redisearch.so MINPREFIX  3", 
            "title": "Example:"
        }, 
        {
            "location": "/Configuring/#maxexpansions", 
            "text": "The maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues.", 
            "title": "MAXEXPANSIONS"
        }, 
        {
            "location": "/Configuring/#default_5", 
            "text": "200", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_5", 
            "text": "$ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS  1000", 
            "title": "Example:"
        }, 
        {
            "location": "/Query_Syntax/", 
            "text": "Search Query Syntax:\n\n\nWe support a simple syntax for complex queries with the following rules:\n\n\n\n\nMulti-word phrases simply a list of tokens, e.g. \nfoo bar baz\n, and imply intersection (AND) of the terms.\n\n\nExact phrases are wrapped in quotes, e.g \n\"hello world\"\n.\n\n\nOR Unions (i.e \nword1 OR word2\n), are expressed with a pipe (\n|\n), e.g. \nhello|hallo|shalom|hola\n.\n\n\nNOT negation (i.e. \nword1 NOT word2\n) of expressions or sub-queries. e.g. \nhello -world\n. As of version 0.19.3, purely negative queries (i.e. \n-foo\n or \n-@title:(foo|bar)\n) are supported. \n\n\nPrefix matches (all terms starting with a prefix) are expressed with a \n*\n following a 3-letter or longer prefix.\n\n\nSelection of specific fields using the syntax \n@field:hello world\n.\n\n\nNumeric Range matches on numeric fields with the syntax \n@field:[{min} {max}]\n.\n\n\nGeo radius matches on geo fields with the syntax \n@field:[{lon} {lat} {radius} {m|km|mi|ft}]\n\n\nTag field filters with the syntax \n@field:{tag | tag | ...}\n. See the full documentation on tag fields.\n\n\nOptional terms or clauses: \nfoo ~bar\n means bar is optional but documents with bar in them will rank higher. \n\n\nAn expression in a query can be wrapped in parentheses to resolve disambiguity, e.g. \n(hello|hella) (world|werld)\n.\n\n\nCombinations of the above can be used together, e.g \nhello (world|foo) \"bar baz\" bbbb\n\n\n\n\nPure Negative Queries\n\n\nAs of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g. \n-hello\n or \n-(@title:foo|bar)\n. The results will be all the documents \nNOT\n containing the query terms.\n\n\nWarning\n: Any complex expression can be negated this way, however caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.\n\n\nField modifiers\n\n\nAs of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword. \n\n\nPer query expression or sub expression, it is possible to specify which fields it matches, by prepending the experssion with the \n@\n symbol, the field name and a \n:\n (colon) symbol. \n\n\nIf a field modifier precedes multiple words, they are considered to be a phrase with the same modifier. \n\n\nIf a field modifier preceds an expression in parentheses, it applies only to the expression inside the parentheses.\n\n\nMultiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:\n\n\nFT.SEARCH cars \n@country:korea @engine:(diesel|hybrid) @class:suv\n\n\n\n\n\n\nMultiple modifiers can be applied to the same term or grouped terms. e.g.:\n\n\nFT.SEARCH idx \n@title|body:(hello world) @url|image:mydomain\n\n\n\n\n\n\nThis will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.\n\n\nNumeric Filters in Query\n\n\nIf a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the redis request, or filter with it by specifying filtering rules in the query. The syntax is \n@field:[{min} {max}]\n - e.g. \n@price:[100 200]\n.\n\n\nA few notes on numeric predicates:\n\n\n\n\n\n\nIt is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.\n\n\n\n\n\n\nIt is possible to interesect or union multiple numeric filters in the same query, be it for the same field or different ones.\n\n\n\n\n\n\n-inf\n, \ninf\n and \n+inf\n are acceptable numbers in range. Thus greater-than 100 is expressed as \n[(100 inf]\n.\n\n\n\n\n\n\nNumeric filters are inclusive. Exclusive min or max are expressed with \n(\n prepended to the number, e.g. \n[(100 (200]\n.\n\n\n\n\n\n\nIt is possible to negate a numeric filter by prepending a \n-\n sign to the filter, e.g. returnig a result where price differs from 100 is expressed as: \n@title:foo -@price:[100 100]\n. \n\n\n\n\n\n\nTag Filters\n\n\nRediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax: \n\n\n@field:{ tag | tag | ...}\n\ne.g.\n\n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\nTags can have multiple words, or include other punctuation marks other than the field's separator (\n,\n by default). Punctuation marks in tags should be escaped with a backslash (\n\\\n). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.\n\n\nNotice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing \nall\n tags, you should repeat the tag filter several times, e.g.:\n\n\n# This will return all documents containing all three cities as tags:\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n# This will return all documents containing either city:\n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\nTag clauses can be combined into any sub clause, used as negative expressions, optional expressions, etc.\n\n\nGeo Filters in Query\n\n\nAs of version 0.21, it is possible to add geo radius queries directly into the query language  with the syntax \n@field:[{lon} {lat} {radius} {m|km|mi|ft}]\n. This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own GEORADIUS command for more details (internall we use GEORADIUS for that).\n\n\nRadius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as: \nchinese restaurant @location:[-122.41 37.77 5 km]\n.\n\n\nPrefix Matching\n\n\nOn index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending \n*\n to a prefix token. For example:\n\n\nhel* world\n\n\n\n\n\nWill be expanded to cover \n(hello|help|helm|...) world\n. \n\n\nA few notes on prefix searches:\n\n\n\n\n\n\nAs prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffxies.\n\n\n\n\n\n\nAs a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:\n\n\n\n\n\n\nPrefixes are limited to 3 letters or more. \n\n\n\n\n\n\nExpansion is limited to 200 terms or less. \n\n\n\n\n\n\nPrefix matching fully supports unicode and is case insensitive.\n\n\n\n\n\n\nCurrently there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap. \n\n\n\n\n\n\nA Few Query Examples\n\n\n\n\n\n\nSimple phrase query - hello AND world\n\n\nhello world\n\n\n\n\n\n\n\n\n\nExact phrase query - \nhello\n FOLLOWED BY \nworld\n\n\nhello world\n\n\n\n\n\n\n\n\n\n\nUnion: documents containing either \nhello\n OR \nworld\n\n\nhello|world\n\n\n\n\n\n\n\n\n\nNot: documents containing \nhello\n but not \nworld\n\n\nhello -world\n\n\n\n\n\n\n\n\n\nIntersection of unions\n\n\n(hello|halo) (world|werld)\n\n\n\n\n\n\n\n\n\nNegation of union\n\n\nhello -(world|werld)\n\n\n\n\n\n\n\n\n\nUnion inside phrase\n\n\n(barack|barrack) obama\n\n\n\n\n\n\n\n\n\nOptional terms with higher priority to ones containing more matches:\n\n\nobama ~barack ~michelle\n\n\n\n\n\n\n\n\n\nExact phrase in one field, one word in aonther field:\n\n\n@title:\nbarack obama\n @job:president\n\n\n\n\n\n\n\n\n\nCombined AND, OR with field specifiers:\n\n\n@title:hello world @body:(foo bar) @category:(articles|biographies)\n\n\n\n\n\n\n\n\n\nPrefix Queries:\n\n\nhello worl*\n\nhel* worl*\n\nhello -worl*\n\n\n\n\n\n\n\n\n\nNumeric Filtering - products named \"tv\" with a price range of 200-500:\n\n\n@name:tv @price:[200 500]\n\n\n\n\n\n\n\n\n\nNumeric Filtering - users with age greater than 18:\n\n\n@age:[(18 +inf]\n\n\n\n\n\n\n\n\n\nMapping Common SQL Predicates to RediSearch\n\n\n\n\n\n\n\n\nSQL Condition\n\n\nRediSearch Equivalent\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nWHERE x='foo' AND y='bar'\n\n\n@x:foo @y:bar\n\n\nfor less ambiguity use (@x:foo) (@y:bar)\n\n\n\n\n\n\nWHERE x='foo' AND y!='bar'\n\n\n@x:foo -@y:bar\n\n\n\n\n\n\n\n\nWHERE x='foo' OR y='bar'\n\n\n(@x:foo)|(@y:bar)\n\n\n\n\n\n\n\n\nWHERE x IN ('foo', 'bar','hello world')\n\n\n@x:(foo|bar|\"hello world\")\n\n\nquotes mean exact phrase\n\n\n\n\n\n\nWHERE y='foo' AND x NOT IN ('foo','bar')\n\n\n@y:foo (-@x:foo) (-@x:bar)\n\n\n\n\n\n\n\n\nWHERE x NOT IN ('foo','bar')\n\n\n-@x:(foo|bar)\n\n\n\n\n\n\n\n\nWHERE num BETWEEN 10 AND 20\n\n\n@num:[10 20]\n\n\n\n\n\n\n\n\nWHERE num \n= 10\n\n\n@num:[10 +inf]\n\n\n\n\n\n\n\n\nWHERE num \n 10\n\n\n@num:[(10 +inf]\n\n\n\n\n\n\n\n\nWHERE num \n 10\n\n\n@num:[-inf (10]\n\n\n\n\n\n\n\n\nWHERE num \n= 10\n\n\n@num:[-inf 10]\n\n\n\n\n\n\n\n\nWHERE num \n 10 OR num \n 20\n\n\n@num:[-inf (10] | @num:[(20 +inf]\n\n\n\n\n\n\n\n\nWHERE name LIKE 'john%'\n\n\n@name:john*\n\n\n\n\n\n\n\n\n\n\nTechnical Note\n\n\nThe query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition \nat the git repo.", 
            "title": "Query Syntax"
        }, 
        {
            "location": "/Query_Syntax/#search\"_\"query\"_\"syntax", 
            "text": "We support a simple syntax for complex queries with the following rules:   Multi-word phrases simply a list of tokens, e.g.  foo bar baz , and imply intersection (AND) of the terms.  Exact phrases are wrapped in quotes, e.g  \"hello world\" .  OR Unions (i.e  word1 OR word2 ), are expressed with a pipe ( | ), e.g.  hello|hallo|shalom|hola .  NOT negation (i.e.  word1 NOT word2 ) of expressions or sub-queries. e.g.  hello -world . As of version 0.19.3, purely negative queries (i.e.  -foo  or  -@title:(foo|bar) ) are supported.   Prefix matches (all terms starting with a prefix) are expressed with a  *  following a 3-letter or longer prefix.  Selection of specific fields using the syntax  @field:hello world .  Numeric Range matches on numeric fields with the syntax  @field:[{min} {max}] .  Geo radius matches on geo fields with the syntax  @field:[{lon} {lat} {radius} {m|km|mi|ft}]  Tag field filters with the syntax  @field:{tag | tag | ...} . See the full documentation on tag fields.  Optional terms or clauses:  foo ~bar  means bar is optional but documents with bar in them will rank higher.   An expression in a query can be wrapped in parentheses to resolve disambiguity, e.g.  (hello|hella) (world|werld) .  Combinations of the above can be used together, e.g  hello (world|foo) \"bar baz\" bbbb", 
            "title": "Search Query Syntax:"
        }, 
        {
            "location": "/Query_Syntax/#pure\"_\"negative\"_\"queries", 
            "text": "As of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g.  -hello  or  -(@title:foo|bar) . The results will be all the documents  NOT  containing the query terms.  Warning : Any complex expression can be negated this way, however caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.", 
            "title": "Pure Negative Queries"
        }, 
        {
            "location": "/Query_Syntax/#field\"_\"modifiers", 
            "text": "As of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword.   Per query expression or sub expression, it is possible to specify which fields it matches, by prepending the experssion with the  @  symbol, the field name and a  :  (colon) symbol.   If a field modifier precedes multiple words, they are considered to be a phrase with the same modifier.   If a field modifier preceds an expression in parentheses, it applies only to the expression inside the parentheses.  Multiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:  FT.SEARCH cars  @country:korea @engine:(diesel|hybrid) @class:suv   Multiple modifiers can be applied to the same term or grouped terms. e.g.:  FT.SEARCH idx  @title|body:(hello world) @url|image:mydomain   This will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.", 
            "title": "Field modifiers"
        }, 
        {
            "location": "/Query_Syntax/#numeric\"_\"filters\"_\"in\"_\"query", 
            "text": "If a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the redis request, or filter with it by specifying filtering rules in the query. The syntax is  @field:[{min} {max}]  - e.g.  @price:[100 200] .", 
            "title": "Numeric Filters in Query"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"notes\"_\"on\"_\"numeric\"_\"predicates", 
            "text": "It is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.    It is possible to interesect or union multiple numeric filters in the same query, be it for the same field or different ones.    -inf ,  inf  and  +inf  are acceptable numbers in range. Thus greater-than 100 is expressed as  [(100 inf] .    Numeric filters are inclusive. Exclusive min or max are expressed with  (  prepended to the number, e.g.  [(100 (200] .    It is possible to negate a numeric filter by prepending a  -  sign to the filter, e.g. returnig a result where price differs from 100 is expressed as:  @title:foo -@price:[100 100] .", 
            "title": "A few notes on numeric predicates:"
        }, 
        {
            "location": "/Query_Syntax/#tag\"_\"filters", 
            "text": "RediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax:   @field:{ tag | tag | ...}\n\ne.g.\n\n@cities:{ New York | Los Angeles | Barcelona }  Tags can have multiple words, or include other punctuation marks other than the field's separator ( ,  by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.  Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing  all  tags, you should repeat the tag filter several times, e.g.:  # This will return all documents containing all three cities as tags:\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n# This will return all documents containing either city:\n@cities:{ New York | Los Angeles | Barcelona }  Tag clauses can be combined into any sub clause, used as negative expressions, optional expressions, etc.", 
            "title": "Tag Filters"
        }, 
        {
            "location": "/Query_Syntax/#geo\"_\"filters\"_\"in\"_\"query", 
            "text": "As of version 0.21, it is possible to add geo radius queries directly into the query language  with the syntax  @field:[{lon} {lat} {radius} {m|km|mi|ft}] . This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own GEORADIUS command for more details (internall we use GEORADIUS for that).  Radius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as:  chinese restaurant @location:[-122.41 37.77 5 km] .", 
            "title": "Geo Filters in Query"
        }, 
        {
            "location": "/Query_Syntax/#prefix\"_\"matching", 
            "text": "On index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending  *  to a prefix token. For example:  hel* world  Will be expanded to cover  (hello|help|helm|...) world .", 
            "title": "Prefix Matching"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"notes\"_\"on\"_\"prefix\"_\"searches", 
            "text": "As prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffxies.    As a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:    Prefixes are limited to 3 letters or more.     Expansion is limited to 200 terms or less.     Prefix matching fully supports unicode and is case insensitive.    Currently there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap.", 
            "title": "A few notes on prefix searches:"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"query\"_\"examples", 
            "text": "Simple phrase query - hello AND world  hello world    Exact phrase query -  hello  FOLLOWED BY  world  hello world     Union: documents containing either  hello  OR  world  hello|world    Not: documents containing  hello  but not  world  hello -world    Intersection of unions  (hello|halo) (world|werld)    Negation of union  hello -(world|werld)    Union inside phrase  (barack|barrack) obama    Optional terms with higher priority to ones containing more matches:  obama ~barack ~michelle    Exact phrase in one field, one word in aonther field:  @title: barack obama  @job:president    Combined AND, OR with field specifiers:  @title:hello world @body:(foo bar) @category:(articles|biographies)    Prefix Queries:  hello worl*\n\nhel* worl*\n\nhello -worl*    Numeric Filtering - products named \"tv\" with a price range of 200-500:  @name:tv @price:[200 500]    Numeric Filtering - users with age greater than 18:  @age:[(18 +inf]", 
            "title": "A Few Query Examples"
        }, 
        {
            "location": "/Query_Syntax/#mapping\"_\"common\"_\"sql\"_\"predicates\"_\"to\"_\"redisearch", 
            "text": "SQL Condition  RediSearch Equivalent  Comments      WHERE x='foo' AND y='bar'  @x:foo @y:bar  for less ambiguity use (@x:foo) (@y:bar)    WHERE x='foo' AND y!='bar'  @x:foo -@y:bar     WHERE x='foo' OR y='bar'  (@x:foo)|(@y:bar)     WHERE x IN ('foo', 'bar','hello world')  @x:(foo|bar|\"hello world\")  quotes mean exact phrase    WHERE y='foo' AND x NOT IN ('foo','bar')  @y:foo (-@x:foo) (-@x:bar)     WHERE x NOT IN ('foo','bar')  -@x:(foo|bar)     WHERE num BETWEEN 10 AND 20  @num:[10 20]     WHERE num  = 10  @num:[10 +inf]     WHERE num   10  @num:[(10 +inf]     WHERE num   10  @num:[-inf (10]     WHERE num  = 10  @num:[-inf 10]     WHERE num   10 OR num   20  @num:[-inf (10] | @num:[(20 +inf]     WHERE name LIKE 'john%'  @name:john*", 
            "title": "Mapping Common SQL Predicates to RediSearch"
        }, 
        {
            "location": "/Query_Syntax/#technical\"_\"note", 
            "text": "The query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition  at the git repo.", 
            "title": "Technical Note"
        }, 
        {
            "location": "/Stopwords/", 
            "text": "Stop-Words\n\n\nRediSearch has a pre-defined default list of \nstop-words\n. These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index. \n\n\nWhen indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query. \n\n\nAt the moment, the default stop-word list applies to all full-text indexes in all languages, and can be overridden manually at index creation time. \n\n\nDefault Stop-Word List\n\n\nThe following words are treated as stop-words by default: \n\n\n a,    is,    the,   an,   and,  are, as,  at,   be,   but,  by,   for,\n if,   in,    into,  it,   no,   not, of,  on,   or,   such, that, their,\n then, there, these, they, this, to,  was, will, with\n\n\n\n\n\nOverriding The Default Stop-Words\n\n\nStop-words for an index can be defined (or disabled completely) on index creation using the \nSTOPWORDS\n argument in the \nFT.CREATE\n command.\n\n\nThe format is \nSTOPWORDS {number} {stopword} ...\n where number is the number of stopwords given. The \nSTOPWORDS\n argument must come before the \nSCHEMA\n argument. For example:\n\n\nFT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT \n\n\n\n\n\nDisabling Stop-Words Completely\n\n\nDisabling stopwords completely can be done by passing \nSTOPWORDS 0\n on \nFT.CREATE\n.\n\n\nAvoiding Stop-Word Detection In Search Queries\n\n\nIn rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time, and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.", 
            "title": "Stop-Words"
        }, 
        {
            "location": "/Stopwords/#stop-words", 
            "text": "RediSearch has a pre-defined default list of  stop-words . These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index.   When indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query.   At the moment, the default stop-word list applies to all full-text indexes in all languages, and can be overridden manually at index creation time.", 
            "title": "Stop-Words"
        }, 
        {
            "location": "/Stopwords/#default\"_\"stop-word\"_\"list", 
            "text": "The following words are treated as stop-words by default:    a,    is,    the,   an,   and,  are, as,  at,   be,   but,  by,   for,\n if,   in,    into,  it,   no,   not, of,  on,   or,   such, that, their,\n then, there, these, they, this, to,  was, will, with", 
            "title": "Default Stop-Word List"
        }, 
        {
            "location": "/Stopwords/#overriding\"_\"the\"_\"default\"_\"stop-words", 
            "text": "Stop-words for an index can be defined (or disabled completely) on index creation using the  STOPWORDS  argument in the  FT.CREATE  command.  The format is  STOPWORDS {number} {stopword} ...  where number is the number of stopwords given. The  STOPWORDS  argument must come before the  SCHEMA  argument. For example:  FT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT", 
            "title": "Overriding The Default Stop-Words"
        }, 
        {
            "location": "/Stopwords/#disabling\"_\"stop-words\"_\"completely", 
            "text": "Disabling stopwords completely can be done by passing  STOPWORDS 0  on  FT.CREATE .", 
            "title": "Disabling Stop-Words Completely"
        }, 
        {
            "location": "/Stopwords/#avoiding\"_\"stop-word\"_\"detection\"_\"in\"_\"search\"_\"queries", 
            "text": "In rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time, and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.", 
            "title": "Avoiding Stop-Word Detection In Search Queries"
        }, 
        {
            "location": "/Sorting/", 
            "text": "Sorting By Indexed Fields\n\n\nAs of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name. \n\n\nDeclaring Sortable Fields\n\n\nWhen creating the index with FT.CREATE, you can declare \nTEXT\n and \nNUMERIC\n properties to be \nSORTABLE\n. When a property is sortable, we can later decide to order the results by its values. For example, in the followig schema:\n\n\n FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE\n\n\n\n\n\nThe fields \nlast_name\n and \nage\n are sortable, but \nfirst_name\n isn't. This means we can search by either first and/or last name, and sort by last name or age. \n\n\nNote on sortable TEXT fields\n\n\nIn the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.\n\n\nAlso note that text fields get normalized and lowercased in a unicode-safe way when stored for sorting , and currently there is no way to change this behavior. This means that \nAmerica\n and \namerica\n are considered equal in terms of sorting.\n\n\nSpecifying SORTBY\n\n\nIf an index includes sortable fields, you can add the \nSORTBY\n parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If \nWITHSCORES\n is specified along with \nSORTBY\n, the scores returned are simply the relative position of each result in the result set.\n\n\nThe syntax for SORTBY is:\n\n\nSORTBY {field_name} [ASC|DESC]\n\n\n\n\n\n\n\n\n\nfield_name must be a sortabl field defined in the schema.\n\n\n\n\n\n\nASC means the order will be ascending, DESC that it will be descending.\n\n\n\n\n\n\nThe default ordering is ASC if not specified otherwise.\n\n\n\n\n\n\nQuick Example\n\n\n FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users\n\n FT.ADD users user1 1.0 FIELDS first_name \nalice\n last_name \njones\n age 35\n\n FT.ADD users user2 1.0 FIELDS first_name \nbob\n last_name \njones\n age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name\n\n FT.SEARCH users \n@last_name:jones\n SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age\n\n FT.SEARCH users \nalice jones\n SORTBY age ASC", 
            "title": "Sortable Values"
        }, 
        {
            "location": "/Sorting/#sorting\"_\"by\"_\"indexed\"_\"fields", 
            "text": "As of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name.", 
            "title": "Sorting By Indexed Fields"
        }, 
        {
            "location": "/Sorting/#declaring\"_\"sortable\"_\"fields", 
            "text": "When creating the index with FT.CREATE, you can declare  TEXT  and  NUMERIC  properties to be  SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the followig schema:   FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE  The fields  last_name  and  age  are sortable, but  first_name  isn't. This means we can search by either first and/or last name, and sort by last name or age.", 
            "title": "Declaring Sortable Fields"
        }, 
        {
            "location": "/Sorting/#note\"_\"on\"_\"sortable\"_\"text\"_\"fields", 
            "text": "In the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.  Also note that text fields get normalized and lowercased in a unicode-safe way when stored for sorting , and currently there is no way to change this behavior. This means that  America  and  america  are considered equal in terms of sorting.", 
            "title": "Note on sortable TEXT fields"
        }, 
        {
            "location": "/Sorting/#specifying\"_\"sortby", 
            "text": "If an index includes sortable fields, you can add the  SORTBY  parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If  WITHSCORES  is specified along with  SORTBY , the scores returned are simply the relative position of each result in the result set.  The syntax for SORTBY is:  SORTBY {field_name} [ASC|DESC]    field_name must be a sortabl field defined in the schema.    ASC means the order will be ascending, DESC that it will be descending.    The default ordering is ASC if not specified otherwise.", 
            "title": "Specifying SORTBY"
        }, 
        {
            "location": "/Sorting/#quick\"_\"example", 
            "text": "FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users  FT.ADD users user1 1.0 FIELDS first_name  alice  last_name  jones  age 35  FT.ADD users user2 1.0 FIELDS first_name  bob  last_name  jones  age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name  FT.SEARCH users  @last_name:jones  SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age  FT.SEARCH users  alice jones  SORTBY age ASC", 
            "title": "Quick Example"
        }, 
        {
            "location": "/Tags/", 
            "text": "Tag Fields\n\n\nRediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text field, but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax.\n\n\nThe main differences between tag fields and full-text fields are:\n\n\n\n\n\n\nAn entire tag field index resides in a single redis key, and doesn't have a key per term as the full-text one.\n\n\n\n\n\n\nWe do not perform stemming on tag indexes.\n\n\n\n\n\n\nThe tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags,\nand we only do whitespace trimming at the end of tags.\nThus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations\nwe perform are lower-casing (for latin languages only as of now), and whitespace trimming.\n\n\n\n\n\n\nTags cannot be found from a general full-text search. If a document has a field called\n\"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag\nmodifier (see below) will not return this document.\n\n\n\n\n\n\nThe index is much simpler and more compressed: We do not store frequencies, offset vectors of\nfield flags. The index contains only docuent ids encoded as deltas. This means that an entry in a tag index\nis usually one or two bytes long. This makes them very memory efficient and fast.\n\n\n\n\n\n\nAn unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024.\n\n\n\n\n\n\nCreating A Tag Field\n\n\nTag fields can be added to the schema in FT.ADD with the following syntax:\n\n\nFT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}]\n\n\n\n\n\nSEPARATOR defaults to a comma (\n,\n), and can be any printable ascii character. For example:\n\n\nFT.CREATE idx SCHEMA tags TAG SEPARATOR \n;\n\n\n\n\n\n\nQuerying Tag Fields\n\n\nAs mentioned above, just searching for a tag without any modifiers will not retrieve documents\ncontaining it.\n\n\nThe syntax for matching tags in a query is as follows (the curly braces are part of the syntax in\nthis case):\n\n\n@\nfield_name\n:{ \ntag\n | \ntag\n | ...}\n\n\ne.g.\n\n\n    @tags:{hello world | foo bar}\n\n\n\n\n\nTag clauses can be combined into any sub clause, used as negative expressions, optional expressions, etc. For example:\n\n\nFT.SEARCH idx \n@title:hello @price:[0 100] @tags:{ foo bar | hello world }\n\n\n\n\n\nMultiple Tags In A Single Filter\n\n\nNotice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing \nall\n tags, you should repeat the tag filter several times.\n\n\nFor example, imagine an index of travellers, with a tag field for the cities each traveller has visited:\n\n\nFT.CREATE myIndex SCHEMA name TEXT cities TAG\n\nFT.ADD myIndex user1 1.0 FIELDS name \nJohn Doe\n cities \nNew York, Barcelona, San Francisco\n\n\n\n\n\n\nFor this index, the following query will return all the people who visited \nat least one\n of the following cities:\n\n\nFT.SEARCH myIndex \n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\n\nBut the next query will return all people who have visited \nall three cities\n:\n\n\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n\n\n\n\nMulti-Word Tags And Escaping\n\n\nTags can be composed multiple words, or include other punctuation marks other than the field's separator (\n,\n by default). Punctuation marks in tags should be escaped with a backslash (\n\\\n). \n\n\nIt is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.\n\n\nThe following are identical:\n\n\n@tags:{foo\\ bar\\ baz | hello\\ world}\n\n@tags:{foo bar baz | hello world }", 
            "title": "Tag Fields"
        }, 
        {
            "location": "/Tags/#tag\"_\"fields", 
            "text": "RediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text field, but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax.  The main differences between tag fields and full-text fields are:    An entire tag field index resides in a single redis key, and doesn't have a key per term as the full-text one.    We do not perform stemming on tag indexes.    The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags,\nand we only do whitespace trimming at the end of tags.\nThus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations\nwe perform are lower-casing (for latin languages only as of now), and whitespace trimming.    Tags cannot be found from a general full-text search. If a document has a field called\n\"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag\nmodifier (see below) will not return this document.    The index is much simpler and more compressed: We do not store frequencies, offset vectors of\nfield flags. The index contains only docuent ids encoded as deltas. This means that an entry in a tag index\nis usually one or two bytes long. This makes them very memory efficient and fast.    An unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024.", 
            "title": "Tag Fields"
        }, 
        {
            "location": "/Tags/#creating\"_\"a\"_\"tag\"_\"field", 
            "text": "Tag fields can be added to the schema in FT.ADD with the following syntax:  FT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}]  SEPARATOR defaults to a comma ( , ), and can be any printable ascii character. For example:  FT.CREATE idx SCHEMA tags TAG SEPARATOR  ;", 
            "title": "Creating A Tag Field"
        }, 
        {
            "location": "/Tags/#querying\"_\"tag\"_\"fields", 
            "text": "As mentioned above, just searching for a tag without any modifiers will not retrieve documents\ncontaining it.  The syntax for matching tags in a query is as follows (the curly braces are part of the syntax in\nthis case):  @ field_name :{  tag  |  tag  | ...}  e.g.      @tags:{hello world | foo bar}  Tag clauses can be combined into any sub clause, used as negative expressions, optional expressions, etc. For example:  FT.SEARCH idx  @title:hello @price:[0 100] @tags:{ foo bar | hello world }", 
            "title": "Querying Tag Fields"
        }, 
        {
            "location": "/Tags/#multiple\"_\"tags\"_\"in\"_\"a\"_\"single\"_\"filter", 
            "text": "Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing  all  tags, you should repeat the tag filter several times.  For example, imagine an index of travellers, with a tag field for the cities each traveller has visited:  FT.CREATE myIndex SCHEMA name TEXT cities TAG\n\nFT.ADD myIndex user1 1.0 FIELDS name  John Doe  cities  New York, Barcelona, San Francisco   For this index, the following query will return all the people who visited  at least one  of the following cities:  FT.SEARCH myIndex  @cities:{ New York | Los Angeles | Barcelona }   But the next query will return all people who have visited  all three cities :  @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }", 
            "title": "Multiple Tags In A Single Filter"
        }, 
        {
            "location": "/Tags/#multi-word\"_\"tags\"_\"and\"_\"escaping", 
            "text": "Tags can be composed multiple words, or include other punctuation marks other than the field's separator ( ,  by default). Punctuation marks in tags should be escaped with a backslash ( \\ ).   It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.  The following are identical:  @tags:{foo\\ bar\\ baz | hello\\ world}\n\n@tags:{foo bar baz | hello world }", 
            "title": "Multi-Word Tags And Escaping"
        }, 
        {
            "location": "/Highlight/", 
            "text": "Highlighting API\n\n\nThe highlighting API allows you to have only the relvant (matching)\nportions of document matches returned. This allows users to quickly see how a\ndocument relates to their query.\n\n\nCommand Syntax\n\n\nFT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}]\n    HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]\n\n\n\n\n\nThere are two sub-commands commands used for highlighting. One is \nHIGHLIGHT\n\nwhich surrounds matching text with an open and/or close tag; and the other is\n\nSUMMARIZE\n which splits a field into contextual fragments surrounding the\nfound terms. It is possible to summarize a field, highlight a field, or perform\nboth actions in the same query.\n\n\nSummarization\n\n\nFT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}]\n\n\n\n\n\nSummarization or snippetization will fragment the text into smaller sized\nsnippets; each snippet will contain the found term(s) and some additional\nsurrounding context.\n\n\nRedis Search can perform summarization using the \nSUMMARIZE\n keyword. If no\nadditional arguments are passed, all \nreturned fields\n are summarized using\nbuilt-in defaults.\n\n\nThe \nSUMMARIZE\n keyword accepts the following arguments:\n\n\n\n\nFIELDS\n If present, must be the first argument. This should be followed\n    by the number of fields to summarize, which itself is followed by a list of\n    fields. Each field present is summarized. If no \nFIELD\n directive is passed,\n    then \nall\n fields returned are summarized.\n\n\nFRAGS\n How many fragments should be returned. If not specified, a sane\n    default is used.\n\n\nLEN\n The number of context words each fragment should contain. Context\n    words surround the found term. A higher value will return a larger block of\n    text.\n\n\nSEPARATOR\n The string used to divide between individual summary snippets.\n    The default is \n...\n which is common among search engines; but you may\n    override this with any other string if you desire to programmatically divide them\n    later on. You may use a newline sequence, as newlines are stripped from the\n    result body anyway (thus, it will not be conflated with an embedded newline\n    in the text)\n\n\n\n\nHighlighting\n\n\nFT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]\n\n\n\n\n\nHighlighting will highlight the found term (and its variants) with a user-defined\ntag. This may be used to display the matched text in a different typeface using\na markup language, or to otherwise make the text appear differently.\n\n\nRedis Search can perform highlighting using the \nHIGHLIGHT\n keyword. If no\nadditional arguments are passed, all \nreturned fields\n are highlighted using\nbuild-in defaults.\n\n\nThe \nHIGHLIGHT\n keyword accepts the following arguments:\n\n\n\n\nFIELDS\n If present, must be the first argument. This should be followed\n    by the number of fields to highlight, which itself is followed by a list of\n    fields. Each field present is highlighted. If no \nFIELD\n directive is passed,\n    then \nall\n fields returned are highlighted.\n\n\nTAGS\n If present, must be followed by two strings; the first is prepended\n    to each term match, and the second is appended to it. If no \nTAGS\n are\n    specified, a built-in tag value is appended and prepended.\n\n\n\n\nField Selection\n\n\nIf no specific fields are passed to the \nRETURN\n, \nSUMMARIZE\n, or \nHIGHLIGHT\n\nkeywords, then all of a document's fields are returned. However, if any of these\nkeywords contain a \nFIELD\n directive, then the \nSEARCH\n command will only retun\nthe sum total of all fields enumerated in any of those directives.\n\n\nThe \nRETURN\n keyword is treated specially, as it overrides any fields specified\nin \nSUMMARIZE\n or \nHIGHLIGHT\n.\n\n\nIn the command \nRETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz\n,\nthe fields \nfoo\n is returned as-is, while \nbar\n and \nbaz\n are not returned, because\n\nRETURN\n was specified, but did not include those fields.\n\n\nIn the command \nSUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz\n, \nbar\n is returned\nsummarized and \nbaz\n is returned highlighted.", 
            "title": "Highlighting Results"
        }, 
        {
            "location": "/Highlight/#highlighting\"_\"api", 
            "text": "The highlighting API allows you to have only the relvant (matching)\nportions of document matches returned. This allows users to quickly see how a\ndocument relates to their query.", 
            "title": "Highlighting API"
        }, 
        {
            "location": "/Highlight/#command\"_\"syntax", 
            "text": "FT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}]\n    HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]  There are two sub-commands commands used for highlighting. One is  HIGHLIGHT \nwhich surrounds matching text with an open and/or close tag; and the other is SUMMARIZE  which splits a field into contextual fragments surrounding the\nfound terms. It is possible to summarize a field, highlight a field, or perform\nboth actions in the same query.", 
            "title": "Command Syntax"
        }, 
        {
            "location": "/Highlight/#summarization", 
            "text": "FT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}]  Summarization or snippetization will fragment the text into smaller sized\nsnippets; each snippet will contain the found term(s) and some additional\nsurrounding context.  Redis Search can perform summarization using the  SUMMARIZE  keyword. If no\nadditional arguments are passed, all  returned fields  are summarized using\nbuilt-in defaults.  The  SUMMARIZE  keyword accepts the following arguments:   FIELDS  If present, must be the first argument. This should be followed\n    by the number of fields to summarize, which itself is followed by a list of\n    fields. Each field present is summarized. If no  FIELD  directive is passed,\n    then  all  fields returned are summarized.  FRAGS  How many fragments should be returned. If not specified, a sane\n    default is used.  LEN  The number of context words each fragment should contain. Context\n    words surround the found term. A higher value will return a larger block of\n    text.  SEPARATOR  The string used to divide between individual summary snippets.\n    The default is  ...  which is common among search engines; but you may\n    override this with any other string if you desire to programmatically divide them\n    later on. You may use a newline sequence, as newlines are stripped from the\n    result body anyway (thus, it will not be conflated with an embedded newline\n    in the text)", 
            "title": "Summarization"
        }, 
        {
            "location": "/Highlight/#highlighting", 
            "text": "FT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]  Highlighting will highlight the found term (and its variants) with a user-defined\ntag. This may be used to display the matched text in a different typeface using\na markup language, or to otherwise make the text appear differently.  Redis Search can perform highlighting using the  HIGHLIGHT  keyword. If no\nadditional arguments are passed, all  returned fields  are highlighted using\nbuild-in defaults.  The  HIGHLIGHT  keyword accepts the following arguments:   FIELDS  If present, must be the first argument. This should be followed\n    by the number of fields to highlight, which itself is followed by a list of\n    fields. Each field present is highlighted. If no  FIELD  directive is passed,\n    then  all  fields returned are highlighted.  TAGS  If present, must be followed by two strings; the first is prepended\n    to each term match, and the second is appended to it. If no  TAGS  are\n    specified, a built-in tag value is appended and prepended.", 
            "title": "Highlighting"
        }, 
        {
            "location": "/Highlight/#field\"_\"selection", 
            "text": "If no specific fields are passed to the  RETURN ,  SUMMARIZE , or  HIGHLIGHT \nkeywords, then all of a document's fields are returned. However, if any of these\nkeywords contain a  FIELD  directive, then the  SEARCH  command will only retun\nthe sum total of all fields enumerated in any of those directives.  The  RETURN  keyword is treated specially, as it overrides any fields specified\nin  SUMMARIZE  or  HIGHLIGHT .  In the command  RETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz ,\nthe fields  foo  is returned as-is, while  bar  and  baz  are not returned, because RETURN  was specified, but did not include those fields.  In the command  SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz ,  bar  is returned\nsummarized and  baz  is returned highlighted.", 
            "title": "Field Selection"
        }, 
        {
            "location": "/Scoring/", 
            "text": "Scoring In RediSearch\n\n\nRediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use \nsortable fields\n. Scoring functions are specified by adding the \nSCORER {scorer_name}\n argument to a search query.\n\n\nIf you prefer a custom scoring function, it is possible to add more functions using the \nExtension API\n.\n\n\nThese are the pre-bunldled scoring functions availabe in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a SCORER argument in FT.SEARCH.\n\n\nTFIDF (Default)\n\n\nBasic \nTF-IDF scoring\n with a few extra features thrown inside:\n\n\n\n\n\n\nFor each term in each result we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is \nnormalized by the highest term frequency in each document\n.\n\n\n\n\n\n\nWe multiply the total TF-IDF for the query term by the a priory document score given on \nFT.ADD\n.\n\n\n\n\n\n\nWe give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penlty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared - \n1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...)\n. \n\n\n\n\n\n\nSo for N terms in a document D, \nT1...Tn\n, the resulting score could be described with this python function:\n\n\ndef\n \nget_score\n(\nterms\n,\n \ndoc\n):\n\n    \n# the sum of tf-idf\n\n    \nscore\n \n=\n \n0\n\n\n    \n# the distance penalty for all terms\n\n    \ndist_penalty\n \n=\n \n0\n\n\n    \nfor\n \ni\n,\n \nterm\n \nin\n \nenumerate\n(\nterms\n):\n\n        \n# tf normalized by maximum frequency\n\n        \ntf\n \n=\n \ndoc\n.\nfreq\n(\nterm\n)\n \n/\n \ndoc\n.\nmax_freq\n\n\n        \n# idf is global for the index, and not calculated each time in real life\n\n        \nidf\n \n=\n \nlog2\n(\n1\n \n+\n \ntotal_docs\n \n/\n \ndocs_with_term\n(\nterm\n))\n\n\n        \nscore\n \n+=\n \ntf\n*\nidf\n\n\n        \n# sum up the distance penalty\n\n        \nif\n \ni\n \n \n0\n:\n\n            \ndist_penalty\n \n+=\n \nmin_distance\n(\nterm\n,\n \nterms\n[\ni\n-\n1\n])\n**\n2\n\n\n    \n# multiply the score by the document score\n\n    \nscore\n \n*=\n \ndoc\n.\nscore\n\n\n    \n# divide the score by the root of the cumulative distance\n\n    \nif\n \nlen\n(\nterms\n)\n \n \n1\n:\n\n        \nscore\n \n/=\n \nsqrt\n(\ndist_penalty\n)\n\n\n    \nreturn\n \nscore\n\n\n\n\n\n\nTFIDF.DOCNORM\n\n\nIdentical to the default TFIDF scorer, with one important distinction:\n\n\nTerm frequencies are normalized by the length of the document (in number of terms). The length is weighted, so that if a document contains two terms, one in a feild that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER TFIDF.DOCNORM\n\n\n\n\n\nBM25\n\n\nA vraiation on the basic TF-IDF scorer, see \nthis Wikipedia article for more info\n.\n\n\nWe also multiply the relevance score for each document by the a priory docment score, and apply a penalty based on slop as in TFIDF.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER BM25\n\n\n\n\n\nDISMAX\n\n\nA simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied.\n\n\nIt is not a 1 to 1 implementation of \nSolr's DISMAX algorithm\n, but follows it in broad terms.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER DISMAX\n\n\n\n\n\nDOCSCORE\n\n\nA scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updates, this can be useful if you'd like to use an external score and nothing further.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER DOCSCORE", 
            "title": "Scoring Documents"
        }, 
        {
            "location": "/Scoring/#scoring\"_\"in\"_\"redisearch", 
            "text": "RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use  sortable fields . Scoring functions are specified by adding the  SCORER {scorer_name}  argument to a search query.  If you prefer a custom scoring function, it is possible to add more functions using the  Extension API .  These are the pre-bunldled scoring functions availabe in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a SCORER argument in FT.SEARCH.", 
            "title": "Scoring In RediSearch"
        }, 
        {
            "location": "/Scoring/#tfidf\"_\"default", 
            "text": "Basic  TF-IDF scoring  with a few extra features thrown inside:    For each term in each result we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is  normalized by the highest term frequency in each document .    We multiply the total TF-IDF for the query term by the a priory document score given on  FT.ADD .    We give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penlty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared -  1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...) .     So for N terms in a document D,  T1...Tn , the resulting score could be described with this python function:  def   get_score ( terms ,   doc ): \n     # the sum of tf-idf \n     score   =   0 \n\n     # the distance penalty for all terms \n     dist_penalty   =   0 \n\n     for   i ,   term   in   enumerate ( terms ): \n         # tf normalized by maximum frequency \n         tf   =   doc . freq ( term )   /   doc . max_freq \n\n         # idf is global for the index, and not calculated each time in real life \n         idf   =   log2 ( 1   +   total_docs   /   docs_with_term ( term )) \n\n         score   +=   tf * idf \n\n         # sum up the distance penalty \n         if   i     0 : \n             dist_penalty   +=   min_distance ( term ,   terms [ i - 1 ]) ** 2 \n\n     # multiply the score by the document score \n     score   *=   doc . score \n\n     # divide the score by the root of the cumulative distance \n     if   len ( terms )     1 : \n         score   /=   sqrt ( dist_penalty ) \n\n     return   score", 
            "title": "TFIDF (Default)"
        }, 
        {
            "location": "/Scoring/#tfidfdocnorm", 
            "text": "Identical to the default TFIDF scorer, with one important distinction:  Term frequencies are normalized by the length of the document (in number of terms). The length is weighted, so that if a document contains two terms, one in a feild that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2.  FT.SEARCH myIndex  foo  SCORER TFIDF.DOCNORM", 
            "title": "TFIDF.DOCNORM"
        }, 
        {
            "location": "/Scoring/#bm25", 
            "text": "A vraiation on the basic TF-IDF scorer, see  this Wikipedia article for more info .  We also multiply the relevance score for each document by the a priory docment score, and apply a penalty based on slop as in TFIDF.  FT.SEARCH myIndex  foo  SCORER BM25", 
            "title": "BM25"
        }, 
        {
            "location": "/Scoring/#dismax", 
            "text": "A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied.  It is not a 1 to 1 implementation of  Solr's DISMAX algorithm , but follows it in broad terms.  FT.SEARCH myIndex  foo  SCORER DISMAX", 
            "title": "DISMAX"
        }, 
        {
            "location": "/Scoring/#docscore", 
            "text": "A scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updates, this can be useful if you'd like to use an external score and nothing further.  FT.SEARCH myIndex  foo  SCORER DOCSCORE", 
            "title": "DOCSCORE"
        }, 
        {
            "location": "/Extensions/", 
            "text": "Extending RediSearch\n\n\nRediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time.\n\n\nThere are two kinds of extension APIs at the moment: \n\n\n\n\nQuery Expanders\n, whose role is to expand query tokens (i.e. stemmers).\n\n\nScoring Funtions\n, whose role is to rank search results in query time.\n\n\n\n\nRegistering and Loading Extensions\n\n\nExtensions should be compiled into .so files, and loaded into RediSearch on initialization of the module. \n\n\n\n\n\n\nCompiling \n\n\nExtensions should be compiled and linked as dynamic libraries. An example Makefile for an extension \ncan be found here\n. \n\n\nThat folder also contains an example extension that is used for testing, and can be taken as a skeleton for implementing your own extension.\n\n\n\n\n\n\nLoading \n\n\nLoading an extension is done by apending \nEXTLOAD {path/to/ext.so}\n after the \nloadmodule\n configuration directive when loading RediSearch. For example:\n\n\nsh\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so\n\n\nThis causes RediSearch to automatically load the extension and register its expanders and scorers. \n\n\n\n\n\n\nInitializing an Extension\n\n\nThe entry point of an extension is a function with the signature:\n\n\nint\n \nRS_ExtensionInit\n(\nRSExtensionCtx\n \n*\nctx\n);\n\n\n\n\n\n\nWhen loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers. \n\n\nIt should return REDISEARCH_ERR on error or REDISEARCH_OK on success.\n\n\nExample Init Function\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nint\n \nRS_ExtensionInit\n(\nRSExtensionCtx\n \n*\nctx\n)\n \n{\n\n\n  \n/* Register  a scoring function with an alias my_scorer and no special private data and free function */\n\n  \nif\n \n(\nctx\n-\nRegisterScoringFunction\n(\nmy_scorer\n,\n \nMyCustomScorer\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \n/* Register a query expander  */\n\n  \nif\n \n(\nctx\n-\nRegisterQueryExpander\n(\nmy_expander\n,\n \nMyExpander\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n\n      \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \nreturn\n \nREDISEARCH_OK\n;\n\n\n}\n\n\n\n\n\n\nCalling your custom functions\n\n\nWhen performing a query, you can tell RediSearch to use your scorers or expanders by specifing the SCORER or EXPANDER arguments, with the given alias.\ne.g.:\n\n\nFT.SEARCH my_index \nfoo bar\n EXPANDER my_expander SCORER my_scorer\n\n\n\n\n\nNOTE\n: Expander and scorer aliases are \ncase sensitive\n.\n\n\nThe Query Expander API\n\n\nAt the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.\n\n\nThe API for an expander is the following:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nMyQueryExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \n...\n\n\n}\n\n\n\n\n\n\nRSQueryExpanderCtx\n\n\nRSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:\n\n\ntypedef\n \nstruct\n \nRSQueryExpanderCtx\n \n{\n\n\n  \n/* Opaque query object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQuery\n \n*\nquery\n;\n\n\n  \n/* Opaque query node object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQueryNode\n \n**\ncurrentNode\n;\n\n\n  \n/* Private data of the extension, set on extension initialization */\n\n  \nvoid\n \n*\nprivdata\n;\n\n\n  \n/* The language of the query, defaults to \nenglish\n */\n\n  \nconst\n \nchar\n \n*\nlanguage\n;\n\n\n  \n/* ExpandToken allows the user to add an expansion of the token in the query, that will be\n\n\n   * union-merged with the given token in query time. str is the expanded string, len is its length,\n\n\n   * and flags is a 32 bit flag mask that can be used by the extension to set private information on\n\n\n   * the token */\n\n  \nvoid\n \n(\n*\nExpandToken\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nconst\n \nchar\n \n*\nstr\n,\n \nsize_t\n \nlen\n,\n\n                      \nRSTokenFlags\n \nflags\n);\n\n\n  \n/* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)\n\n\n   */\n\n  \nvoid\n \n(\n*\nSetPayload\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSPayload\n \npayload\n);\n\n\n\n}\n \nRSQueryExpanderCtx\n;\n\n\n\n\n\n\nRSToken\n\n\nRSToken represents a single query token to be expanded, and is defined as:\n\n\n/* A token in the query. The expanders receive query tokens and can expand the query with more query\n\n\n * tokens */\n\n\ntypedef\n \nstruct\n \n{\n\n  \n/* The token string - which may or may not be NULL terminated */\n\n  \nconst\n \nchar\n \n*\nstr\n;\n\n  \n/* The token length */\n\n  \nsize_t\n \nlen\n;\n\n\n  \n/* 1 if the token is the result of query expansion */\n\n  \nuint8_t\n \nexpanded\n:\n1\n;\n\n\n  \n/* Extension specific token flags that can be examined later by the scoring function */\n\n  \nRSTokenFlags\n \nflags\n;\n\n\n}\n \nRSToken\n;\n\n\n\n\n\n\nThe Scoring Function API\n\n\nA scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.\n\n\nSince the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized. \n\n\nA scoring function is applied to each potential result (per document) and is implemented with the following signature:\n\n\ndouble\n \nMyScoringFunction\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nres\n,\n\n                                    \nRSDocumentMetadata\n \n*\ndmd\n,\n \ndouble\n \nminScore\n);\n\n\n\n\n\n\nRSScoringFunctionCtx is a context that implements some helper methods. \n\n\nRSIndexResult is the result information - containing the document id, frequency, terms and offsets. \n\n\nRSDocumentMetadata is an object holding global information about the document, such as its a-priory score. \n\n\nminSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.\n\n\nThe return value of the function is double representing the final score of the result. \nReturning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. \nTo completely filter out a result and not count it in the totals, the scorer should return the special value \nRS_SCORE_FILTEROUT\n (which is internally to negative infinity, or -1/0). \n\n\nRSScoringFunctionCtx\n\n\nThis is an object containing the following members:\n\n\n\n\nvoid *privdata\n: a pointer to an object set by the extension on initialization time.\n\n\nRSPayload payload\n: A Payload object set either by the query expander or the client.\n\n\nint GetSlop(RSIndexResult *res)\n: A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.\n\n\n\n\nRSIndexResult\n\n\nThis is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.\n\n\nSee redisearch.h for details\n\n\nRSDocumentMetadata\n\n\nThis is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function. \n\n\nExample Query Expander\n\n\nThis example query expander expands each token with the the term foo:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nDummyExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \nctx\n-\nExpandToken\n(\nctx\n,\n \nstrdup\n(\nfoo\n),\n \nstrlen\n(\nfoo\n),\n \n0x1337\n);\n  \n\n}\n\n\n\n\n\n\nExample Scoring Function\n\n\nThis is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\ndouble\n \nTFIDFScorer\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nh\n,\n \nRSDocumentMetadata\n \n*\ndmd\n,\n\n                   \ndouble\n \nminScore\n)\n \n{\n\n  \n// no need to evaluate documents with score 0 \n\n  \nif\n \n(\ndmd\n-\nscore\n \n==\n \n0\n)\n \nreturn\n \n0\n;\n\n\n  \n// calculate sum(tf-idf) for each term in the result\n\n  \ndouble\n \ntfidf\n \n=\n \n0\n;\n\n  \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nh\n-\nnumRecords\n;\n \ni\n++\n)\n \n{\n\n    \n// take the term frequency and multiply by the term IDF, add that to the total\n\n    \ntfidf\n \n+=\n \n(\nfloat\n)\nh\n-\nrecords\n[\ni\n].\nfreq\n \n*\n \n(\nh\n-\nrecords\n[\ni\n].\nterm\n \n?\n \nh\n-\nrecords\n[\ni\n].\nterm\n-\nidf\n \n:\n \n0\n);\n\n  \n}\n\n  \n// normalize by the maximal frequency of any term in the document   \n\n  \ntfidf\n \n/=\n  \n(\ndouble\n)\ndmd\n-\nmaxFreq\n;\n\n\n  \n// multiply by the document score (between 0 and 1)\n\n  \ntfidf\n \n*=\n \ndmd\n-\nscore\n;\n\n\n  \n// no need to factor the slop if tfidf is already below minimal score\n\n  \nif\n \n(\ntfidf\n \n \nminScore\n)\n \n{\n\n    \nreturn\n \n0\n;\n\n  \n}\n\n\n  \n// get the slop and divide the result by it, making sure we prefer results with closer terms\n\n  \ntfidf\n \n/=\n \n(\ndouble\n)\nctx\n-\nGetSlop\n(\nh\n);\n\n\n  \nreturn\n \ntfidf\n;\n\n\n}", 
            "title": "Extension API"
        }, 
        {
            "location": "/Extensions/#extending\"_\"redisearch", 
            "text": "RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time.  There are two kinds of extension APIs at the moment:    Query Expanders , whose role is to expand query tokens (i.e. stemmers).  Scoring Funtions , whose role is to rank search results in query time.", 
            "title": "Extending RediSearch"
        }, 
        {
            "location": "/Extensions/#registering\"_\"and\"_\"loading\"_\"extensions", 
            "text": "Extensions should be compiled into .so files, and loaded into RediSearch on initialization of the module.     Compiling   Extensions should be compiled and linked as dynamic libraries. An example Makefile for an extension  can be found here .   That folder also contains an example extension that is used for testing, and can be taken as a skeleton for implementing your own extension.    Loading   Loading an extension is done by apending  EXTLOAD {path/to/ext.so}  after the  loadmodule  configuration directive when loading RediSearch. For example:  sh\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so  This causes RediSearch to automatically load the extension and register its expanders and scorers.", 
            "title": "Registering and Loading Extensions"
        }, 
        {
            "location": "/Extensions/#initializing\"_\"an\"_\"extension", 
            "text": "The entry point of an extension is a function with the signature:  int   RS_ExtensionInit ( RSExtensionCtx   * ctx );   When loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers.   It should return REDISEARCH_ERR on error or REDISEARCH_OK on success.", 
            "title": "Initializing an Extension"
        }, 
        {
            "location": "/Extensions/#example\"_\"init\"_\"function", 
            "text": "#include   redisearch.h  //must be in the include path  int   RS_ExtensionInit ( RSExtensionCtx   * ctx )   { \n\n   /* Register  a scoring function with an alias my_scorer and no special private data and free function */ \n   if   ( ctx - RegisterScoringFunction ( my_scorer ,   MyCustomScorer ,   NULL ,   NULL )   ==   REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   /* Register a query expander  */ \n   if   ( ctx - RegisterQueryExpander ( my_expander ,   MyExpander ,   NULL ,   NULL )   == \n       REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   return   REDISEARCH_OK ;  }", 
            "title": "Example Init Function"
        }, 
        {
            "location": "/Extensions/#calling\"_\"your\"_\"custom\"_\"functions", 
            "text": "When performing a query, you can tell RediSearch to use your scorers or expanders by specifing the SCORER or EXPANDER arguments, with the given alias.\ne.g.:  FT.SEARCH my_index  foo bar  EXPANDER my_expander SCORER my_scorer  NOTE : Expander and scorer aliases are  case sensitive .", 
            "title": "Calling your custom functions"
        }, 
        {
            "location": "/Extensions/#the\"_\"query\"_\"expander\"_\"api", 
            "text": "At the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.  The API for an expander is the following:  #include   redisearch.h  //must be in the include path  void   MyQueryExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ...  }", 
            "title": "The Query Expander API"
        }, 
        {
            "location": "/Extensions/#rsqueryexpanderctx", 
            "text": "RSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:  typedef   struct   RSQueryExpanderCtx   { \n\n   /* Opaque query object used internally by the engine, and should not be accessed */ \n   struct   RSQuery   * query ; \n\n   /* Opaque query node object used internally by the engine, and should not be accessed */ \n   struct   RSQueryNode   ** currentNode ; \n\n   /* Private data of the extension, set on extension initialization */ \n   void   * privdata ; \n\n   /* The language of the query, defaults to  english  */ \n   const   char   * language ; \n\n   /* ExpandToken allows the user to add an expansion of the token in the query, that will be     * union-merged with the given token in query time. str is the expanded string, len is its length,     * and flags is a 32 bit flag mask that can be used by the extension to set private information on     * the token */ \n   void   ( * ExpandToken )( struct   RSQueryExpanderCtx   * ctx ,   const   char   * str ,   size_t   len , \n                       RSTokenFlags   flags ); \n\n   /* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)     */ \n   void   ( * SetPayload )( struct   RSQueryExpanderCtx   * ctx ,   RSPayload   payload );  }   RSQueryExpanderCtx ;", 
            "title": "RSQueryExpanderCtx"
        }, 
        {
            "location": "/Extensions/#rstoken", 
            "text": "RSToken represents a single query token to be expanded, and is defined as:  /* A token in the query. The expanders receive query tokens and can expand the query with more query   * tokens */  typedef   struct   { \n   /* The token string - which may or may not be NULL terminated */ \n   const   char   * str ; \n   /* The token length */ \n   size_t   len ; \n\n   /* 1 if the token is the result of query expansion */ \n   uint8_t   expanded : 1 ; \n\n   /* Extension specific token flags that can be examined later by the scoring function */ \n   RSTokenFlags   flags ;  }   RSToken ;", 
            "title": "RSToken"
        }, 
        {
            "location": "/Extensions/#the\"_\"scoring\"_\"function\"_\"api", 
            "text": "A scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.  Since the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized.   A scoring function is applied to each potential result (per document) and is implemented with the following signature:  double   MyScoringFunction ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * res , \n                                     RSDocumentMetadata   * dmd ,   double   minScore );   RSScoringFunctionCtx is a context that implements some helper methods.   RSIndexResult is the result information - containing the document id, frequency, terms and offsets.   RSDocumentMetadata is an object holding global information about the document, such as its a-priory score.   minSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.  The return value of the function is double representing the final score of the result. \nReturning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. \nTo completely filter out a result and not count it in the totals, the scorer should return the special value  RS_SCORE_FILTEROUT  (which is internally to negative infinity, or -1/0).", 
            "title": "The Scoring Function API"
        }, 
        {
            "location": "/Extensions/#rsscoringfunctionctx", 
            "text": "This is an object containing the following members:   void *privdata : a pointer to an object set by the extension on initialization time.  RSPayload payload : A Payload object set either by the query expander or the client.  int GetSlop(RSIndexResult *res) : A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.", 
            "title": "RSScoringFunctionCtx"
        }, 
        {
            "location": "/Extensions/#rsindexresult", 
            "text": "This is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.  See redisearch.h for details", 
            "title": "RSIndexResult"
        }, 
        {
            "location": "/Extensions/#rsdocumentmetadata", 
            "text": "This is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function.", 
            "title": "RSDocumentMetadata"
        }, 
        {
            "location": "/Extensions/#example\"_\"query\"_\"expander", 
            "text": "This example query expander expands each token with the the term foo:  #include   redisearch.h  //must be in the include path  void   DummyExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ctx - ExpandToken ( ctx ,   strdup ( foo ),   strlen ( foo ),   0x1337 );    }", 
            "title": "Example Query Expander"
        }, 
        {
            "location": "/Extensions/#example\"_\"scoring\"_\"function", 
            "text": "This is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:  #include   redisearch.h  //must be in the include path  double   TFIDFScorer ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * h ,   RSDocumentMetadata   * dmd , \n                    double   minScore )   { \n   // no need to evaluate documents with score 0  \n   if   ( dmd - score   ==   0 )   return   0 ; \n\n   // calculate sum(tf-idf) for each term in the result \n   double   tfidf   =   0 ; \n   for   ( int   i   =   0 ;   i     h - numRecords ;   i ++ )   { \n     // take the term frequency and multiply by the term IDF, add that to the total \n     tfidf   +=   ( float ) h - records [ i ]. freq   *   ( h - records [ i ]. term   ?   h - records [ i ]. term - idf   :   0 ); \n   } \n   // normalize by the maximal frequency of any term in the document    \n   tfidf   /=    ( double ) dmd - maxFreq ; \n\n   // multiply by the document score (between 0 and 1) \n   tfidf   *=   dmd - score ; \n\n   // no need to factor the slop if tfidf is already below minimal score \n   if   ( tfidf     minScore )   { \n     return   0 ; \n   } \n\n   // get the slop and divide the result by it, making sure we prefer results with closer terms \n   tfidf   /=   ( double ) ctx - GetSlop ( h ); \n\n   return   tfidf ;  }", 
            "title": "Example Scoring Function"
        }, 
        {
            "location": "/Stemming/", 
            "text": "Stemming Support\n\n\nRediSearch supports stemming - that is adding the base form of a word to the index. This allows \nthe query for \"going\" to also return results for \"go\" and \"gone\", for example. \n\n\nThe current stemming support is based on the Snowball stemmer library, which supports most European\nlanguages, as well as Arabic and other. We hope to include more languages soon (if you need a specicif\nlangauge support, please open an issue). \n\n\nFor further details see the \nSnowball Stemmer website\n.\n\n\nSupported languages:\n\n\nThe following languages are supported, and can be passed to the engine \nwhen indexing or querying, with lowercase letters:\n\n\n\n\narabic\n\n\ndanish\n\n\ndutch\n\n\nenglish\n\n\nfinnish\n\n\nfrench\n\n\ngerman\n\n\nhungarian\n\n\nitalian\n\n\nnorwegian\n\n\nportuguese\n\n\nromanian\n\n\nrussian\n\n\nspanish\n\n\nswedish\n\n\ntamil\n\n\nturkish\n\n\nchinese (see below)\n\n\n\n\nChinese support\n\n\nIndexing a Chinese document is different than indexing a document in most other\nlanguages because of how tokens are extracted. While most languages can have\ntheir tokens distinguished by separation characters and whitespace, this\nis not common in Chinese.\n\n\nChinese tokenization is done by scanning the input text and checking every\ncharacter or sequence of characters against a dictionary of predefined terms\nand determining the most likely (based on the surrounding terms and characters)\nmatch.\n\n\nRedis Search makes use of the \nFriso\n\nchinese tokenization library for this purpose. This is largely transparent to\nthe user and often no additional configuration is required.\n\n\nUsing custom dictionaries\n\n\nIf you wish to use a custom dictionary, you can do so at the module level when\nloading the module. The \nFRISOINI\n setting can point to the location of a\n\nfriso.ini\n file which contains the relevant settings and paths to the dictionary\nfiles.\n\n\nNote that there is no \"default\" friso.ini file location. Redis Search comes with\nits own \nfriso.ini\n and dictionary files which are compiled into the module\nbinary at build-time.", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#stemming\"_\"support", 
            "text": "RediSearch supports stemming - that is adding the base form of a word to the index. This allows \nthe query for \"going\" to also return results for \"go\" and \"gone\", for example.   The current stemming support is based on the Snowball stemmer library, which supports most European\nlanguages, as well as Arabic and other. We hope to include more languages soon (if you need a specicif\nlangauge support, please open an issue).   For further details see the  Snowball Stemmer website .", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#supported\"_\"languages", 
            "text": "The following languages are supported, and can be passed to the engine \nwhen indexing or querying, with lowercase letters:   arabic  danish  dutch  english  finnish  french  german  hungarian  italian  norwegian  portuguese  romanian  russian  spanish  swedish  tamil  turkish  chinese (see below)", 
            "title": "Supported languages:"
        }, 
        {
            "location": "/Stemming/#chinese\"_\"support", 
            "text": "Indexing a Chinese document is different than indexing a document in most other\nlanguages because of how tokens are extracted. While most languages can have\ntheir tokens distinguished by separation characters and whitespace, this\nis not common in Chinese.  Chinese tokenization is done by scanning the input text and checking every\ncharacter or sequence of characters against a dictionary of predefined terms\nand determining the most likely (based on the surrounding terms and characters)\nmatch.  Redis Search makes use of the  Friso \nchinese tokenization library for this purpose. This is largely transparent to\nthe user and often no additional configuration is required.", 
            "title": "Chinese support"
        }, 
        {
            "location": "/Stemming/#using\"_\"custom\"_\"dictionaries", 
            "text": "If you wish to use a custom dictionary, you can do so at the module level when\nloading the module. The  FRISOINI  setting can point to the location of a friso.ini  file which contains the relevant settings and paths to the dictionary\nfiles.  Note that there is no \"default\" friso.ini file location. Redis Search comes with\nits own  friso.ini  and dictionary files which are compiled into the module\nbinary at build-time.", 
            "title": "Using custom dictionaries"
        }, 
        {
            "location": "/payloads/", 
            "text": "Document Payloads\n\n\nUsually, redisearch stores documents as HASH keys. But if you want to access some data for \naggregation or scoring functions, we might want to store that data as an inline payload. \nThis will allow us to evaluate properties of a document for scoring purposes at very low cost.\n\n\nSince the scoring functions already have access to the DocumentMetaData, which contains document flags and score,\nWe can add custom payloads that can be evaluated in run-time.\n\n\nPayloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose \nof evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, \nif you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.\n\n\nAdding payloads for documents:\n\n\nWhen inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload.\nThis is done with the PAYLOAD keyword:\n\n\nFT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...\n\n\n\n\n\nEvaluating Payloads in Query Time\n\n\nWhen imlplementing a scoring function, the signature of the function exposed is:\n\n\ndouble\n \n(\n*\nScoringFunction\n)(\nDocumentMetadata\n \n*\ndmd\n,\n \nIndexResult\n \n*\nh\n);\n\n\n\n\n\n\n\n\nNOTE: currently scoring functions cannot be dynamically added, and forking the engine and replacing them is required.\n\n\n\n\nDocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with\narbitrary length:\n\n\ntypedef\n \nstruct\n  \n{\n\n    \nchar\n \n*\ndata\n,\n\n    \nuint32_t\n \nlen\n;\n\n\n}\n \nDocumentPayload\n;\n\n\n\n\n\n\nIf no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it.\nIt is recommended to encode some metadata about the payload inside it, like a leading version number, etc.\n\n\nRetrieving payloads from documents\n\n\nWhen searching, it is possible to request the document payloads from the engine. \n\n\nThis is done by adding the keyword \nWITHPAYLOADS\n to \nFT.SEARCH\n. \n\n\nIf \nWITHPAYLOADS\n is set, the payloads follow the document id in the returned result. \nIf \nWITHSCORES\n is set as well, the payloads follow the scores. e.g.:\n\n\n127.0.0.1:6379\n FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379\n FT.ADD foo doc2 1.0 PAYLOAD \nhi there!\n FIELDS bar \nhello\n\nOK\n127.0.0.1:6379\n FT.SEARCH foo \nhello\n WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2) \ndoc2\n           # id\n3) \n1\n              # score\n4) \nhi there!\n      # payload\n5) 1) \nbar\n         # fields\n   2) \nhello", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#document\"_\"payloads", 
            "text": "Usually, redisearch stores documents as HASH keys. But if you want to access some data for \naggregation or scoring functions, we might want to store that data as an inline payload. \nThis will allow us to evaluate properties of a document for scoring purposes at very low cost.  Since the scoring functions already have access to the DocumentMetaData, which contains document flags and score,\nWe can add custom payloads that can be evaluated in run-time.  Payloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose \nof evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, \nif you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#adding\"_\"payloads\"_\"for\"_\"documents", 
            "text": "When inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload.\nThis is done with the PAYLOAD keyword:  FT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...", 
            "title": "Adding payloads for documents:"
        }, 
        {
            "location": "/payloads/#evaluating\"_\"payloads\"_\"in\"_\"query\"_\"time", 
            "text": "When imlplementing a scoring function, the signature of the function exposed is:  double   ( * ScoringFunction )( DocumentMetadata   * dmd ,   IndexResult   * h );    NOTE: currently scoring functions cannot be dynamically added, and forking the engine and replacing them is required.   DocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with\narbitrary length:  typedef   struct    { \n     char   * data , \n     uint32_t   len ;  }   DocumentPayload ;   If no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it.\nIt is recommended to encode some metadata about the payload inside it, like a leading version number, etc.", 
            "title": "Evaluating Payloads in Query Time"
        }, 
        {
            "location": "/payloads/#retrieving\"_\"payloads\"_\"from\"_\"documents", 
            "text": "When searching, it is possible to request the document payloads from the engine.   This is done by adding the keyword  WITHPAYLOADS  to  FT.SEARCH .   If  WITHPAYLOADS  is set, the payloads follow the document id in the returned result. \nIf  WITHSCORES  is set as well, the payloads follow the scores. e.g.:  127.0.0.1:6379  FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379  FT.ADD foo doc2 1.0 PAYLOAD  hi there!  FIELDS bar  hello \nOK\n127.0.0.1:6379  FT.SEARCH foo  hello  WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2)  doc2            # id\n3)  1               # score\n4)  hi there!       # payload\n5) 1)  bar          # fields\n   2)  hello", 
            "title": "Retrieving payloads from documents"
        }, 
        {
            "location": "/Clients/", 
            "text": "RediSearch Client Libraries\n\n\nRediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages. \n\n\nWhile it is possible and simple to use the raw redis commands API, in most cases it's easier to just use a client library abstracting it. \n\n\nCurrently available Libraries\n\n\n\n\n\n\n\n\nLanguage\n\n\nLibrary\n\n\nAuthor\n\n\nLicense\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nredisearch-py\n\n\nRedis Labs\n\n\nBSD\n\n\nUsually the most up-to-date client library\n\n\n\n\n\n\nJava\n\n\nJRediSearch\n\n\nRedis Labs\n\n\nBSD\n\n\n\n\n\n\n\n\nGo\n\n\nredisearch-go\n\n\nRedis Labs\n\n\nBSD\n\n\nIncomplete API\n\n\n\n\n\n\nJavaScript\n\n\nRedRediSearch\n\n\nKyle J. Davis\n\n\nMIT\n\n\nPartial API, compatible with \nReds\n\n\n\n\n\n\nC#\n\n\nNRediSearch\n\n\nMarc Gravell\n\n\nMIT\n\n\nPart of StackExchange.Redis\n\n\n\n\n\n\nPHP\n\n\nredisearch-php\n\n\nEthan Hann\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby on Rails\n\n\nredi_search_rails\n\n\nDmitry Polyakovsky\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby\n\n\nredisearch-rb\n\n\nVictor Ruiz\n\n\nMIT", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/Clients/#redisearch\"_\"client\"_\"libraries", 
            "text": "RediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages.   While it is possible and simple to use the raw redis commands API, in most cases it's easier to just use a client library abstracting it.", 
            "title": "RediSearch Client Libraries"
        }, 
        {
            "location": "/Clients/#currently\"_\"available\"_\"libraries", 
            "text": "Language  Library  Author  License  Comments      Python  redisearch-py  Redis Labs  BSD  Usually the most up-to-date client library    Java  JRediSearch  Redis Labs  BSD     Go  redisearch-go  Redis Labs  BSD  Incomplete API    JavaScript  RedRediSearch  Kyle J. Davis  MIT  Partial API, compatible with  Reds    C#  NRediSearch  Marc Gravell  MIT  Part of StackExchange.Redis    PHP  redisearch-php  Ethan Hann  MIT     Ruby on Rails  redi_search_rails  Dmitry Polyakovsky  MIT     Ruby  redisearch-rb  Victor Ruiz  MIT", 
            "title": "Currently available Libraries"
        }, 
        {
            "location": "/python_client/", 
            "text": "Package redisearch Documentation\n\n\nOverview\n\n\nredisearch-py\n is a python search engine library that utilizes the RediSearch Redis Module API.\n\n\nIt is the \"official\" client of redisearch, and should be regarded as its canonical client implementation.\n\n\nThe source code can be found at \nhttp://github.com/RedisLabs/redisearch-py\n\n\nExample: Using the Python Client\n\n\nfrom\n \nredisearch\n \nimport\n \nClient\n,\n \nTextField\n,\n \nNumericField\n,\n \nQuery\n\n\n\n# Creating a client with a given index name\n\n\nclient\n \n=\n \nClient\n(\nmyIndex\n)\n\n\n\n# Creating the index definition and schema\n\n\nclient\n.\ncreate_index\n([\nTextField\n(\ntitle\n,\n \nweight\n=\n5.0\n),\n \nTextField\n(\nbody\n)])\n\n\n\n# Indexing a document\n\n\nclient\n.\nadd_document\n(\ndoc1\n,\n \ntitle\n \n=\n \nRediSearch\n,\n \nbody\n \n=\n \nRedisearch impements a search engine on top of redis\n)\n\n\n\n# Simple search\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n)\n\n\n\n# the result has the total number of results, and a list of documents\n\n\nprint\n \nres\n.\ntotal\n \n# \n1\n\n\nprint\n \nres\n.\ndocs\n[\n0\n]\n.\ntitle\n \n\n\n# Searching with snippets\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n,\n \nsnippet_sizes\n \n=\n \n{\nbody\n:\n \n50\n})\n\n\n\n# Searching with complext parameters:\n\n\nq\n \n=\n \nQuery\n(\nsearch engine\n)\n.\nverbatim\n()\n.\nno_content\n()\n.\npaging\n(\n0\n,\n5\n)\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nq\n)\n\n\n\n\n\n\nExample: Using the Auto Completer Client:\n\n\n# Using the auto-completer\n\n\nac\n \n=\n \nAutoCompleter\n(\nac\n)\n\n\n\n# Adding some terms\n\n\nac\n.\nadd_suggestions\n(\nSuggestion\n(\nfoo\n,\n \n5.0\n),\n \nSuggestion\n(\nbar\n,\n \n1.0\n))\n\n\n\n# Getting suggestions\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n)\n \n# returns nothing\n\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n,\n \nfuzzy\n \n=\n \nTrue\n)\n \n# returns [\nfoo\n]\n\n\n\n\n\n\nInstalling\n\n\n\n\n\n\nInstall redis 4.0 RC2 or above\n\n\n\n\n\n\nInstall RediSearch\n\n\n\n\n\n\nInstall the python client\n\n\n\n\n\n\n$ pip install redisearch\n\n\n\n\n\nClass AutoCompleter\n\n\nA client to RediSearch's AutoCompleter API\n\n\nIt provides prefix searches with optionally fuzzy matching of prefixes    \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nkey\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new AutoCompleter client for the given key, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_suggestions\n\n\ndef\n \nadd_suggestions\n(\nself\n,\n \n*\nsuggestions\n,\n \n**\nkwargs\n)\n\n\n\n\n\n\nAdd suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.\n\n\nIf kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores\n\n\ndelete\n\n\ndef\n \ndelete\n(\nself\n,\n \nstring\n)\n\n\n\n\n\n\nDelete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise\n\n\nget_suggestions\n\n\ndef\n \nget_suggestions\n(\nself\n,\n \nprefix\n,\n \nfuzzy\n=\nFalse\n,\n \nnum\n=\n10\n,\n \nwith_scores\n=\nFalse\n,\n \nwith_payloads\n=\nFalse\n)\n\n\n\n\n\n\nGet a list of suggestions from the AutoCompleter, for a given prefix\n\n\nParameters:\n\n\n\n\nprefix\n: the prefix we are searching. \nMust be valid ascii or utf-8\n\n\nfuzzy\n: If set to true, the prefix search is done in fuzzy mode. \n    \nNOTE\n: Running fuzzy searches on short (\n3 letters) prefixes can be very slow, and even scan the entire index.\n\n\nwith_scores\n: if set to true, we also return the (refactored) score of each suggestion. \n  This is normally not needed, and is NOT the original score inserted into the index\n\n\nwith_payloads\n: Return suggestion payloads\n\n\nnum\n: The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.\n\n\n\n\nReturns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.\n\n\nlen\n\n\ndef\n \nlen\n(\nself\n)\n\n\n\n\n\n\nReturn the number of entries in the AutoCompleter index\n\n\nClass Client\n\n\nA client for the RediSearch module. \nIt abstracts the API of the module and lets you just use the engine \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nindex_name\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new Client for the given index_name, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \npartial\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a single document to the index.\n\n\nParameters\n\n\n\n\ndoc_id\n: the id of the saved document.\n\n\nnosave\n: if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.\n\n\nscore\n: the document ranking, between 0.0 and 1.0 \n\n\npayload\n: optional inner-index payload we can save for fast access in scoring functions\n\n\nreplace\n: if True, and the document already is in the index, we perform an update and reindex the document\n\n\npartial\n: if True, the fields specified will be added to the existing document.\n               This has the added benefit that any fields specified with \nno_index\n\n               will not be reindexed again. Implies \nreplace\n\n\nfields\n kwargs dictionary of the document fields to be saved and/or indexed. \n             NOTE: Geo points shoule be encoded as strings of \"lon,lat\"\n\n\n\n\nbatch_indexer\n\n\ndef\n \nbatch_indexer\n(\nself\n,\n \nchunk_size\n=\n100\n)\n\n\n\n\n\n\nCreate a new batch indexer from the client with a given chunk size\n\n\ncreate_index\n\n\ndef\n \ncreate_index\n(\nself\n,\n \nfields\n,\n \nno_term_offsets\n=\nFalse\n,\n \nno_field_flags\n=\nFalse\n,\n \nstopwords\n=\nNone\n)\n\n\n\n\n\n\nCreate the search index. Creating an existing index juts updates its properties\n\n\nParameters:\n\n\n\n\nfields\n: a list of TextField or NumericField objects\n\n\nno_term_offsets\n: If true, we will not save term offsets in the index\n\n\nno_field_flags\n: If true, we will not save field flags that allow searching in specific fields\n\n\nstopwords\n: If not None, we create the index with this custom stopword list. The list can be empty\n\n\n\n\ndelete_document\n\n\ndef\n \ndelete_document\n(\nself\n,\n \ndoc_id\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nDelete a document from index\nReturns 1 if the document was deleted, 0 if not\n\n\ndrop_index\n\n\ndef\n \ndrop_index\n(\nself\n)\n\n\n\n\n\n\nDrop the index if it exists\n\n\nexplain\n\n\ndef\n \nexplain\n(\nself\n,\n \nquery\n)\n\n\n\n\n\n\ninfo\n\n\ndef\n \ninfo\n(\nself\n)\n\n\n\n\n\n\nGet info an stats about the the current index, including the number of documents, memory consumption, etc\n\n\nload_document\n\n\ndef\n \nload_document\n(\nself\n,\n \nid\n)\n\n\n\n\n\n\nLoad a single document by id\n\n\nsearch\n\n\ndef\n \nsearch\n(\nself\n,\n \nquery\n)\n\n\n\n\n\n\nSearch the index for a given query, and return a result of documents\n\n\nParameters\n\n\n\n\nquery\n: the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format\n\n\nsnippet_sizes\n: A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}\n\n\n\n\nClass BatchIndexer\n\n\nA batch indexer allows you to automatically batch \ndocument indexeing in pipelines, flushing it every N documents. \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nclient\n,\n \nchunk_size\n=\n1000\n)\n\n\n\n\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \npartial\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a document to the batch query\n\n\ncommit\n\n\ndef\n \ncommit\n(\nself\n)\n\n\n\n\n\n\nManually commit and flush the batch indexing query\n\n\nClass Document\n\n\nRepresents a single document in a result set \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nid\n,\n \npayload\n=\nNone\n,\n \n**\nfields\n)\n\n\n\n\n\n\nClass GeoField\n\n\nGeoField is used to define a geo-indexing field in a schema defintion\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass GeoFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nlon\n,\n \nlat\n,\n \nradius\n,\n \nunit\n=\nkm\n)\n\n\n\n\n\n\nClass NumericField\n\n\nNumericField is used to define a numeric field in a schema defintion\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nsortable\n=\nFalse\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass NumericFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nminval\n,\n \nmaxval\n,\n \nminExclusive\n=\nFalse\n,\n \nmaxExclusive\n=\nFalse\n)\n\n\n\n\n\n\nClass Query\n\n\nQuery is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.\n\n\nThe setter functions return the query object, so they can be chained, \ni.e. \nQuery(\"foo\").verbatim().filter(...)\n etc.\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nquery_string\n)\n\n\n\n\n\n\nCreate a new query object. \nThe query string is set in the constructor, and other options have setter functions.\n\n\nadd_filter\n\n\ndef\n \nadd_filter\n(\nself\n,\n \nflt\n)\n\n\n\n\n\n\nAdd a numeric or geo filter to the query. \n\nCurrently only one of each filter is supported by the engine\n\n\n\n\nflt\n: A NumericFilter or GeoFilter object, used on a corresponding field\n\n\n\n\nget_args\n\n\ndef\n \nget_args\n(\nself\n)\n\n\n\n\n\n\nFormat the redis arguments for this query and return them\n\n\nhighlight\n\n\ndef\n \nhighlight\n(\nself\n,\n \nfields\n=\nNone\n,\n \ntags\n=\nNone\n)\n\n\n\n\n\n\nApply specified markup to matched term(s) within the returned field(s)\n\n\n\n\nfields\n If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted\n\n\ntags\n A list of two strings to surround the match.\n\n\n\n\nin_order\n\n\ndef\n \nin_order\n(\nself\n)\n\n\n\n\n\n\nMatch only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'\n\n\nlimit_fields\n\n\ndef\n \nlimit_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nLimit the search to specific TEXT fields only\n\n\n\n\nfields\n: A list of strings, case sensitive field names from the defined schema\n\n\n\n\nlimit_ids\n\n\ndef\n \nlimit_ids\n(\nself\n,\n \n*\nids\n)\n\n\n\n\n\n\nLimit the results to a specific set of pre-known document ids of any length\n\n\nno_content\n\n\ndef\n \nno_content\n(\nself\n)\n\n\n\n\n\n\nSet the query to only return ids and not the document content\n\n\nno_stopwords\n\n\ndef\n \nno_stopwords\n(\nself\n)\n\n\n\n\n\n\nPrevent the query from being filtered for stopwords. \nOnly useful in very big queries that you are certain contain no stopwords.\n\n\npaging\n\n\ndef\n \npaging\n(\nself\n,\n \noffset\n,\n \nnum\n)\n\n\n\n\n\n\nSet the paging for the query (defaults to 0..10).\n\n\n\n\noffset\n: Paging offset for the results. Defaults to 0\n\n\nnum\n: How many results do we want\n\n\n\n\nquery_string\n\n\ndef\n \nquery_string\n(\nself\n)\n\n\n\n\n\n\nReturn the query string of this query only\n\n\nreturn_fields\n\n\ndef\n \nreturn_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nOnly return values from these fields\n\n\nslop\n\n\ndef\n \nslop\n(\nself\n,\n \nslop\n)\n\n\n\n\n\n\nAllow a masimum of N intervening non matched terms between phrase terms (0 means exact phrase)\n\n\nsort_by\n\n\ndef\n \nsort_by\n(\nself\n,\n \nfield\n,\n \nasc\n=\nTrue\n)\n\n\n\n\n\n\nAdd a sortby field to the query\n\n\n\n\nfield\n - the name of the field to sort by\n\n\nasc\n - when \nTrue\n, sorting will be done in asceding order\n\n\n\n\nsummarize\n\n\ndef\n \nsummarize\n(\nself\n,\n \nfields\n=\nNone\n,\n \ncontext_len\n=\nNone\n,\n \nnum_frags\n=\nNone\n,\n \nsep\n=\nNone\n)\n\n\n\n\n\n\nReturn an abridged format of the field, containing only the segments of\nthe field which contain the matching term(s).\n\n\nIf \nfields\n is specified, then only the mentioned fields are\nsummarized; otherwise all results are summarized.\n\n\nServer side defaults are used for each option (except \nfields\n) if not specified\n\n\n\n\nfields\n List of fields to summarize. All fields are summarized if not specified\n\n\ncontext_len\n Amount of context to include with each fragment\n\n\nnum_frags\n Number of fragments per document\n\n\nsep\n Separator string to separate fragments\n\n\n\n\nverbatim\n\n\ndef\n \nverbatim\n(\nself\n)\n\n\n\n\n\n\nSet the query to be verbatim, i.e. use no query expansion or stemming\n\n\nwith_payloads\n\n\ndef\n \nwith_payloads\n(\nself\n)\n\n\n\n\n\n\nAsk the engine to return document payloads\n\n\nClass Result\n\n\nRepresents the result of a search query, and has an array of Document objects\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nres\n,\n \nhascontent\n,\n \nduration\n=\n0\n,\n \nhas_payload\n=\nFalse\n)\n\n\n\n\n\n\n\n\nsnippets\n: An optional dictionary of the form {field: snippet_size} for snippet formatting\n\n\n\n\nClass SortbyField\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nasc\n=\nTrue\n)\n\n\n\n\n\n\nClass Suggestion\n\n\nRepresents a single suggestion being sent or returned from the auto complete server\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nstring\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n)\n\n\n\n\n\n\nClass TagField\n\n\nTagField is a tag-indexing field with simpler compression and tokenization. \nSee http://redisearch.io/Tags/\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nseparator\n=\n,\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass TextField\n\n\nTextField is used to define a text field in a schema definition\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nweight\n=\n1.0\n,\n \nsortable\n=\nFalse\n,\n \nno_stem\n=\nFalse\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)", 
            "title": "Python API"
        }, 
        {
            "location": "/python_client/#package\"_\"redisearch\"_\"documentation", 
            "text": "", 
            "title": "Package redisearch Documentation"
        }, 
        {
            "location": "/python_client/#overview", 
            "text": "redisearch-py  is a python search engine library that utilizes the RediSearch Redis Module API.  It is the \"official\" client of redisearch, and should be regarded as its canonical client implementation.  The source code can be found at  http://github.com/RedisLabs/redisearch-py", 
            "title": "Overview"
        }, 
        {
            "location": "/python_client/#example\"_\"using\"_\"the\"_\"python\"_\"client", 
            "text": "from   redisearch   import   Client ,   TextField ,   NumericField ,   Query  # Creating a client with a given index name  client   =   Client ( myIndex )  # Creating the index definition and schema  client . create_index ([ TextField ( title ,   weight = 5.0 ),   TextField ( body )])  # Indexing a document  client . add_document ( doc1 ,   title   =   RediSearch ,   body   =   Redisearch impements a search engine on top of redis )  # Simple search  res   =   client . search ( search engine )  # the result has the total number of results, and a list of documents  print   res . total   #  1  print   res . docs [ 0 ] . title   # Searching with snippets  res   =   client . search ( search engine ,   snippet_sizes   =   { body :   50 })  # Searching with complext parameters:  q   =   Query ( search engine ) . verbatim () . no_content () . paging ( 0 , 5 )  res   =   client . search ( q )", 
            "title": "Example: Using the Python Client"
        }, 
        {
            "location": "/python_client/#example\"_\"using\"_\"the\"_\"auto\"_\"completer\"_\"client", 
            "text": "# Using the auto-completer  ac   =   AutoCompleter ( ac )  # Adding some terms  ac . add_suggestions ( Suggestion ( foo ,   5.0 ),   Suggestion ( bar ,   1.0 ))  # Getting suggestions  suggs   =   ac . get_suggestions ( goo )   # returns nothing  suggs   =   ac . get_suggestions ( goo ,   fuzzy   =   True )   # returns [ foo ]", 
            "title": "Example: Using the Auto Completer Client:"
        }, 
        {
            "location": "/python_client/#installing", 
            "text": "Install redis 4.0 RC2 or above    Install RediSearch    Install the python client    $ pip install redisearch", 
            "title": "Installing"
        }, 
        {
            "location": "/python_client/#class\"_\"autocompleter", 
            "text": "A client to RediSearch's AutoCompleter API  It provides prefix searches with optionally fuzzy matching of prefixes", 
            "title": "Class AutoCompleter"
        }, 
        {
            "location": "/python_client/#9595init9595", 
            "text": "def   __init__ ( self ,   key ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new AutoCompleter client for the given key, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95suggestions", 
            "text": "def   add_suggestions ( self ,   * suggestions ,   ** kwargs )   Add suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.  If kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores", 
            "title": "add_suggestions"
        }, 
        {
            "location": "/python_client/#delete", 
            "text": "def   delete ( self ,   string )   Delete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise", 
            "title": "delete"
        }, 
        {
            "location": "/python_client/#get95suggestions", 
            "text": "def   get_suggestions ( self ,   prefix ,   fuzzy = False ,   num = 10 ,   with_scores = False ,   with_payloads = False )   Get a list of suggestions from the AutoCompleter, for a given prefix", 
            "title": "get_suggestions"
        }, 
        {
            "location": "/python_client/#parameters", 
            "text": "prefix : the prefix we are searching.  Must be valid ascii or utf-8  fuzzy : If set to true, the prefix search is done in fuzzy mode. \n     NOTE : Running fuzzy searches on short ( 3 letters) prefixes can be very slow, and even scan the entire index.  with_scores : if set to true, we also return the (refactored) score of each suggestion. \n  This is normally not needed, and is NOT the original score inserted into the index  with_payloads : Return suggestion payloads  num : The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.   Returns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#len", 
            "text": "def   len ( self )   Return the number of entries in the AutoCompleter index", 
            "title": "len"
        }, 
        {
            "location": "/python_client/#class\"_\"client", 
            "text": "A client for the RediSearch module. \nIt abstracts the API of the module and lets you just use the engine", 
            "title": "Class Client"
        }, 
        {
            "location": "/python_client/#9595init9595_1", 
            "text": "def   __init__ ( self ,   index_name ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new Client for the given index_name, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   partial = False ,   ** fields )   Add a single document to the index.", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#parameters_1", 
            "text": "doc_id : the id of the saved document.  nosave : if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.  score : the document ranking, between 0.0 and 1.0   payload : optional inner-index payload we can save for fast access in scoring functions  replace : if True, and the document already is in the index, we perform an update and reindex the document  partial : if True, the fields specified will be added to the existing document.\n               This has the added benefit that any fields specified with  no_index \n               will not be reindexed again. Implies  replace  fields  kwargs dictionary of the document fields to be saved and/or indexed. \n             NOTE: Geo points shoule be encoded as strings of \"lon,lat\"", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#batch95indexer", 
            "text": "def   batch_indexer ( self ,   chunk_size = 100 )   Create a new batch indexer from the client with a given chunk size", 
            "title": "batch_indexer"
        }, 
        {
            "location": "/python_client/#create95index", 
            "text": "def   create_index ( self ,   fields ,   no_term_offsets = False ,   no_field_flags = False ,   stopwords = None )   Create the search index. Creating an existing index juts updates its properties", 
            "title": "create_index"
        }, 
        {
            "location": "/python_client/#parameters_2", 
            "text": "fields : a list of TextField or NumericField objects  no_term_offsets : If true, we will not save term offsets in the index  no_field_flags : If true, we will not save field flags that allow searching in specific fields  stopwords : If not None, we create the index with this custom stopword list. The list can be empty", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#delete95document", 
            "text": "def   delete_document ( self ,   doc_id ,   conn = None )   Delete a document from index\nReturns 1 if the document was deleted, 0 if not", 
            "title": "delete_document"
        }, 
        {
            "location": "/python_client/#drop95index", 
            "text": "def   drop_index ( self )   Drop the index if it exists", 
            "title": "drop_index"
        }, 
        {
            "location": "/python_client/#explain", 
            "text": "def   explain ( self ,   query )", 
            "title": "explain"
        }, 
        {
            "location": "/python_client/#info", 
            "text": "def   info ( self )   Get info an stats about the the current index, including the number of documents, memory consumption, etc", 
            "title": "info"
        }, 
        {
            "location": "/python_client/#load95document", 
            "text": "def   load_document ( self ,   id )   Load a single document by id", 
            "title": "load_document"
        }, 
        {
            "location": "/python_client/#search", 
            "text": "def   search ( self ,   query )   Search the index for a given query, and return a result of documents", 
            "title": "search"
        }, 
        {
            "location": "/python_client/#parameters_3", 
            "text": "query : the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format  snippet_sizes : A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#class\"_\"batchindexer", 
            "text": "A batch indexer allows you to automatically batch \ndocument indexeing in pipelines, flushing it every N documents.", 
            "title": "Class BatchIndexer"
        }, 
        {
            "location": "/python_client/#9595init9595_2", 
            "text": "def   __init__ ( self ,   client ,   chunk_size = 1000 )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document_1", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   partial = False ,   ** fields )   Add a document to the batch query", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#commit", 
            "text": "def   commit ( self )   Manually commit and flush the batch indexing query", 
            "title": "commit"
        }, 
        {
            "location": "/python_client/#class\"_\"document", 
            "text": "Represents a single document in a result set", 
            "title": "Class Document"
        }, 
        {
            "location": "/python_client/#9595init9595_3", 
            "text": "def   __init__ ( self ,   id ,   payload = None ,   ** fields )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"geofield", 
            "text": "GeoField is used to define a geo-indexing field in a schema defintion", 
            "title": "Class GeoField"
        }, 
        {
            "location": "/python_client/#9595init9595_4", 
            "text": "def   __init__ ( self ,   name )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"geofilter", 
            "text": "None", 
            "title": "Class GeoFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_5", 
            "text": "def   __init__ ( self ,   field ,   lon ,   lat ,   radius ,   unit = km )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"numericfield", 
            "text": "NumericField is used to define a numeric field in a schema defintion", 
            "title": "Class NumericField"
        }, 
        {
            "location": "/python_client/#9595init9595_6", 
            "text": "def   __init__ ( self ,   name ,   sortable = False ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_1", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"numericfilter", 
            "text": "None", 
            "title": "Class NumericFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_7", 
            "text": "def   __init__ ( self ,   field ,   minval ,   maxval ,   minExclusive = False ,   maxExclusive = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"query", 
            "text": "Query is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.  The setter functions return the query object, so they can be chained, \ni.e.  Query(\"foo\").verbatim().filter(...)  etc.", 
            "title": "Class Query"
        }, 
        {
            "location": "/python_client/#9595init9595_8", 
            "text": "def   __init__ ( self ,   query_string )   Create a new query object. \nThe query string is set in the constructor, and other options have setter functions.", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95filter", 
            "text": "def   add_filter ( self ,   flt )   Add a numeric or geo filter to the query.  Currently only one of each filter is supported by the engine   flt : A NumericFilter or GeoFilter object, used on a corresponding field", 
            "title": "add_filter"
        }, 
        {
            "location": "/python_client/#get95args", 
            "text": "def   get_args ( self )   Format the redis arguments for this query and return them", 
            "title": "get_args"
        }, 
        {
            "location": "/python_client/#highlight", 
            "text": "def   highlight ( self ,   fields = None ,   tags = None )   Apply specified markup to matched term(s) within the returned field(s)   fields  If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted  tags  A list of two strings to surround the match.", 
            "title": "highlight"
        }, 
        {
            "location": "/python_client/#in95order", 
            "text": "def   in_order ( self )   Match only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'", 
            "title": "in_order"
        }, 
        {
            "location": "/python_client/#limit95fields", 
            "text": "def   limit_fields ( self ,   * fields )   Limit the search to specific TEXT fields only   fields : A list of strings, case sensitive field names from the defined schema", 
            "title": "limit_fields"
        }, 
        {
            "location": "/python_client/#limit95ids", 
            "text": "def   limit_ids ( self ,   * ids )   Limit the results to a specific set of pre-known document ids of any length", 
            "title": "limit_ids"
        }, 
        {
            "location": "/python_client/#no95content", 
            "text": "def   no_content ( self )   Set the query to only return ids and not the document content", 
            "title": "no_content"
        }, 
        {
            "location": "/python_client/#no95stopwords", 
            "text": "def   no_stopwords ( self )   Prevent the query from being filtered for stopwords. \nOnly useful in very big queries that you are certain contain no stopwords.", 
            "title": "no_stopwords"
        }, 
        {
            "location": "/python_client/#paging", 
            "text": "def   paging ( self ,   offset ,   num )   Set the paging for the query (defaults to 0..10).   offset : Paging offset for the results. Defaults to 0  num : How many results do we want", 
            "title": "paging"
        }, 
        {
            "location": "/python_client/#query95string", 
            "text": "def   query_string ( self )   Return the query string of this query only", 
            "title": "query_string"
        }, 
        {
            "location": "/python_client/#return95fields", 
            "text": "def   return_fields ( self ,   * fields )   Only return values from these fields", 
            "title": "return_fields"
        }, 
        {
            "location": "/python_client/#slop", 
            "text": "def   slop ( self ,   slop )   Allow a masimum of N intervening non matched terms between phrase terms (0 means exact phrase)", 
            "title": "slop"
        }, 
        {
            "location": "/python_client/#sort95by", 
            "text": "def   sort_by ( self ,   field ,   asc = True )   Add a sortby field to the query   field  - the name of the field to sort by  asc  - when  True , sorting will be done in asceding order", 
            "title": "sort_by"
        }, 
        {
            "location": "/python_client/#summarize", 
            "text": "def   summarize ( self ,   fields = None ,   context_len = None ,   num_frags = None ,   sep = None )   Return an abridged format of the field, containing only the segments of\nthe field which contain the matching term(s).  If  fields  is specified, then only the mentioned fields are\nsummarized; otherwise all results are summarized.  Server side defaults are used for each option (except  fields ) if not specified   fields  List of fields to summarize. All fields are summarized if not specified  context_len  Amount of context to include with each fragment  num_frags  Number of fragments per document  sep  Separator string to separate fragments", 
            "title": "summarize"
        }, 
        {
            "location": "/python_client/#verbatim", 
            "text": "def   verbatim ( self )   Set the query to be verbatim, i.e. use no query expansion or stemming", 
            "title": "verbatim"
        }, 
        {
            "location": "/python_client/#with95payloads", 
            "text": "def   with_payloads ( self )   Ask the engine to return document payloads", 
            "title": "with_payloads"
        }, 
        {
            "location": "/python_client/#class\"_\"result", 
            "text": "Represents the result of a search query, and has an array of Document objects", 
            "title": "Class Result"
        }, 
        {
            "location": "/python_client/#9595init9595_9", 
            "text": "def   __init__ ( self ,   res ,   hascontent ,   duration = 0 ,   has_payload = False )    snippets : An optional dictionary of the form {field: snippet_size} for snippet formatting", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"sortbyfield", 
            "text": "None", 
            "title": "Class SortbyField"
        }, 
        {
            "location": "/python_client/#9595init9595_10", 
            "text": "def   __init__ ( self ,   field ,   asc = True )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"suggestion", 
            "text": "Represents a single suggestion being sent or returned from the auto complete server", 
            "title": "Class Suggestion"
        }, 
        {
            "location": "/python_client/#9595init9595_11", 
            "text": "def   __init__ ( self ,   string ,   score = 1.0 ,   payload = None )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"tagfield", 
            "text": "TagField is a tag-indexing field with simpler compression and tokenization. \nSee http://redisearch.io/Tags/", 
            "title": "Class TagField"
        }, 
        {
            "location": "/python_client/#9595init9595_12", 
            "text": "def   __init__ ( self ,   name ,   separator = , ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_2", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"textfield", 
            "text": "TextField is used to define a text field in a schema definition", 
            "title": "Class TextField"
        }, 
        {
            "location": "/python_client/#9595init9595_13", 
            "text": "def   __init__ ( self ,   name ,   weight = 1.0 ,   sortable = False ,   no_stem = False ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_3", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/java_client/", 
            "text": "JRediSearch - RediSearch Java Client\n\n\nhttps://github.com/RedisLabs/JRediSearch\n\n\nOverview\n\n\nJRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis. \n\n\nSee full documentation at \nhttps://github.com/RedisLabs/JRediSearch\n.\n\n\nUsage example\n\n\nInitializing the client:\n\n\nimport\n \nio.redisearch.client.Client\n;\n\n\nimport\n \nio.redisearch.Document\n;\n\n\nimport\n \nio.redisearch.SearchResult\n;\n\n\nimport\n \nio.redisearch.Query\n;\n\n\nimport\n \nio.redisearch.Schema\n;\n\n\n\n...\n\n\n\nClient\n \nclient\n \n=\n \nnew\n \nClient\n(\ntestung\n,\n \nlocalhost\n,\n \n6379\n);\n\n\n\n\n\n\nDefining a schema for an index and creating it:\n\n\nSchema\n \nsc\n \n=\n \nnew\n \nSchema\n()\n\n                \n.\naddTextField\n(\ntitle\n,\n \n5.0\n)\n\n                \n.\naddTextField\n(\nbody\n,\n \n1.0\n)\n\n                \n.\naddNumericField\n(\nprice\n);\n\n\n\nclient\n.\ncreateIndex\n(\nsc\n,\n \nClient\n.\nIndexOptions\n.\nDefault\n());\n\n\n\n\n\n\nAdding documents to the index:\n\n\nMap\nString\n,\n \nObject\n \nfields\n \n=\n \nnew\n \nHashMap\n();\n\n\nfields\n.\nput\n(\ntitle\n,\n \nhello world\n);\n\n\nfields\n.\nput\n(\nbody\n,\n \nlorem ipsum\n);\n\n\nfields\n.\nput\n(\nprice\n,\n \n1337\n);\n\n\n\nclient\n.\naddDocument\n(\ndoc1\n,\n \nfields\n);\n\n\n\n\n\n\nSearching the index:\n\n\n// Creating a complex query\n\n\nQuery\n \nq\n \n=\n \nnew\n \nQuery\n(\nhello world\n)\n\n                    \n.\naddFilter\n(\nnew\n \nQuery\n.\nNumericFilter\n(\nprice\n,\n \n0\n,\n \n1000\n))\n\n                    \n.\nlimit\n(\n0\n,\n5\n);\n\n\n\n// actual search\n\n\nSearchResult\n \nres\n \n=\n \nclient\n.\nsearch\n(\nq\n);", 
            "title": "Java API"
        }, 
        {
            "location": "/java_client/#jredisearch\"_\"-\"_\"redisearch\"_\"java\"_\"client", 
            "text": "https://github.com/RedisLabs/JRediSearch", 
            "title": "JRediSearch - RediSearch Java Client"
        }, 
        {
            "location": "/java_client/#overview", 
            "text": "JRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis.   See full documentation at  https://github.com/RedisLabs/JRediSearch .", 
            "title": "Overview"
        }, 
        {
            "location": "/java_client/#usage\"_\"example", 
            "text": "Initializing the client:  import   io.redisearch.client.Client ;  import   io.redisearch.Document ;  import   io.redisearch.SearchResult ;  import   io.redisearch.Query ;  import   io.redisearch.Schema ;  ...  Client   client   =   new   Client ( testung ,   localhost ,   6379 );   Defining a schema for an index and creating it:  Schema   sc   =   new   Schema () \n                 . addTextField ( title ,   5.0 ) \n                 . addTextField ( body ,   1.0 ) \n                 . addNumericField ( price );  client . createIndex ( sc ,   Client . IndexOptions . Default ());   Adding documents to the index:  Map String ,   Object   fields   =   new   HashMap ();  fields . put ( title ,   hello world );  fields . put ( body ,   lorem ipsum );  fields . put ( price ,   1337 );  client . addDocument ( doc1 ,   fields );   Searching the index:  // Creating a complex query  Query   q   =   new   Query ( hello world ) \n                     . addFilter ( new   Query . NumericFilter ( price ,   0 ,   1000 )) \n                     . limit ( 0 , 5 );  // actual search  SearchResult   res   =   client . search ( q );", 
            "title": "Usage example"
        }, 
        {
            "location": "/go_client/", 
            "text": "redisearch\n\n\n--\n    import \"github.com/RedisLabs/redisearch-go/redisearch\"\n\n\nPackage redisearch provides a Go client for the RediSearch search engine.\n\n\nFor the full documentation of RediSearch, see\n\nhttp://redisearch.io\n\n\nExample Usage\n\n\n    \nimport\n \n(\n\n      \ngithub.com/RedisLabs/redisearch-go/redisearch\n\n      \nlog\n\n      \nfmt\n\n    \n)\n\n\n    \nfunc\n \nExampleClient\n()\n \n{\n\n      \n// Create a client. By default a client is schemaless\n\n      \n// unless a schema is provided when creating the index\n\n      \nc\n \n:=\n \ncreateClient\n(\nmyIndex\n)\n\n\n      \n// Create a schema\n\n      \nsc\n \n:=\n \nredisearch\n.\nNewSchema\n(\nredisearch\n.\nDefaultOptions\n).\n\n        \nAddField\n(\nredisearch\n.\nNewTextField\n(\nbody\n)).\n\n        \nAddField\n(\nredisearch\n.\nNewTextFieldOptions\n(\ntitle\n,\n \nredisearch\n.\nTextFieldOptions\n{\nWeight\n:\n \n5.0\n,\n \nSortable\n:\n \ntrue\n})).\n\n        \nAddField\n(\nredisearch\n.\nNewNumericField\n(\ndate\n))\n\n\n      \n// Drop an existing index. If the index does not exist an error is returned\n\n      \nc\n.\nDrop\n()\n\n\n      \n// Create the index with the given schema\n\n      \nif\n \nerr\n \n:=\n \nc\n.\nCreateIndex\n(\nsc\n);\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n      \n}\n\n\n      \n// Create a document with an id and given score\n\n      \ndoc\n \n:=\n \nredisearch\n.\nNewDocument\n(\ndoc1\n,\n \n1.0\n)\n\n      \ndoc\n.\nSet\n(\ntitle\n,\n \nHello world\n).\n\n        \nSet\n(\nbody\n,\n \nfoo bar\n).\n\n        \nSet\n(\ndate\n,\n \ntime\n.\nNow\n().\nUnix\n())\n\n\n      \n// Index the document. The API accepts multiple documents at a time\n\n      \nif\n \nerr\n \n:=\n \nc\n.\nIndexOptions\n(\nredisearch\n.\nDefaultIndexingOptions\n,\n \ndoc\n);\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n      \n}\n\n\n      \n// Searching with limit and sorting\n\n      \ndocs\n,\n \ntotal\n,\n \nerr\n \n:=\n \nc\n.\nSearch\n(\nredisearch\n.\nNewQuery\n(\nhello world\n).\n\n        \nLimit\n(\n0\n,\n \n2\n).\n\n        \nSetReturnFields\n(\ntitle\n))\n\n\n      \nfmt\n.\nPrintln\n(\ndocs\n[\n0\n].\nId\n,\n \ndocs\n[\n0\n].\nProperties\n[\ntitle\n],\n \ntotal\n,\n \nerr\n)\n\n      \n// Output: doc1 Hello world 1 \nnil\n\n    \n}\n\n\n\n\n\n\nUsage\n\n\nvar\n \nDefaultIndexingOptions\n \n=\n \nIndexingOptions\n{\n\n    \nLanguage\n:\n \n,\n\n    \nNoSave\n:\n   \nfalse\n,\n\n    \nReplace\n:\n  \nfalse\n,\n\n    \nPartial\n:\n  \nfalse\n,\n\n\n}\n\n\n\n\n\n\nDefaultIndexingOptions are the default options for document indexing\n\n\nvar\n \nDefaultOptions\n \n=\n \nOptions\n{\n\n    \nNoSave\n:\n          \nfalse\n,\n\n    \nNoFieldFlags\n:\n    \nfalse\n,\n\n    \nNoFrequencies\n:\n   \nfalse\n,\n\n    \nNoOffsetVectors\n:\n \nfalse\n,\n\n    \nStopwords\n:\n       \nnil\n,\n\n\n}\n\n\n\n\n\n\nDefaultOptions represents the default options\n\n\ntype Autocompleter\n\n\ntype\n \nAutocompleter\n \nstruct\n \n{\n\n\n}\n\n\n\n\n\n\nAutocompleter implements a redisearch auto-completer API\n\n\nfunc  NewAutocompleter\n\n\nfunc\n \nNewAutocompleter\n(\naddr\n,\n \nname\n \nstring\n)\n \n*\nAutocompleter\n\n\n\n\n\n\nNewAutocompleter creates a new Autocompleter with the given host and key name\n\n\nfunc (*Autocompleter) AddTerms\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nAddTerms\n(\nterms\n \n...\nSuggestion\n)\n \nerror\n\n\n\n\n\n\nAddTerms pushes new term suggestions to the index\n\n\nfunc (*Autocompleter) Delete\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nDelete\n()\n \nerror\n\n\n\n\n\n\nDelete deletes the Autocompleter key for this AC\n\n\nfunc (*Autocompleter) Suggest\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nSuggest\n(\nprefix\n \nstring\n,\n \nnum\n \nint\n,\n \nfuzzy\n \nbool\n)\n \n([]\nSuggestion\n,\n \nerror\n)\n\n\n\n\n\n\nSuggest gets completion suggestions from the Autocompleter dictionary to the\ngiven prefix. If fuzzy is set, we also complete for prefixes that are in 1\nLevenshten distance from the given prefix\n\n\ntype Client\n\n\ntype\n \nClient\n \nstruct\n \n{\n\n\n}\n\n\n\n\n\n\nClient is an interface to redisearch's redis commands\n\n\nfunc  NewClient\n\n\nfunc\n \nNewClient\n(\naddr\n,\n \nname\n \nstring\n)\n \n*\nClient\n\n\n\n\n\n\nNewClient creates a new client connecting to the redis host, and using the given\nname as key prefix. Addr can be a single host:port pair, or a comma separated\nlist of host:port,host:port... In the case of multiple hosts we create a\nmulti-pool and select connections at random\n\n\nfunc (*Client) CreateIndex\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nCreateIndex\n(\ns\n \n*\nSchema\n)\n \nerror\n\n\n\n\n\n\nCreateIndex configues the index and creates it on redis\n\n\nfunc (*Client) Drop\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nDrop\n()\n \nerror\n\n\n\n\n\n\nDrop the Currentl just flushes the DB - note that this will delete EVERYTHING on\nthe redis instance\n\n\nfunc (*Client) Explain\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nExplain\n(\nq\n \n*\nQuery\n)\n \n(\nstring\n,\n \nerror\n)\n\n\n\n\n\n\nExplain Return a textual string explaining the query\n\n\nfunc (*Client) Index\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nIndex\n(\ndocs\n \n...\nDocument\n)\n \nerror\n\n\n\n\n\n\nIndex indexes a list of documents with the default options\n\n\nfunc (*Client) IndexOptions\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nIndexOptions\n(\nopts\n \nIndexingOptions\n,\n \ndocs\n \n...\nDocument\n)\n \nerror\n\n\n\n\n\n\nIndexOptions indexes multiple documents on the index, with optional Options\npassed to options\n\n\nfunc (*Client) Info\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nInfo\n()\n \n(\n*\nIndexInfo\n,\n \nerror\n)\n\n\n\n\n\n\nInfo - Get information about the index. This can also be used to check if the\nindex exists\n\n\nfunc (*Client) Search\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nSearch\n(\nq\n \n*\nQuery\n)\n \n(\ndocs\n \n[]\nDocument\n,\n \ntotal\n \nint\n,\n \nerr\n \nerror\n)\n\n\n\n\n\n\nSearch searches the index for the given query, and returns documents, the total\nnumber of results, or an error if something went wrong\n\n\ntype ConnPool\n\n\ntype\n \nConnPool\n \ninterface\n \n{\n\n    \nGet\n()\n \nredis\n.\nConn\n\n\n}\n\n\n\n\n\n\ntype Document\n\n\ntype\n \nDocument\n \nstruct\n \n{\n\n    \nId\n         \nstring\n\n    \nScore\n      \nfloat32\n\n    \nPayload\n    \n[]\nbyte\n\n    \nProperties\n \nmap\n[\nstring\n]\ninterface\n{}\n\n\n}\n\n\n\n\n\n\nDocument represents a single document to be indexed or returned from a query.\nBesides a score and id, the Properties are completely arbitrary\n\n\nfunc  NewDocument\n\n\nfunc\n \nNewDocument\n(\nid\n \nstring\n,\n \nscore\n \nfloat32\n)\n \nDocument\n\n\n\n\n\n\nNewDocument creates a document with the specific id and score\n\n\nfunc (*Document) EstimateSize\n\n\nfunc\n \n(\nd\n \n*\nDocument\n)\n \nEstimateSize\n()\n \n(\nsz\n \nint\n)\n\n\n\n\n\n\nfunc (Document) Set\n\n\nfunc\n \n(\nd\n \nDocument\n)\n \nSet\n(\nname\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nDocument\n\n\n\n\n\n\nSet sets a property and its value in the document\n\n\nfunc (*Document) SetPayload\n\n\nfunc\n \n(\nd\n \n*\nDocument\n)\n \nSetPayload\n(\npayload\n \n[]\nbyte\n)\n\n\n\n\n\n\nSetPayload Sets the document payload\n\n\ntype DocumentList\n\n\ntype\n \nDocumentList\n \n[]\nDocument\n\n\n\n\n\n\nDocumentList is used to sort documents by descending score\n\n\nfunc (DocumentList) Len\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nLen\n()\n \nint\n\n\n\n\n\n\nfunc (DocumentList) Less\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nLess\n(\ni\n,\n \nj\n \nint\n)\n \nbool\n\n\n\n\n\n\nfunc (DocumentList) Sort\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nSort\n()\n\n\n\n\n\n\nSort the DocumentList\n\n\nfunc (DocumentList) Swap\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nSwap\n(\ni\n,\n \nj\n \nint\n)\n\n\n\n\n\n\ntype Field\n\n\ntype\n \nField\n \nstruct\n \n{\n\n    \nName\n     \nstring\n\n    \nType\n     \nFieldType\n\n    \nSortable\n \nbool\n\n    \nOptions\n  \ninterface\n{}\n\n\n}\n\n\n\n\n\n\nField represents a single field's Schema\n\n\nfunc  NewNumericField\n\n\nfunc\n \nNewNumericField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewNumericField creates a new numeric field with the given name\n\n\nfunc  NewNumericFieldOptions\n\n\nfunc\n \nNewNumericFieldOptions\n(\nname\n \nstring\n,\n \noptions\n \nNumericFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewNumericFieldOptions defines a numeric field with additional options\n\n\nfunc  NewSortableNumericField\n\n\nfunc\n \nNewSortableNumericField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewSortableNumericField creates a new numeric field with the given name and a\nsortable flag\n\n\nfunc  NewSortableTextField\n\n\nfunc\n \nNewSortableTextField\n(\nname\n \nstring\n,\n \nweight\n \nfloat32\n)\n \nField\n\n\n\n\n\n\nNewSortableTextField creates a text field with the sortable flag set\n\n\nfunc  NewTagField\n\n\nfunc\n \nNewTagField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewTagField creates a new text field with default options (separator: ,)\n\n\nfunc  NewTagFieldOptions\n\n\nfunc\n \nNewTagFieldOptions\n(\nname\n \nstring\n,\n \nopts\n \nTagFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewTagFieldOptions creates a new tag field with the given options\n\n\nfunc  NewTextField\n\n\nfunc\n \nNewTextField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewTextField creates a new text field with the given weight\n\n\nfunc  NewTextFieldOptions\n\n\nfunc\n \nNewTextFieldOptions\n(\nname\n \nstring\n,\n \nopts\n \nTextFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewTextFieldOptions creates a new text field with given options\n(weight/sortable)\n\n\ntype FieldType\n\n\ntype\n \nFieldType\n \nint\n\n\n\n\n\n\nFieldType is an enumeration of field/property types\n\n\nconst\n \n(\n\n    \n// TextField full-text field\n\n    \nTextField\n \nFieldType\n \n=\n \niota\n\n\n    \n// NumericField numeric range field\n\n    \nNumericField\n\n\n    \n// GeoField geo-indexed point field\n\n    \nGeoField\n\n\n    \n// TagField is a field used for compact indexing of comma separated values\n\n    \nTagField\n\n\n)\n\n\n\n\n\n\ntype Flag\n\n\ntype\n \nFlag\n \nuint64\n\n\n\n\n\n\nFlag is a type for query flags\n\n\nconst\n \n(\n\n    \n// Treat the terms verbatim and do not perform expansion\n\n    \nQueryVerbatim\n \nFlag\n \n=\n \n0x1\n\n\n    \n// Do not load any content from the documents, return just IDs\n\n    \nQueryNoContent\n \nFlag\n \n=\n \n0x2\n\n\n    \n// Fetch document scores as well as IDs and fields\n\n    \nQueryWithScores\n \nFlag\n \n=\n \n0x4\n\n\n    \n// The query terms must appear in order in the document\n\n    \nQueryInOrder\n \nFlag\n \n=\n \n0x08\n\n\n    \n// Fetch document payloads as well as fields. See documentation for payloads on redisearch.io\n\n    \nQueryWithPayloads\n \nFlag\n \n=\n \n0x10\n\n\n    \nDefaultOffset\n \n=\n \n0\n\n    \nDefaultNum\n    \n=\n \n10\n\n\n)\n\n\n\n\n\n\nQuery Flags\n\n\ntype HighlightOptions\n\n\ntype\n \nHighlightOptions\n \nstruct\n \n{\n\n    \nFields\n \n[]\nstring\n\n    \nTags\n   \n[\n2\n]\nstring\n\n\n}\n\n\n\n\n\n\nHighlightOptions represents the options to higlight specific document fields.\nSee http://redisearch.io/Highlight/\n\n\ntype IndexInfo\n\n\ntype\n \nIndexInfo\n \nstruct\n \n{\n\n    \nSchema\n               \nSchema\n\n    \nName\n                 \nstring\n  \n`redis:\nindex_name\n`\n\n    \nDocCount\n             \nuint64\n  \n`redis:\nnum_docs\n`\n\n    \nRecordCount\n          \nuint64\n  \n`redis:\nnum_records\n`\n\n    \nTermCount\n            \nuint64\n  \n`redis:\nnum_terms\n`\n\n    \nMaxDocID\n             \nuint64\n  \n`redis:\nmax_doc_id\n`\n\n    \nInvertedIndexSizeMB\n  \nfloat64\n \n`redis:\ninverted_sz_mb\n`\n\n    \nOffsetVectorSizeMB\n   \nfloat64\n \n`redis:\noffset_vector_sz_mb\n`\n\n    \nDocTableSizeMB\n       \nfloat64\n \n`redis:\ndoc_table_size_mb\n`\n\n    \nKeyTableSizeMB\n       \nfloat64\n \n`redis:\nkey_table_size_mb\n`\n\n    \nRecordsPerDocAvg\n     \nfloat64\n \n`redis:\nrecords_per_doc_avg\n`\n\n    \nBytesPerRecordAvg\n    \nfloat64\n \n`redis:\nbytes_per_record_avg\n`\n\n    \nOffsetsPerTermAvg\n    \nfloat64\n \n`redis:\noffsets_per_term_avg\n`\n\n    \nOffsetBitsPerTermAvg\n \nfloat64\n \n`redis:\noffset_bits_per_record_avg\n`\n\n\n}\n\n\n\n\n\n\nIndexInfo - Structure showing information about an existing index\n\n\ntype IndexingOptions\n\n\ntype\n \nIndexingOptions\n \nstruct\n \n{\n\n    \nLanguage\n \nstring\n\n    \nNoSave\n   \nbool\n\n    \nReplace\n  \nbool\n\n    \nPartial\n  \nbool\n\n\n}\n\n\n\n\n\n\nIndexingOptions represent the options for indexing a single document\n\n\ntype MultiError\n\n\ntype\n \nMultiError\n \n[]\nerror\n\n\n\n\n\n\nMultiError Represents one or more errors\n\n\nfunc  NewMultiError\n\n\nfunc\n \nNewMultiError\n(\nlen\n \nint\n)\n \nMultiError\n\n\n\n\n\n\nNewMultiError initializes a multierror with the given len, and all sub-errors\nset to nil\n\n\nfunc (MultiError) Error\n\n\nfunc\n \n(\ne\n \nMultiError\n)\n \nError\n()\n \nstring\n\n\n\n\n\n\nError returns a string representation of the error, in this case it just chains\nall the sub errors if they are not nil\n\n\ntype MultiHostPool\n\n\ntype\n \nMultiHostPool\n \nstruct\n \n{\n\n    \nsync\n.\nMutex\n\n\n}\n\n\n\n\n\n\nfunc  NewMultiHostPool\n\n\nfunc\n \nNewMultiHostPool\n(\nhosts\n \n[]\nstring\n)\n \n*\nMultiHostPool\n\n\n\n\n\n\nfunc (*MultiHostPool) Get\n\n\nfunc\n \n(\np\n \n*\nMultiHostPool\n)\n \nGet\n()\n \nredis\n.\nConn\n\n\n\n\n\n\ntype NumericFieldOptions\n\n\ntype\n \nNumericFieldOptions\n \nstruct\n \n{\n\n    \nSortable\n \nbool\n\n    \nNoIndex\n  \nbool\n\n\n}\n\n\n\n\n\n\nNumericFieldOptions Options for numeric fields\n\n\ntype Operator\n\n\ntype\n \nOperator\n \nstring\n\n\n\n\n\n\nconst\n \n(\n\n    \nEq\n \nOperator\n \n=\n \n=\n\n\n    \nGt\n  \nOperator\n \n=\n \n\n    \nGte\n \nOperator\n \n=\n \n=\n\n\n    \nLt\n  \nOperator\n \n=\n \n\n    \nLte\n \nOperator\n \n=\n \n=\n\n\n    \nBetween\n          \nOperator\n \n=\n \nBETWEEN\n\n    \nBetweenInclusive\n \nOperator\n \n=\n \nBETWEEEN_EXCLUSIVE\n\n\n)\n\n\n\n\n\n\ntype Options\n\n\ntype\n \nOptions\n \nstruct\n \n{\n\n\n    \n// If set, we will not save the documents contents, just index them, for fetching ids only\n\n    \nNoSave\n \nbool\n\n\n    \nNoFieldFlags\n \nbool\n\n\n    \nNoFrequencies\n \nbool\n\n\n    \nNoOffsetVectors\n \nbool\n\n\n    \nStopwords\n \n[]\nstring\n\n\n}\n\n\n\n\n\n\nOptions are flags passed to the the abstract Index call, which receives them as\ninterface{}, allowing for implementation specific options\n\n\ntype Paging\n\n\ntype\n \nPaging\n \nstruct\n \n{\n\n    \nOffset\n \nint\n\n    \nNum\n    \nint\n\n\n}\n\n\n\n\n\n\nPaging represents the offset paging of a search result\n\n\ntype Predicate\n\n\ntype\n \nPredicate\n \nstruct\n \n{\n\n    \nProperty\n \nstring\n\n    \nOperator\n \nOperator\n\n    \nValue\n    \n[]\ninterface\n{}\n\n\n}\n\n\n\n\n\n\nfunc  Equals\n\n\nfunc\n \nEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  GreaterThan\n\n\nfunc\n \nGreaterThan\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  GreaterThanEquals\n\n\nfunc\n \nGreaterThanEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  InRange\n\n\nfunc\n \nInRange\n(\nproperty\n \nstring\n,\n \nmin\n,\n \nmax\n \ninterface\n{},\n \ninclusive\n \nbool\n)\n \nPredicate\n\n\n\n\n\n\nfunc  LessThan\n\n\nfunc\n \nLessThan\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  LessThanEquals\n\n\nfunc\n \nLessThanEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  NewPredicate\n\n\nfunc\n \nNewPredicate\n(\nproperty\n \nstring\n,\n \noperator\n \nOperator\n,\n \nvalues\n \n...\ninterface\n{})\n \nPredicate\n\n\n\n\n\n\ntype Query\n\n\ntype\n \nQuery\n \nstruct\n \n{\n\n    \nRaw\n \nstring\n\n\n    \nPaging\n \nPaging\n\n    \nFlags\n  \nFlag\n\n    \nSlop\n   \nint\n\n\n    \nFilters\n       \n[]\nPredicate\n\n    \nInKeys\n        \n[]\nstring\n\n    \nReturnFields\n  \n[]\nstring\n\n    \nLanguage\n      \nstring\n\n    \nExpander\n      \nstring\n\n    \nScorer\n        \nstring\n\n    \nPayload\n       \n[]\nbyte\n\n    \nSortBy\n        \n*\nSortingKey\n\n    \nHighlightOpts\n \n*\nHighlightOptions\n\n    \nSummarizeOpts\n \n*\nSummaryOptions\n\n\n}\n\n\n\n\n\n\nQuery is a single search query and all its parameters and predicates\n\n\nfunc  NewQuery\n\n\nfunc\n \nNewQuery\n(\nraw\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nNewQuery creates a new query for a given index with the given search term. For\ncurrently the index parameter is ignored\n\n\nfunc (*Query) Highlight\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nHighlight\n(\nfields\n \n[]\nstring\n,\n \nopenTag\n,\n \ncloseTag\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nHighlight sets highighting on given fields. Highlighting marks all the query\nterms with the given open and close tags (i.e. \n and \n for HTML)\n\n\nfunc (*Query) Limit\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nLimit\n(\noffset\n,\n \nnum\n \nint\n)\n \n*\nQuery\n\n\n\n\n\n\nLimit sets the paging offset and limit for the query\n\n\nfunc (*Query) SetExpander\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetExpander\n(\nexp\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetExpander sets a custom user query expander to be used\n\n\nfunc (*Query) SetFlags\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetFlags\n(\nflags\n \nFlag\n)\n \n*\nQuery\n\n\n\n\n\n\nSetFlags sets the query's optional flags\n\n\nfunc (*Query) SetInKeys\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetInKeys\n(\nkeys\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetInKeys sets the INKEYS argument of the query - limiting the search to a given\nset of IDs\n\n\nfunc (*Query) SetLanguage\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetLanguage\n(\nlang\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetLanguage sets the query language, used by the stemmer to expand the query\n\n\nfunc (*Query) SetPayload\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetPayload\n(\npayload\n \n[]\nbyte\n)\n \n*\nQuery\n\n\n\n\n\n\nSetPayload sets a binary payload to the query, that can be used by custom\nscoring functions\n\n\nfunc (*Query) SetReturnFields\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetReturnFields\n(\nfields\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetReturnFields sets the fields that should be returned from each result. By\ndefault we return everything\n\n\nfunc (*Query) SetScorer\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetScorer\n(\nscorer\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetScorer sets an alternative scoring function to be used. The only pre-compiled\nsupported one at the moment is DISMAX\n\n\nfunc (*Query) SetSortBy\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetSortBy\n(\nfield\n \nstring\n,\n \nascending\n \nbool\n)\n \n*\nQuery\n\n\n\n\n\n\nSetSortBy sets the sorting key for the query\n\n\nfunc (*Query) Summarize\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSummarize\n(\nfields\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSummarize sets summarization on the given list of fields. It will instruct the\nengine to extract the most relevant snippets from the fields and return them as\nthe field content. This function works with the default values of the engine,\nand only sets the fields. There is a function that accepts all options -\nSummarizeOptions\n\n\nfunc (*Query) SummarizeOptions\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSummarizeOptions\n(\nopts\n \nSummaryOptions\n)\n \n*\nQuery\n\n\n\n\n\n\nSummarizeOptions sets summarization on the given list of fields. It will\ninstruct the engine to extract the most relevant snippets from the fields and\nreturn them as the field content.\n\n\nThis function accepts advanced settings for snippet length, separators and\nnumber of snippets\n\n\ntype Schema\n\n\ntype\n \nSchema\n \nstruct\n \n{\n\n    \nFields\n  \n[]\nField\n\n    \nOptions\n \nOptions\n\n\n}\n\n\n\n\n\n\nSchema represents an index schema Schema, or how the index would treat documents\nsent to it.\n\n\nfunc  NewSchema\n\n\nfunc\n \nNewSchema\n(\nopts\n \nOptions\n)\n \n*\nSchema\n\n\n\n\n\n\nNewSchema creates a new Schema object\n\n\nfunc (*Schema) AddField\n\n\nfunc\n \n(\nm\n \n*\nSchema\n)\n \nAddField\n(\nf\n \nField\n)\n \n*\nSchema\n\n\n\n\n\n\nAddField adds a field to the Schema object\n\n\ntype SingleHostPool\n\n\ntype\n \nSingleHostPool\n \nstruct\n \n{\n\n    \n*\nredis\n.\nPool\n\n\n}\n\n\n\n\n\n\nfunc  NewSingleHostPool\n\n\nfunc\n \nNewSingleHostPool\n(\nhost\n \nstring\n)\n \n*\nSingleHostPool\n\n\n\n\n\n\ntype SortingKey\n\n\ntype\n \nSortingKey\n \nstruct\n \n{\n\n    \nField\n     \nstring\n\n    \nAscending\n \nbool\n\n\n}\n\n\n\n\n\n\nSortingKey represents the sorting option if the query needs to be sorted based\non a sortable fields and not a ranking function. See\nhttp://redisearch.io/Sorting/\n\n\ntype Suggestion\n\n\ntype\n \nSuggestion\n \nstruct\n \n{\n\n    \nTerm\n    \nstring\n\n    \nScore\n   \nfloat64\n\n    \nPayload\n \nstring\n\n\n}\n\n\n\n\n\n\nSuggestion is a single suggestion being added or received from the Autocompleter\n\n\ntype SuggestionList\n\n\ntype\n \nSuggestionList\n \n[]\nSuggestion\n\n\n\n\n\n\nSuggestionList is a sortable list of suggestions returned from an engine\n\n\nfunc (SuggestionList) Len\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nLen\n()\n \nint\n\n\n\n\n\n\nfunc (SuggestionList) Less\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nLess\n(\ni\n,\n \nj\n \nint\n)\n \nbool\n\n\n\n\n\n\nfunc (SuggestionList) Sort\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nSort\n()\n\n\n\n\n\n\nSort the SuggestionList\n\n\nfunc (SuggestionList) Swap\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nSwap\n(\ni\n,\n \nj\n \nint\n)\n\n\n\n\n\n\ntype SummaryOptions\n\n\ntype\n \nSummaryOptions\n \nstruct\n \n{\n\n    \nFields\n       \n[]\nstring\n\n    \nFragmentLen\n  \nint\n    \n// default 20\n\n    \nNumFragments\n \nint\n    \n// default 3\n\n    \nSeparator\n    \nstring\n \n// default \n...\n\n\n}\n\n\n\n\n\n\nSummaryOptions represents the configuration used to create field summaries. See\nhttp://redisearch.io/Highlight/\n\n\ntype TagFieldOptions\n\n\ntype\n \nTagFieldOptions\n \nstruct\n \n{\n\n    \n// Separator is the custom separator between tags. defaults to comma (,)\n\n    \nSeparator\n \nbyte\n\n    \nNoIndex\n   \nbool\n\n\n}\n\n\n\n\n\n\nTagFieldOptions options for indexing tag fields\n\n\ntype TextFieldOptions\n\n\ntype\n \nTextFieldOptions\n \nstruct\n \n{\n\n    \nWeight\n   \nfloat32\n\n    \nSortable\n \nbool\n\n    \nNoStem\n   \nbool\n\n    \nNoIndex\n  \nbool\n\n\n}\n\n\n\n\n\n\nTextFieldOptions Options for text fields - weight and stemming enabled/disabled.", 
            "title": "Go API"
        }, 
        {
            "location": "/go_client/#redisearch", 
            "text": "--\n    import \"github.com/RedisLabs/redisearch-go/redisearch\"  Package redisearch provides a Go client for the RediSearch search engine.  For the full documentation of RediSearch, see http://redisearch.io", 
            "title": "redisearch"
        }, 
        {
            "location": "/go_client/#example\"_\"usage", 
            "text": "import   ( \n       github.com/RedisLabs/redisearch-go/redisearch \n       log \n       fmt \n     ) \n\n     func   ExampleClient ()   { \n       // Create a client. By default a client is schemaless \n       // unless a schema is provided when creating the index \n       c   :=   createClient ( myIndex ) \n\n       // Create a schema \n       sc   :=   redisearch . NewSchema ( redisearch . DefaultOptions ). \n         AddField ( redisearch . NewTextField ( body )). \n         AddField ( redisearch . NewTextFieldOptions ( title ,   redisearch . TextFieldOptions { Weight :   5.0 ,   Sortable :   true })). \n         AddField ( redisearch . NewNumericField ( date )) \n\n       // Drop an existing index. If the index does not exist an error is returned \n       c . Drop () \n\n       // Create the index with the given schema \n       if   err   :=   c . CreateIndex ( sc );   err   !=   nil   { \n         log . Fatal ( err ) \n       } \n\n       // Create a document with an id and given score \n       doc   :=   redisearch . NewDocument ( doc1 ,   1.0 ) \n       doc . Set ( title ,   Hello world ). \n         Set ( body ,   foo bar ). \n         Set ( date ,   time . Now (). Unix ()) \n\n       // Index the document. The API accepts multiple documents at a time \n       if   err   :=   c . IndexOptions ( redisearch . DefaultIndexingOptions ,   doc );   err   !=   nil   { \n         log . Fatal ( err ) \n       } \n\n       // Searching with limit and sorting \n       docs ,   total ,   err   :=   c . Search ( redisearch . NewQuery ( hello world ). \n         Limit ( 0 ,   2 ). \n         SetReturnFields ( title )) \n\n       fmt . Println ( docs [ 0 ]. Id ,   docs [ 0 ]. Properties [ title ],   total ,   err ) \n       // Output: doc1 Hello world 1  nil \n     }", 
            "title": "Example Usage"
        }, 
        {
            "location": "/go_client/#usage", 
            "text": "var   DefaultIndexingOptions   =   IndexingOptions { \n     Language :   , \n     NoSave :     false , \n     Replace :    false , \n     Partial :    false ,  }   DefaultIndexingOptions are the default options for document indexing  var   DefaultOptions   =   Options { \n     NoSave :            false , \n     NoFieldFlags :      false , \n     NoFrequencies :     false , \n     NoOffsetVectors :   false , \n     Stopwords :         nil ,  }   DefaultOptions represents the default options", 
            "title": "Usage"
        }, 
        {
            "location": "/go_client/#type\"_\"autocompleter", 
            "text": "type   Autocompleter   struct   {  }   Autocompleter implements a redisearch auto-completer API", 
            "title": "type Autocompleter"
        }, 
        {
            "location": "/go_client/#func\"_\"newautocompleter", 
            "text": "func   NewAutocompleter ( addr ,   name   string )   * Autocompleter   NewAutocompleter creates a new Autocompleter with the given host and key name", 
            "title": "func  NewAutocompleter"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"addterms", 
            "text": "func   ( a   * Autocompleter )   AddTerms ( terms   ... Suggestion )   error   AddTerms pushes new term suggestions to the index", 
            "title": "func (*Autocompleter) AddTerms"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"delete", 
            "text": "func   ( a   * Autocompleter )   Delete ()   error   Delete deletes the Autocompleter key for this AC", 
            "title": "func (*Autocompleter) Delete"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"suggest", 
            "text": "func   ( a   * Autocompleter )   Suggest ( prefix   string ,   num   int ,   fuzzy   bool )   ([] Suggestion ,   error )   Suggest gets completion suggestions from the Autocompleter dictionary to the\ngiven prefix. If fuzzy is set, we also complete for prefixes that are in 1\nLevenshten distance from the given prefix", 
            "title": "func (*Autocompleter) Suggest"
        }, 
        {
            "location": "/go_client/#type\"_\"client", 
            "text": "type   Client   struct   {  }   Client is an interface to redisearch's redis commands", 
            "title": "type Client"
        }, 
        {
            "location": "/go_client/#func\"_\"newclient", 
            "text": "func   NewClient ( addr ,   name   string )   * Client   NewClient creates a new client connecting to the redis host, and using the given\nname as key prefix. Addr can be a single host:port pair, or a comma separated\nlist of host:port,host:port... In the case of multiple hosts we create a\nmulti-pool and select connections at random", 
            "title": "func  NewClient"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"createindex", 
            "text": "func   ( i   * Client )   CreateIndex ( s   * Schema )   error   CreateIndex configues the index and creates it on redis", 
            "title": "func (*Client) CreateIndex"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"drop", 
            "text": "func   ( i   * Client )   Drop ()   error   Drop the Currentl just flushes the DB - note that this will delete EVERYTHING on\nthe redis instance", 
            "title": "func (*Client) Drop"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"explain", 
            "text": "func   ( i   * Client )   Explain ( q   * Query )   ( string ,   error )   Explain Return a textual string explaining the query", 
            "title": "func (*Client) Explain"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"index", 
            "text": "func   ( i   * Client )   Index ( docs   ... Document )   error   Index indexes a list of documents with the default options", 
            "title": "func (*Client) Index"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"indexoptions", 
            "text": "func   ( i   * Client )   IndexOptions ( opts   IndexingOptions ,   docs   ... Document )   error   IndexOptions indexes multiple documents on the index, with optional Options\npassed to options", 
            "title": "func (*Client) IndexOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"info", 
            "text": "func   ( i   * Client )   Info ()   ( * IndexInfo ,   error )   Info - Get information about the index. This can also be used to check if the\nindex exists", 
            "title": "func (*Client) Info"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"search", 
            "text": "func   ( i   * Client )   Search ( q   * Query )   ( docs   [] Document ,   total   int ,   err   error )   Search searches the index for the given query, and returns documents, the total\nnumber of results, or an error if something went wrong", 
            "title": "func (*Client) Search"
        }, 
        {
            "location": "/go_client/#type\"_\"connpool", 
            "text": "type   ConnPool   interface   { \n     Get ()   redis . Conn  }", 
            "title": "type ConnPool"
        }, 
        {
            "location": "/go_client/#type\"_\"document", 
            "text": "type   Document   struct   { \n     Id           string \n     Score        float32 \n     Payload      [] byte \n     Properties   map [ string ] interface {}  }   Document represents a single document to be indexed or returned from a query.\nBesides a score and id, the Properties are completely arbitrary", 
            "title": "type Document"
        }, 
        {
            "location": "/go_client/#func\"_\"newdocument", 
            "text": "func   NewDocument ( id   string ,   score   float32 )   Document   NewDocument creates a document with the specific id and score", 
            "title": "func  NewDocument"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"estimatesize", 
            "text": "func   ( d   * Document )   EstimateSize ()   ( sz   int )", 
            "title": "func (*Document) EstimateSize"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"set", 
            "text": "func   ( d   Document )   Set ( name   string ,   value   interface {})   Document   Set sets a property and its value in the document", 
            "title": "func (Document) Set"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"setpayload", 
            "text": "func   ( d   * Document )   SetPayload ( payload   [] byte )   SetPayload Sets the document payload", 
            "title": "func (*Document) SetPayload"
        }, 
        {
            "location": "/go_client/#type\"_\"documentlist", 
            "text": "type   DocumentList   [] Document   DocumentList is used to sort documents by descending score", 
            "title": "type DocumentList"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"len", 
            "text": "func   ( l   DocumentList )   Len ()   int", 
            "title": "func (DocumentList) Len"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"less", 
            "text": "func   ( l   DocumentList )   Less ( i ,   j   int )   bool", 
            "title": "func (DocumentList) Less"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"sort", 
            "text": "func   ( l   DocumentList )   Sort ()   Sort the DocumentList", 
            "title": "func (DocumentList) Sort"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"swap", 
            "text": "func   ( l   DocumentList )   Swap ( i ,   j   int )", 
            "title": "func (DocumentList) Swap"
        }, 
        {
            "location": "/go_client/#type\"_\"field", 
            "text": "type   Field   struct   { \n     Name       string \n     Type       FieldType \n     Sortable   bool \n     Options    interface {}  }   Field represents a single field's Schema", 
            "title": "type Field"
        }, 
        {
            "location": "/go_client/#func\"_\"newnumericfield", 
            "text": "func   NewNumericField ( name   string )   Field   NewNumericField creates a new numeric field with the given name", 
            "title": "func  NewNumericField"
        }, 
        {
            "location": "/go_client/#func\"_\"newnumericfieldoptions", 
            "text": "func   NewNumericFieldOptions ( name   string ,   options   NumericFieldOptions )   Field   NewNumericFieldOptions defines a numeric field with additional options", 
            "title": "func  NewNumericFieldOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"newsortablenumericfield", 
            "text": "func   NewSortableNumericField ( name   string )   Field   NewSortableNumericField creates a new numeric field with the given name and a\nsortable flag", 
            "title": "func  NewSortableNumericField"
        }, 
        {
            "location": "/go_client/#func\"_\"newsortabletextfield", 
            "text": "func   NewSortableTextField ( name   string ,   weight   float32 )   Field   NewSortableTextField creates a text field with the sortable flag set", 
            "title": "func  NewSortableTextField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtagfield", 
            "text": "func   NewTagField ( name   string )   Field   NewTagField creates a new text field with default options (separator: ,)", 
            "title": "func  NewTagField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtagfieldoptions", 
            "text": "func   NewTagFieldOptions ( name   string ,   opts   TagFieldOptions )   Field   NewTagFieldOptions creates a new tag field with the given options", 
            "title": "func  NewTagFieldOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"newtextfield", 
            "text": "func   NewTextField ( name   string )   Field   NewTextField creates a new text field with the given weight", 
            "title": "func  NewTextField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtextfieldoptions", 
            "text": "func   NewTextFieldOptions ( name   string ,   opts   TextFieldOptions )   Field   NewTextFieldOptions creates a new text field with given options\n(weight/sortable)", 
            "title": "func  NewTextFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"fieldtype", 
            "text": "type   FieldType   int   FieldType is an enumeration of field/property types  const   ( \n     // TextField full-text field \n     TextField   FieldType   =   iota \n\n     // NumericField numeric range field \n     NumericField \n\n     // GeoField geo-indexed point field \n     GeoField \n\n     // TagField is a field used for compact indexing of comma separated values \n     TagField  )", 
            "title": "type FieldType"
        }, 
        {
            "location": "/go_client/#type\"_\"flag", 
            "text": "type   Flag   uint64   Flag is a type for query flags  const   ( \n     // Treat the terms verbatim and do not perform expansion \n     QueryVerbatim   Flag   =   0x1 \n\n     // Do not load any content from the documents, return just IDs \n     QueryNoContent   Flag   =   0x2 \n\n     // Fetch document scores as well as IDs and fields \n     QueryWithScores   Flag   =   0x4 \n\n     // The query terms must appear in order in the document \n     QueryInOrder   Flag   =   0x08 \n\n     // Fetch document payloads as well as fields. See documentation for payloads on redisearch.io \n     QueryWithPayloads   Flag   =   0x10 \n\n     DefaultOffset   =   0 \n     DefaultNum      =   10  )   Query Flags", 
            "title": "type Flag"
        }, 
        {
            "location": "/go_client/#type\"_\"highlightoptions", 
            "text": "type   HighlightOptions   struct   { \n     Fields   [] string \n     Tags     [ 2 ] string  }   HighlightOptions represents the options to higlight specific document fields.\nSee http://redisearch.io/Highlight/", 
            "title": "type HighlightOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"indexinfo", 
            "text": "type   IndexInfo   struct   { \n     Schema                 Schema \n     Name                   string    `redis: index_name ` \n     DocCount               uint64    `redis: num_docs ` \n     RecordCount            uint64    `redis: num_records ` \n     TermCount              uint64    `redis: num_terms ` \n     MaxDocID               uint64    `redis: max_doc_id ` \n     InvertedIndexSizeMB    float64   `redis: inverted_sz_mb ` \n     OffsetVectorSizeMB     float64   `redis: offset_vector_sz_mb ` \n     DocTableSizeMB         float64   `redis: doc_table_size_mb ` \n     KeyTableSizeMB         float64   `redis: key_table_size_mb ` \n     RecordsPerDocAvg       float64   `redis: records_per_doc_avg ` \n     BytesPerRecordAvg      float64   `redis: bytes_per_record_avg ` \n     OffsetsPerTermAvg      float64   `redis: offsets_per_term_avg ` \n     OffsetBitsPerTermAvg   float64   `redis: offset_bits_per_record_avg `  }   IndexInfo - Structure showing information about an existing index", 
            "title": "type IndexInfo"
        }, 
        {
            "location": "/go_client/#type\"_\"indexingoptions", 
            "text": "type   IndexingOptions   struct   { \n     Language   string \n     NoSave     bool \n     Replace    bool \n     Partial    bool  }   IndexingOptions represent the options for indexing a single document", 
            "title": "type IndexingOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"multierror", 
            "text": "type   MultiError   [] error   MultiError Represents one or more errors", 
            "title": "type MultiError"
        }, 
        {
            "location": "/go_client/#func\"_\"newmultierror", 
            "text": "func   NewMultiError ( len   int )   MultiError   NewMultiError initializes a multierror with the given len, and all sub-errors\nset to nil", 
            "title": "func  NewMultiError"
        }, 
        {
            "location": "/go_client/#func\"_\"multierror\"_\"error", 
            "text": "func   ( e   MultiError )   Error ()   string   Error returns a string representation of the error, in this case it just chains\nall the sub errors if they are not nil", 
            "title": "func (MultiError) Error"
        }, 
        {
            "location": "/go_client/#type\"_\"multihostpool", 
            "text": "type   MultiHostPool   struct   { \n     sync . Mutex  }", 
            "title": "type MultiHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"newmultihostpool", 
            "text": "func   NewMultiHostPool ( hosts   [] string )   * MultiHostPool", 
            "title": "func  NewMultiHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"multihostpool\"_\"get", 
            "text": "func   ( p   * MultiHostPool )   Get ()   redis . Conn", 
            "title": "func (*MultiHostPool) Get"
        }, 
        {
            "location": "/go_client/#type\"_\"numericfieldoptions", 
            "text": "type   NumericFieldOptions   struct   { \n     Sortable   bool \n     NoIndex    bool  }   NumericFieldOptions Options for numeric fields", 
            "title": "type NumericFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"operator", 
            "text": "type   Operator   string   const   ( \n     Eq   Operator   =   = \n\n     Gt    Operator   =   \n     Gte   Operator   =   = \n\n     Lt    Operator   =   \n     Lte   Operator   =   = \n\n     Between            Operator   =   BETWEEN \n     BetweenInclusive   Operator   =   BETWEEEN_EXCLUSIVE  )", 
            "title": "type Operator"
        }, 
        {
            "location": "/go_client/#type\"_\"options", 
            "text": "type   Options   struct   { \n\n     // If set, we will not save the documents contents, just index them, for fetching ids only \n     NoSave   bool \n\n     NoFieldFlags   bool \n\n     NoFrequencies   bool \n\n     NoOffsetVectors   bool \n\n     Stopwords   [] string  }   Options are flags passed to the the abstract Index call, which receives them as\ninterface{}, allowing for implementation specific options", 
            "title": "type Options"
        }, 
        {
            "location": "/go_client/#type\"_\"paging", 
            "text": "type   Paging   struct   { \n     Offset   int \n     Num      int  }   Paging represents the offset paging of a search result", 
            "title": "type Paging"
        }, 
        {
            "location": "/go_client/#type\"_\"predicate", 
            "text": "type   Predicate   struct   { \n     Property   string \n     Operator   Operator \n     Value      [] interface {}  }", 
            "title": "type Predicate"
        }, 
        {
            "location": "/go_client/#func\"_\"equals", 
            "text": "func   Equals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  Equals"
        }, 
        {
            "location": "/go_client/#func\"_\"greaterthan", 
            "text": "func   GreaterThan ( property   string ,   value   interface {})   Predicate", 
            "title": "func  GreaterThan"
        }, 
        {
            "location": "/go_client/#func\"_\"greaterthanequals", 
            "text": "func   GreaterThanEquals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  GreaterThanEquals"
        }, 
        {
            "location": "/go_client/#func\"_\"inrange", 
            "text": "func   InRange ( property   string ,   min ,   max   interface {},   inclusive   bool )   Predicate", 
            "title": "func  InRange"
        }, 
        {
            "location": "/go_client/#func\"_\"lessthan", 
            "text": "func   LessThan ( property   string ,   value   interface {})   Predicate", 
            "title": "func  LessThan"
        }, 
        {
            "location": "/go_client/#func\"_\"lessthanequals", 
            "text": "func   LessThanEquals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  LessThanEquals"
        }, 
        {
            "location": "/go_client/#func\"_\"newpredicate", 
            "text": "func   NewPredicate ( property   string ,   operator   Operator ,   values   ... interface {})   Predicate", 
            "title": "func  NewPredicate"
        }, 
        {
            "location": "/go_client/#type\"_\"query", 
            "text": "type   Query   struct   { \n     Raw   string \n\n     Paging   Paging \n     Flags    Flag \n     Slop     int \n\n     Filters         [] Predicate \n     InKeys          [] string \n     ReturnFields    [] string \n     Language        string \n     Expander        string \n     Scorer          string \n     Payload         [] byte \n     SortBy          * SortingKey \n     HighlightOpts   * HighlightOptions \n     SummarizeOpts   * SummaryOptions  }   Query is a single search query and all its parameters and predicates", 
            "title": "type Query"
        }, 
        {
            "location": "/go_client/#func\"_\"newquery", 
            "text": "func   NewQuery ( raw   string )   * Query   NewQuery creates a new query for a given index with the given search term. For\ncurrently the index parameter is ignored", 
            "title": "func  NewQuery"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"highlight", 
            "text": "func   ( q   * Query )   Highlight ( fields   [] string ,   openTag ,   closeTag   string )   * Query   Highlight sets highighting on given fields. Highlighting marks all the query\nterms with the given open and close tags (i.e.   and   for HTML)", 
            "title": "func (*Query) Highlight"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"limit", 
            "text": "func   ( q   * Query )   Limit ( offset ,   num   int )   * Query   Limit sets the paging offset and limit for the query", 
            "title": "func (*Query) Limit"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setexpander", 
            "text": "func   ( q   * Query )   SetExpander ( exp   string )   * Query   SetExpander sets a custom user query expander to be used", 
            "title": "func (*Query) SetExpander"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setflags", 
            "text": "func   ( q   * Query )   SetFlags ( flags   Flag )   * Query   SetFlags sets the query's optional flags", 
            "title": "func (*Query) SetFlags"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setinkeys", 
            "text": "func   ( q   * Query )   SetInKeys ( keys   ... string )   * Query   SetInKeys sets the INKEYS argument of the query - limiting the search to a given\nset of IDs", 
            "title": "func (*Query) SetInKeys"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setlanguage", 
            "text": "func   ( q   * Query )   SetLanguage ( lang   string )   * Query   SetLanguage sets the query language, used by the stemmer to expand the query", 
            "title": "func (*Query) SetLanguage"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setpayload", 
            "text": "func   ( q   * Query )   SetPayload ( payload   [] byte )   * Query   SetPayload sets a binary payload to the query, that can be used by custom\nscoring functions", 
            "title": "func (*Query) SetPayload"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setreturnfields", 
            "text": "func   ( q   * Query )   SetReturnFields ( fields   ... string )   * Query   SetReturnFields sets the fields that should be returned from each result. By\ndefault we return everything", 
            "title": "func (*Query) SetReturnFields"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setscorer", 
            "text": "func   ( q   * Query )   SetScorer ( scorer   string )   * Query   SetScorer sets an alternative scoring function to be used. The only pre-compiled\nsupported one at the moment is DISMAX", 
            "title": "func (*Query) SetScorer"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setsortby", 
            "text": "func   ( q   * Query )   SetSortBy ( field   string ,   ascending   bool )   * Query   SetSortBy sets the sorting key for the query", 
            "title": "func (*Query) SetSortBy"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"summarize", 
            "text": "func   ( q   * Query )   Summarize ( fields   ... string )   * Query   Summarize sets summarization on the given list of fields. It will instruct the\nengine to extract the most relevant snippets from the fields and return them as\nthe field content. This function works with the default values of the engine,\nand only sets the fields. There is a function that accepts all options -\nSummarizeOptions", 
            "title": "func (*Query) Summarize"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"summarizeoptions", 
            "text": "func   ( q   * Query )   SummarizeOptions ( opts   SummaryOptions )   * Query   SummarizeOptions sets summarization on the given list of fields. It will\ninstruct the engine to extract the most relevant snippets from the fields and\nreturn them as the field content.  This function accepts advanced settings for snippet length, separators and\nnumber of snippets", 
            "title": "func (*Query) SummarizeOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"schema", 
            "text": "type   Schema   struct   { \n     Fields    [] Field \n     Options   Options  }   Schema represents an index schema Schema, or how the index would treat documents\nsent to it.", 
            "title": "type Schema"
        }, 
        {
            "location": "/go_client/#func\"_\"newschema", 
            "text": "func   NewSchema ( opts   Options )   * Schema   NewSchema creates a new Schema object", 
            "title": "func  NewSchema"
        }, 
        {
            "location": "/go_client/#func\"_\"schema\"_\"addfield", 
            "text": "func   ( m   * Schema )   AddField ( f   Field )   * Schema   AddField adds a field to the Schema object", 
            "title": "func (*Schema) AddField"
        }, 
        {
            "location": "/go_client/#type\"_\"singlehostpool", 
            "text": "type   SingleHostPool   struct   { \n     * redis . Pool  }", 
            "title": "type SingleHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"newsinglehostpool", 
            "text": "func   NewSingleHostPool ( host   string )   * SingleHostPool", 
            "title": "func  NewSingleHostPool"
        }, 
        {
            "location": "/go_client/#type\"_\"sortingkey", 
            "text": "type   SortingKey   struct   { \n     Field       string \n     Ascending   bool  }   SortingKey represents the sorting option if the query needs to be sorted based\non a sortable fields and not a ranking function. See\nhttp://redisearch.io/Sorting/", 
            "title": "type SortingKey"
        }, 
        {
            "location": "/go_client/#type\"_\"suggestion", 
            "text": "type   Suggestion   struct   { \n     Term      string \n     Score     float64 \n     Payload   string  }   Suggestion is a single suggestion being added or received from the Autocompleter", 
            "title": "type Suggestion"
        }, 
        {
            "location": "/go_client/#type\"_\"suggestionlist", 
            "text": "type   SuggestionList   [] Suggestion   SuggestionList is a sortable list of suggestions returned from an engine", 
            "title": "type SuggestionList"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"len", 
            "text": "func   ( l   SuggestionList )   Len ()   int", 
            "title": "func (SuggestionList) Len"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"less", 
            "text": "func   ( l   SuggestionList )   Less ( i ,   j   int )   bool", 
            "title": "func (SuggestionList) Less"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"sort", 
            "text": "func   ( l   SuggestionList )   Sort ()   Sort the SuggestionList", 
            "title": "func (SuggestionList) Sort"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"swap", 
            "text": "func   ( l   SuggestionList )   Swap ( i ,   j   int )", 
            "title": "func (SuggestionList) Swap"
        }, 
        {
            "location": "/go_client/#type\"_\"summaryoptions", 
            "text": "type   SummaryOptions   struct   { \n     Fields         [] string \n     FragmentLen    int      // default 20 \n     NumFragments   int      // default 3 \n     Separator      string   // default  ...  }   SummaryOptions represents the configuration used to create field summaries. See\nhttp://redisearch.io/Highlight/", 
            "title": "type SummaryOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"tagfieldoptions", 
            "text": "type   TagFieldOptions   struct   { \n     // Separator is the custom separator between tags. defaults to comma (,) \n     Separator   byte \n     NoIndex     bool  }   TagFieldOptions options for indexing tag fields", 
            "title": "type TagFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"textfieldoptions", 
            "text": "type   TextFieldOptions   struct   { \n     Weight     float32 \n     Sortable   bool \n     NoStem     bool \n     NoIndex    bool  }   TextFieldOptions Options for text fields - weight and stemming enabled/disabled.", 
            "title": "type TextFieldOptions"
        }, 
        {
            "location": "/design/gc/", 
            "text": "Garbage Collection in RediSearch\n\n\n1. The Need For GC\n\n\n\n\nDeleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.\n\n\nThis means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.\n\n\nThus all inverted index entries belonging to this document id are just garbage. \n\n\nWe do not want to go and explicitly delete them when deleting a document because it will make this operation very long an depending on the length of the document.\n\n\nOn top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.\n\n\n\n\nAll of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory. \n\n\nThus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.\n\n\n2. Garbage Collecting a Single Term Index\n\n\nA single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage. \n\n\nThe algorithm is pretty simple: \n\n\n\n\nCreate a reader and writer for each block\n\n\nRead each block's records one by one\n\n\nIf no record is invalid, do nothing\n\n\nOnce we found a garbage record, we advance the reader but not the writer.\n\n\nOnce we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.\n\n\n\n\nPseudo code:\n\n\nforeach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer\ns tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length\n\n\n\n\n\nNOTE\n: Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.\n\n\n2.1 Garbage Collection on Numeric Indexes\n\n\nNumeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.\n\n\n3. GC And Concurrency\n\n\nSince RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us). \n\n\nIt GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.\n\n\nThis means, however, that we need to consider a few things:\n\n\n\n\n\n\nFrom the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.\n\n\n\n\n\n\nFrom the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.\n\n\n\n\n\n\nTo solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:\n\n Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed. \n\n Before starting an index iterator, we copy the index's gc marker to the iterator's context.\n\n After waking up from sleep in the iterator, we check the gc markers in both objects.\n\n If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower. \n\n\nTo solve 2 is simpler: \n\n The GC will of course operate only while the GIL is locked.\n\n The GC will never yield execution while in the middle of a block.\n\n The GC will check whether the key has been delted while it slept.\n\n The GC will get a new pointer to the next block on each read, assuring the pointer is safe.\n\n\n4. Scheduling Garbage Collection\n\n\nWhile the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively. \n\n\nSo the GC will use sampling of random terms and collect them. \n\n\nThis leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage. \n\n\nSolving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.\n\n\nThus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.\n\n\nSolving 1 can be done in the following way:\n\n\nWe start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.\n\n\nThe frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "Garbage Collection"
        }, 
        {
            "location": "/design/gc/#garbage\"_\"collection\"_\"in\"_\"redisearch", 
            "text": "", 
            "title": "Garbage Collection in RediSearch"
        }, 
        {
            "location": "/design/gc/#1\"_\"the\"_\"need\"_\"for\"_\"gc", 
            "text": "Deleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.  This means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.  Thus all inverted index entries belonging to this document id are just garbage.   We do not want to go and explicitly delete them when deleting a document because it will make this operation very long an depending on the length of the document.  On top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.   All of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory.   Thus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.", 
            "title": "1. The Need For GC"
        }, 
        {
            "location": "/design/gc/#2\"_\"garbage\"_\"collecting\"_\"a\"_\"single\"_\"term\"_\"index", 
            "text": "A single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage.   The algorithm is pretty simple:    Create a reader and writer for each block  Read each block's records one by one  If no record is invalid, do nothing  Once we found a garbage record, we advance the reader but not the writer.  Once we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.   Pseudo code:  foreach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer s tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length  NOTE : Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.", 
            "title": "2. Garbage Collecting a Single Term Index"
        }, 
        {
            "location": "/design/gc/#21\"_\"garbage\"_\"collection\"_\"on\"_\"numeric\"_\"indexes", 
            "text": "Numeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.", 
            "title": "2.1 Garbage Collection on Numeric Indexes"
        }, 
        {
            "location": "/design/gc/#3\"_\"gc\"_\"and\"_\"concurrency", 
            "text": "Since RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us).   It GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.  This means, however, that we need to consider a few things:    From the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.    From the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.    To solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:  Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed.   Before starting an index iterator, we copy the index's gc marker to the iterator's context.  After waking up from sleep in the iterator, we check the gc markers in both objects.  If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower.   To solve 2 is simpler:   The GC will of course operate only while the GIL is locked.  The GC will never yield execution while in the middle of a block.  The GC will check whether the key has been delted while it slept.  The GC will get a new pointer to the next block on each read, assuring the pointer is safe.", 
            "title": "3. GC And Concurrency"
        }, 
        {
            "location": "/design/gc/#4\"_\"scheduling\"_\"garbage\"_\"collection", 
            "text": "While the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively.   So the GC will use sampling of random terms and collect them.   This leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage.   Solving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.  Thus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.  Solving 1 can be done in the following way:  We start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.  The frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "4. Scheduling Garbage Collection"
        }, 
        {
            "location": "/Threading/", 
            "text": "Multi-Threading in RediSearch\n\n\nBy Dvir Volk, July 2017\n\n\n1. One Thread To Rule Them All\n\n\nRedis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with. \n\n\nWhile keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like \nZUNIONSTORE\n, \nLRANGE\n, \nSINTER\n and of course the infamous \nKEYS\n, can block Redis for seconds or minutes, depending on the size of data they are handling. \n\n\n2. RediSearch and the Single Thread Issue\n\n\nRediSearch\n is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine. \n\n\nWhile it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. \n\n\nThink, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, \nwhich is impossible with current hardware\n. The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.\n\n\nSo taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.\n\n\n3. Enter the Redis GIL\n\n\nLuckily, Redis BDFL \nSalvatore Sanfilippo\n has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API - \nThread Safe Contexts\n and the \nGlobal Lock\n.\n\n\nThe idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the \nGlobal Lock\n when it needs to access Redis data, operate on it, and release it. \n\n\nWe still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.\n\n\n4. Making Search Concurrent\n\n\nUp until now, the flow of a search query was simple - the query would arrive at a \nCommand Handler\n callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result. \n\n\nTo allow concurrency, we adapted the following design:\n\n\n\n\n\n\nRediSearch has a thread pool for running concurrent search queries. \n\n\n\n\n\n\nWhen a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.\n\n\n\n\n\n\nThe thread pool runs a query processing function in its own thread.\n\n\n\n\n\n\nThe function locks the Redis Global lock, and starts executing the query.\n\n\n\n\n\n\nSince the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).\n\n\n\n\n\n\nIf enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.\n\n\n\n\n\n\nWhen the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state. \n\n\n\n\n\n\nThus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently. \n\n\n\n\nFigure 1: Serial vs. Concurrent Search\n\n\n\n\nOn the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.\n\n\n\n\nThe same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.\n\n\nAs a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.\n\n\n5. The Effect of Concurrency\n\n\nWhile this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running \nKEYS *\n in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!\n\n\nThere is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\". \n\n\nThis is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation. \n\n\nTo enable safe mode and disable query concurrency, you can configure RediSearch at load time: \nredis-server --loadmodule redisearch.so SAFEMODE\n in command line, or by adding \nloadmodule redisearch.so SAFEMODE\n to your redis.conf - depending on how you load the module.\n\n\n6. Some Numbers!\n\n\nI've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.\n\n\n\n\nBenchmark Setup\n\n\n\n\nThe data-set consists of about 1,000,000 Reddit comments.\n\n\nTwo clients using Redis-benchmark were running  - first separately, then in parallel:\n\n\nOne client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.\n\n\nOne client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).\n\n\nBoth clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.\n\n\n\n\n\n\nThe Results:\n\n\n\n\n\n\n\n\nNote\n\n\nWhile we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries. \n\n\n\n\n7. Parting Words\n\n\nThis little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.\n\n\nFor RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#multi-threading\"_\"in\"_\"redisearch", 
            "text": "By Dvir Volk, July 2017", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#1\"_\"one\"_\"thread\"_\"to\"_\"rule\"_\"them\"_\"all", 
            "text": "Redis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with.   While keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like  ZUNIONSTORE ,  LRANGE ,  SINTER  and of course the infamous  KEYS , can block Redis for seconds or minutes, depending on the size of data they are handling.", 
            "title": "1. One Thread To Rule Them All"
        }, 
        {
            "location": "/Threading/#2\"_\"redisearch\"_\"and\"_\"the\"_\"single\"_\"thread\"_\"issue", 
            "text": "RediSearch  is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine.   While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked.   Think, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond,  which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.  So taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.", 
            "title": "2. RediSearch and the Single Thread Issue"
        }, 
        {
            "location": "/Threading/#3\"_\"enter\"_\"the\"_\"redis\"_\"gil", 
            "text": "Luckily, Redis BDFL  Salvatore Sanfilippo  has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API -  Thread Safe Contexts  and the  Global Lock .  The idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the  Global Lock  when it needs to access Redis data, operate on it, and release it.   We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.", 
            "title": "3. Enter the Redis GIL"
        }, 
        {
            "location": "/Threading/#4\"_\"making\"_\"search\"_\"concurrent", 
            "text": "Up until now, the flow of a search query was simple - the query would arrive at a  Command Handler  callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result.   To allow concurrency, we adapted the following design:    RediSearch has a thread pool for running concurrent search queries.     When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.    The thread pool runs a query processing function in its own thread.    The function locks the Redis Global lock, and starts executing the query.    Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).    If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.    When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state.     Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently.", 
            "title": "4. Making Search Concurrent"
        }, 
        {
            "location": "/Threading/#figure\"_\"1\"_\"serial\"_\"vs\"_\"concurrent\"_\"search", 
            "text": "On the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.   The same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.  As a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.", 
            "title": "Figure 1: Serial vs. Concurrent Search"
        }, 
        {
            "location": "/Threading/#5\"_\"the\"_\"effect\"_\"of\"_\"concurrency", 
            "text": "While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running  KEYS *  in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!  There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\".   This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation.   To enable safe mode and disable query concurrency, you can configure RediSearch at load time:  redis-server --loadmodule redisearch.so SAFEMODE  in command line, or by adding  loadmodule redisearch.so SAFEMODE  to your redis.conf - depending on how you load the module.", 
            "title": "5. The Effect of Concurrency"
        }, 
        {
            "location": "/Threading/#6\"_\"some\"_\"numbers", 
            "text": "I've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.   Benchmark Setup   The data-set consists of about 1,000,000 Reddit comments.  Two clients using Redis-benchmark were running  - first separately, then in parallel:  One client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.  One client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).  Both clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.", 
            "title": "6. Some Numbers!"
        }, 
        {
            "location": "/Threading/#the\"_\"results", 
            "text": "Note  While we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries.", 
            "title": "The Results:"
        }, 
        {
            "location": "/Threading/#7\"_\"parting\"_\"words", 
            "text": "This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.  For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "7. Parting Words"
        }
    ]
}