{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RediSearch - Redis Powered Search Engine RediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by Redis Labs . Quick Links: Source Code at GitHub . Latest Release: 1.4.2 Docker Image: redislabs/redisearch Quick Start Guide Mailing list / Forum Supported Platforms RediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs. i386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently. Overview Redisearch implements a search engine on top of Redis, but unlike other Redis search libraries, it does not use internal data structures like sorted sets. This also enables more advanced features, like exact phrase matching and numeric filtering for text queries, that are not possible or efficient with traditional Redis search approaches. Client Libraries Official and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. See the Clients page for the full list. Cluster Support and Commercial Version RediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial support for RediSearch. See the Redis Labs Website for more info and contact information. Primary Features Full-Text indexing of multiple fields in documents. Incremental indexing without performance loss. Document ranking (provided manually by the user at index time). Complex boolean queries with AND, OR, NOT operators between sub-queries. Optional query clauses. Prefix based searches. Field weights. Auto-complete suggestions (with fuzzy prefix suggestions). Exact Phrase Search, Slop based search. Stemming based query expansion in many languages (using Snowball ). Support for custom functions for query expansion and scoring (see Extensions ). Limiting searches to specific document fields. Numeric filters and ranges. Geo filtering using Redis' own Geo-commands. Unicode support (UTF-8 input required). Retrieve full document content or just ids Document deletion and updating with index garbage collection. Partial and conditional document updates.","title":"Home"},{"location":"#redisearch_-_redis_powered_search_engine","text":"RediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by Redis Labs . Quick Links: Source Code at GitHub . Latest Release: 1.4.2 Docker Image: redislabs/redisearch Quick Start Guide Mailing list / Forum Supported Platforms RediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs. i386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently.","title":"RediSearch - Redis Powered Search Engine"},{"location":"#overview","text":"Redisearch implements a search engine on top of Redis, but unlike other Redis search libraries, it does not use internal data structures like sorted sets. This also enables more advanced features, like exact phrase matching and numeric filtering for text queries, that are not possible or efficient with traditional Redis search approaches.","title":"Overview"},{"location":"#client_libraries","text":"Official and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. See the Clients page for the full list.","title":"Client Libraries"},{"location":"#cluster_support_and_commercial_version","text":"RediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial support for RediSearch. See the Redis Labs Website for more info and contact information.","title":"Cluster Support and Commercial Version"},{"location":"#primary_features","text":"Full-Text indexing of multiple fields in documents. Incremental indexing without performance loss. Document ranking (provided manually by the user at index time). Complex boolean queries with AND, OR, NOT operators between sub-queries. Optional query clauses. Prefix based searches. Field weights. Auto-complete suggestions (with fuzzy prefix suggestions). Exact Phrase Search, Slop based search. Stemming based query expansion in many languages (using Snowball ). Support for custom functions for query expansion and scoring (see Extensions ). Limiting searches to specific document fields. Numeric filters and ranges. Geo filtering using Redis' own Geo-commands. Unicode support (UTF-8 input required). Retrieve full document content or just ids Document deletion and updating with index garbage collection. Partial and conditional document updates.","title":"Primary Features"},{"location":"Administration/","text":"RediSearch Administration Guide RediSearch doesn't require any configuration to work, but there are a few things worth noting when running RediSearch on top of Redis. Persistence RediSearch supports both RDB and AOF based persistence. For a pure RDB set-up, nothing special is needed beyond the standard Redis RDB configuration. AOF Persistence While RediSearch supports working with AOF based persistence, as of version 1.1.0 it does not support \"classic AOF\" mode, which uses AOF rewriting. Instead, it only supports AOF with RDB preamble mode. In this mode, rewriting the AOF log just creates an RDB file, which is appended to. To enable AOF persistence with RediSearch, add the two following lines to your redis.conf: appendonly yes aof-use-rdb-preamble yes Master/Slave Replication RediSearch supports replication inherently, and using a master/slave set-up, you can use slaves for high availability. On top of that, slaves can be used for searching, to load-balance read traffic. Cluster Support RediSearch will not work correctly on a cluster. The enterprise version of RediSearch, which is commercially available from Redis Labs, does support a cluster set up and scales to hundreds of nodes, billions of documents and terabytes of data. See the Redis Labs Website for more details.","title":"Administration"},{"location":"Administration/#redisearch_administration_guide","text":"RediSearch doesn't require any configuration to work, but there are a few things worth noting when running RediSearch on top of Redis.","title":"RediSearch Administration Guide"},{"location":"Administration/#persistence","text":"RediSearch supports both RDB and AOF based persistence. For a pure RDB set-up, nothing special is needed beyond the standard Redis RDB configuration.","title":"Persistence"},{"location":"Administration/#aof_persistence","text":"While RediSearch supports working with AOF based persistence, as of version 1.1.0 it does not support \"classic AOF\" mode, which uses AOF rewriting. Instead, it only supports AOF with RDB preamble mode. In this mode, rewriting the AOF log just creates an RDB file, which is appended to. To enable AOF persistence with RediSearch, add the two following lines to your redis.conf: appendonly yes aof-use-rdb-preamble yes","title":"AOF Persistence"},{"location":"Administration/#masterslave_replication","text":"RediSearch supports replication inherently, and using a master/slave set-up, you can use slaves for high availability. On top of that, slaves can be used for searching, to load-balance read traffic.","title":"Master/Slave Replication"},{"location":"Administration/#cluster_support","text":"RediSearch will not work correctly on a cluster. The enterprise version of RediSearch, which is commercially available from Redis Labs, does support a cluster set up and scales to hundreds of nodes, billions of documents and terabytes of data. See the Redis Labs Website for more details.","title":"Cluster Support"},{"location":"Aggregations/","text":"RediSearch Aggregations Aggregations are a way to process the results of a search query, group, sort and transform them - and extract analytic insights from them. Much like aggregation queries in other databases and search engines, they can be used to create analytics reports, or perform Faceted Search style queries. For example, indexing a web-server's logs, we can create a report for unique users by hour, country or any other breakdown; or create different reports for errors, warnings, etc. Core concepts The basic idea of an aggregate query is this: Perform a search query, filtering for records you wish to process. Build a pipeline of operations that transform the results by zero or more steps of: Group and Reduce : grouping by fields in the results, and applying reducer functions on each group. Sort : sort the results based on one or more fields. Apply Transformations : Apply mathematical and string functions on fields in the pipeline, optionally creating new fields or replacing existing ones Limit : Limit the result, regardless of sorting the result. Filter : Filter the results (post-query) based on predicates relating to its values. The pipeline is dynamic and reentrant, and every operation can be repeated. For example, you can group by property X, sort the top 100 results by group size, then group by property Y and sort the results by some other property, then apply a transformation on the output. Figure 1: Aggregation Pipeline Example Aggregate request format The aggregate request's syntax is defined as follows: FT . AGGREGATE { index_name : string } { query_string : string } [ WITHSCHEMA ] [ VERBATIM ] [ LOAD { nargs : integer } { property : string } ...] [ GROUPBY { nargs : integer } { property : string } ... REDUCE { FUNC : string } { nargs : integer } { arg : string } ... [ AS { name : string } ] ... ] ... [ SORTBY { nargs : integer } { string } ... [ MAX { num : integer } ] ... ] ... [ APPLY { EXPR : string } AS { name : string } ] ... [ FILTER { EXPR : string } ] ... [ LIMIT { offset : integer } { num : integer } ] ... Parameters in detail Parameters which may take a variable number of arguments are expressed in the form of param {nargs} {property_1... property_N} . The first argument to the parameter is the number of arguments following the parameter. This allows RediSearch to avoid a parsing ambiguity in case one of your arguments has the name of another parameter. For example, to sort by first name, last name, and country, one would specify SORTBY 6 firstName ASC lastName DESC country ASC . index_name : The index the query is executed again. query_string : The base filtering query that retrieves the documents. It follows the exact same syntax as the search query, including filters, unions, not, optional, etc. LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as SORTABLE , where they are available to the aggregation pipeline with very low latency. LOAD hurts the performance of aggregate queries considerably since every processed record needs to execute the equivalent of HMGET against a redis key, which when executed over millions of keys, amounts to very high processing times. GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them or performing multiple aggregate operations (see below). REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example, COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers. The reducers can have their own property names using the AS {name} optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property @foo , the resulting name will be count_distinct(@foo) . SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but ASC or DESC can be added for each property. nargs is the number of sorting parameters, including ASC and DESC. for example: SORTBY 4 @foo ASC @bar DESC . MAX is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to LIMIT , you usually need just SORTBY \u2026 MAX for common queries. APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation. expr is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example: APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline. LIMIT {offset} {num} . Limit the number of results to return just num results starting at index offset (zero based). AS mentioned above, it is much more efficient to use SORTBY \u2026 MAX if you are interested in just limiting the optput of a sort operation. However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting all the records and then paging over results 50-100. FILTER {expr} . Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline. See FILTER Expressions below for full details. Quick example Let's assume we have log of visits to our website, each record containing the following fields/properties: url (text, sortable) timestamp (numeric, sortable) - unix timestamp of visit entry. country (tag, sortable) user_id (text, sortable, not indexed) Example 1: unique users by hour, ordered chronologically. First of all, we want all records in the index, because why not. The first step is to determine the index name and the filtering query. A filter query of * means \"get all records\": FT.AGGREGATE myIndex * Now we want to group the results by hour. Since we have the visit times as unix timestamps in second resolution, we need to extract the hour component of the timestamp. So we first add an APPLY step, that strips the sub-hour information from the timestamp and stores is as a new property, hour : FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour Now we want to group the results by hour, and count the distinct user ids in each hour. This is done by a GROUPBY/REDUCE step: FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users Now we'd like to sort the results by hour, ascending: FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users SORTBY 2 @hour ASC And as a final step, we can format the hour as a human readable timestamp. This is done by calling the transformation function timefmt that formats unix timestamps. You can specify a format to be passed to the system's strftime function ( see documentation ), but not specifying one is equivalent to specifying %FT%TZ to strftime . FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users SORTBY 2 @hour ASC APPLY timefmt(@hour) AS hour Example 2: Sort visits to a specific URL by day and country: In this example we filter by the url, transform the timestamp to its day part, and group by the day and country, simply counting the number of visits per group. sorting by day ascending and country descending. FT.AGGREGATE myIndex @url:\\ about.html\\ APPLY @timestamp - (@timestamp % 86400) AS day GROUPBY 2 @day @country REDUCE count 0 AS num_visits SORTBY 4 @day ASC @country DESC GROUPBY reducers GROUPBY step work similarly to SQL GROUP BY clauses, and create groups of results based on one or more properties in each record. For each group, we return the \"group keys\", or the values common to all records in the group, by which they were grouped together - along with the results of zero or more REDUCE clauses. Each GROUPBY step in the pipeline may be accompanied by zero or more REDUCE clauses. Reducers apply some accumulation function to each record in the group and reduce them into a single record representing the group. When we are finished processing all the records upstream of the GROUPBY step, each group emits its reduced record. For example, the simplest reducer is COUNT, which simply counts the number of records in each group. If multiple REDUCE clauses exist for a single GROUPBY step, each reducer works independently on each result and writes its final output once. Each reducer may have its own alias determined using the AS optional parameter. If AS is not specified, the alias is the reduce function and its parameters, e.g. count_distinct(foo,bar) . Supported GROUPBY reducers COUNT Format REDUCE COUNT 0 Description Count the number of records in each group COUNT_DISTINCT Format REDUCE COUNT_DISTINCT 1 {property} Description Count the number of distinct values for property . Note The reducer creates a hash-set per group, and hashes each record. This can be memory heavy if the groups are big. COUNT_DISTINCTISH Format REDUCE COUNT_DISTINCTISH 1 {property} Description Same as COUNT_DISTINCT - but provide an approximation instead of an exact count, at the expense of less memory and CPU in big groups. Note The reducer uses HyperLogLog counters per group, at ~3% error rate, and 1024 Bytes of constant space allocation per group. This means it is ideal for few huge groups and not ideal for many small groups. In the former case, it can be an order of magnitude faster and consume much less memory than COUNT_DISTINCT, but again, it does not fit every user case. SUM Format REDUCE SUM 1 {property} Description Return the sum of all numeric values of a given property in a group. Non numeric values if the group are counted as 0. MIN Format REDUCE MIN 1 {property} Description Return the minimal value of a property, whether it is a string, number or NULL. MAX Format REDUCE MAX 1 {property} Description Return the maximal value of a property, whether it is a string, number or NULL. AVG Format REDUCE AVG 1 {property} Description Return the average value of a numeric property. This is equivalent to reducing by sum and count, and later on applying the ratio of them as an APPLY step. STDDEV Format REDUCE STDDEV 1 {property} Description Return the standard deviation of a numeric property in the group. QUANTILE Format REDUCE QUANTILE 2 {property} {quantile} Description Return the value of a numeric property at a given quantile of the results. Quantile is expressed as a number between 0 and 1. For example, the median can be expressed as the quantile at 0.5, e.g. REDUCE QUANTILE 2 @foo 0.5 AS median . If multiple quantiles are required, just repeat the QUANTILE reducer for each quantile. e.g. REDUCE QUANTILE 2 @foo 0.5 AS median REDUCE QUANTILE 2 @foo 0.99 AS p99 TOLIST Format REDUCE TOLIST 1 {property} Description Merge all distinct values of a given property into a single array. FIRST_VALUE Format REDUCE FIRST_VALUE {nargs} {property} [BY {property} [ASC|DESC]] Description Return the first or top value of a given property in the group, optionally by comparing that or another property. For example, you can extract the name of the oldest user in the group: REDUCE FIRST_VALUE 4 @name BY @age DESC If no BY is specified, we return the first value we encounter in the group. If you with to get the top or bottom value in the group sorted by the same value, you are better off using the MIN/MAX reducers, but the same effect will be achieved by doing REDUCE FIRST_VALUE 4 @foo BY @foo DESC . RANDOM_SAMPLE Format REDUCE RANDOM_SAMPLE {nargs} {property} {sample_size} Description Perform a reservoir sampling of the group elements with a given size, and return an array of the sampled items with an even distribution. APPLY expressions APPLY performs a 1-to-1 transformation on one or more properties in each record. It either stores the result as a new property down the pipeline, or replaces any property using this transformation. The transformations are expressed as a combination of arithmetic expressions and built in functions. Evaluating functions and expressions is recursively nested and can be composed without limit. For example: sqrt(log(foo) * floor(@bar/baz)) + (3^@qaz % 6) or simply @foo/@bar . If an expression or a function is applied to values that do not match the expected types, no error is emitted but a NULL value is set as the result. APPLY steps must have an explicit alias determined by the AS parameter. Literals inside expressions Numbers are expressed as integers or floating point numbers, i.e. 2 , 3.141 , -34 , etc. inf and -inf are acceptable as well. Strings are quoted with either single or double quotes. Single quotes are acceptable inside strings quoted with double quotes and vice versa. Punctuation marks can be escaped with backslashes. e.g. \"foo's bar\" , 'foo\\'s bar' , \"foo \\\"bar\\\"\" . Any literal or sub expression can be wrapped in parentheses to resolve ambiguities of operator precedence. Arithmetic operations For numeric expressions and properties, we support addition ( + ), subtraction ( - ), multiplication ( * ), division ( / ), modulo ( % ) and power ( ^ ). We currently do not support bitwise logical operators. Note that these operators apply only to numeric values and numeric sub expressions. Any attempt to multiply a string by a number, for instance, will result in a NULL output. List of numeric APPLY functions Function Description Example log(x) Return the logarithm of a number, property or sub-expression log(@foo) abs(x) Return the absolute number of a numeric expression abs(@foo-@bar) ceil(x) Round to the smallest value not less than x ceil(@foo/3.14) floor(x) Round to largest value not greater than x floor(@foo/3.14) log2(x) Return the logarithm of x to base 2 log2(2^@foo) exp(x) Return the exponent of x, i.e. e^x exp(@foo) sqrt(x) Return the square root of x sqrt(@foo) List of string APPLY functions Function upper(s) Return the uppercase conversion of s upper('hello world') lower(s) Return the lowercase conversion of 2 lower(\"HELLO WORLD\") substr(s, offset, count) Return the substring of s, starting at offset and having count characters. If offset is negative, it represents the distance from the end of the string. If count is -1, it means \"the rest of the string starting at offset\". substr(\"hello\", 0, 3) substr(\"hello\", -2, -1) format( fmt, ...) Use the arguments following fmt to format a string. Currently the only format argument supported is %s and it applies to all types of arguments. format(\"Hello, %s, you are %s years old\", @name, @age) matched_terms([max_terms=100]) Return the query terms that matched for each record (up to 100), as a list. If a limit is specified, we will return the first N matches we find - based on query order. matched_terms() split(s, [sep=\",\"], [strip=\" \"]) Split a string by any character in the string sep, and strip any characters in strip. If only s is specified, we split by commas and strip spaces. The output is an array. split(\"foo,bar\") List of date/time APPLY functions Function Description timefmt(x, [fmt]) Return a formatted time string based on a numeric timestamp value x. See strftime for formatting options. Not specifying fmt is equivalent to %FT%TZ . parsetime(timesharing, [fmt]) The opposite of timefmt() - parse a time format using a given format string day(timestamp) Round a Unix timestamp to midnight (00:00) start of the current day. hour(timestamp) Round a Unix timestamp to the beginning of the current hour. minute(timestamp) Round a Unix timestamp to the beginning of the current minute. month(timestamp) Round a unix timestamp to the beginning of the current month. dayofweek(timestamp) Convert a Unix timestamp to the day number (Sunday = 0). dayofmonth(timestamp) Convert a Unix timestamp to the day of month number (1 .. 31). dayofyear(timestamp) Convert a Unix timestamp to the day of year number (0 .. 365). year(timestamp) Convert a Unix timestamp to the current year (e.g. 2018). monthofyear(timestamp) Convert a Unix timestamp to the current month (0 .. 11). FILTER expressions FILTER expressions filter the results using predicates relating to values in the result set. The FILTER expressions are evaluated post-query and relate to the current state of the pipeline. Thus they can be useful to prune the results based on group calculations. Note that the filters are not indexed and will not speed the processing per se. Filter expressions follow the syntax of APPLY expressions, with the addition of the conditions == , != , , = , , = . Two or more predicates can be combined with logical AND ( ) and OR ( || ). A single predicate can be negated with a NOT prefix ( ! ). For example, filtering all results where the user name is 'foo' and the age is less than 20 is expressed as: FT.AGGREGATE ... FILTER @name== foo @age 20 ... Several filter steps can be added, although at the same stage in the pipeline, it is more efficient to combine several predicates into a single filter step. Cursor API FT.AGGREGATE ... WITHCURSOR [COUNT {read size} MAXIDLE {idle timeout}] FT.CURSOR READ {idx} {cid} [COUNT {read size}] FT.CURSOR DEL {idx} {cid} You can use cursors with FT.AGGREGATE , with the WITHCURSOR keyword. Cursors allow you to consume only part of the response, allowing you to fetch additional results as needed. This is much quicker than using LIMIT with offset, since the query is executed only once, and its state is stored on the server. To use cursors, specify the WITHCURSOR keyword in FT.AGGREGATE , e.g. FT.AGGREGATE idx * WITHCURSOR This will return a response of an array with two elements. The first element is the actual (partial) results, and the second is the cursor ID. The cursor ID can then be fed to FT.CURSOR READ repeatedly, until the cursor ID is 0, in which case all results have been returned. To read from an existing cursor, use FT.CURSOR READ , e.g. FT.CURSOR READ idx 342459320 Assuming 342459320 is the cursor ID returned from the FT.AGGREGATE request. Here is an example in pseudo-code: response, cursor = FT.AGGREGATE idx redis WITHCURSOR ; while (1) { processResponse(response) if (!cursor) { break; } response, cursor = FT.CURSOR read idx cursor } Note that even if the cursor is 0, a partial result may still be returned. Cursor settings Read size You can control how many rows are read per each cursor fetch by using the COUNT parameter. This parameter can be specified both in FT.AGGREGATE (immediately after WITHCURSOR ) or in FT.CURSOR READ . FT.AGGREGATE idx query WITHCURSOR COUNT 10 Will read 10 rows at a time. You can override this setting by also specifying COUNT in CURSOR READ , e.g. FT.CURSOR READ idx 342459320 COUNT 50 Will return at most 50 results. The default read size is 1000 Timeouts and limits Because cursors are stateful resources which occupy memory on the server, they have a limited lifetime. In order to safeguard against orphaned/stale cursors, cursors have an idle timeout value. If no activity occurs on the cursor before the idle timeout, the cursor is deleted. The idle timer resets to 0 whenever the cursor is read from using CURSOR READ . The default idle timeout is 30000 milliseconds (or 30 seconds). You can modify the idle timeout using the MAXIDLE keyword when creating the cursor. Note that the value cannot exceed the default 30s. FT.AGGREGATE idx query WITHCURSOR MAXIDLE 10000 Will set the limit for 10 seconds. Other cursor commands Cursors can be explicity deleted using the CURSOR DEL command, e.g. FT.CURSOR DEL idx 342459320 Note that cursors are automatically deleted if all their results have been returned, or if they have been timed out. All idle cursors can be forcefully purged at once using FT.CURSOR GC idx 0 command. By default, RediSearch uses a lazy throttled approach to garbage collection, which collects idle cursors every 500 operations, or every second - whichever is later.","title":"Aggregations"},{"location":"Aggregations/#redisearch_aggregations","text":"Aggregations are a way to process the results of a search query, group, sort and transform them - and extract analytic insights from them. Much like aggregation queries in other databases and search engines, they can be used to create analytics reports, or perform Faceted Search style queries. For example, indexing a web-server's logs, we can create a report for unique users by hour, country or any other breakdown; or create different reports for errors, warnings, etc.","title":"RediSearch Aggregations"},{"location":"Aggregations/#core_concepts","text":"The basic idea of an aggregate query is this: Perform a search query, filtering for records you wish to process. Build a pipeline of operations that transform the results by zero or more steps of: Group and Reduce : grouping by fields in the results, and applying reducer functions on each group. Sort : sort the results based on one or more fields. Apply Transformations : Apply mathematical and string functions on fields in the pipeline, optionally creating new fields or replacing existing ones Limit : Limit the result, regardless of sorting the result. Filter : Filter the results (post-query) based on predicates relating to its values. The pipeline is dynamic and reentrant, and every operation can be repeated. For example, you can group by property X, sort the top 100 results by group size, then group by property Y and sort the results by some other property, then apply a transformation on the output. Figure 1: Aggregation Pipeline Example","title":"Core concepts"},{"location":"Aggregations/#aggregate_request_format","text":"The aggregate request's syntax is defined as follows: FT . AGGREGATE { index_name : string } { query_string : string } [ WITHSCHEMA ] [ VERBATIM ] [ LOAD { nargs : integer } { property : string } ...] [ GROUPBY { nargs : integer } { property : string } ... REDUCE { FUNC : string } { nargs : integer } { arg : string } ... [ AS { name : string } ] ... ] ... [ SORTBY { nargs : integer } { string } ... [ MAX { num : integer } ] ... ] ... [ APPLY { EXPR : string } AS { name : string } ] ... [ FILTER { EXPR : string } ] ... [ LIMIT { offset : integer } { num : integer } ] ...","title":"Aggregate request format"},{"location":"Aggregations/#parameters_in_detail","text":"Parameters which may take a variable number of arguments are expressed in the form of param {nargs} {property_1... property_N} . The first argument to the parameter is the number of arguments following the parameter. This allows RediSearch to avoid a parsing ambiguity in case one of your arguments has the name of another parameter. For example, to sort by first name, last name, and country, one would specify SORTBY 6 firstName ASC lastName DESC country ASC . index_name : The index the query is executed again. query_string : The base filtering query that retrieves the documents. It follows the exact same syntax as the search query, including filters, unions, not, optional, etc. LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as SORTABLE , where they are available to the aggregation pipeline with very low latency. LOAD hurts the performance of aggregate queries considerably since every processed record needs to execute the equivalent of HMGET against a redis key, which when executed over millions of keys, amounts to very high processing times. GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them or performing multiple aggregate operations (see below). REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example, COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers. The reducers can have their own property names using the AS {name} optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property @foo , the resulting name will be count_distinct(@foo) . SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but ASC or DESC can be added for each property. nargs is the number of sorting parameters, including ASC and DESC. for example: SORTBY 4 @foo ASC @bar DESC . MAX is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to LIMIT , you usually need just SORTBY \u2026 MAX for common queries. APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation. expr is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example: APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline. LIMIT {offset} {num} . Limit the number of results to return just num results starting at index offset (zero based). AS mentioned above, it is much more efficient to use SORTBY \u2026 MAX if you are interested in just limiting the optput of a sort operation. However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting all the records and then paging over results 50-100. FILTER {expr} . Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline. See FILTER Expressions below for full details.","title":"Parameters in detail"},{"location":"Aggregations/#quick_example","text":"Let's assume we have log of visits to our website, each record containing the following fields/properties: url (text, sortable) timestamp (numeric, sortable) - unix timestamp of visit entry. country (tag, sortable) user_id (text, sortable, not indexed)","title":"Quick example"},{"location":"Aggregations/#example_1_unique_users_by_hour_ordered_chronologically","text":"First of all, we want all records in the index, because why not. The first step is to determine the index name and the filtering query. A filter query of * means \"get all records\": FT.AGGREGATE myIndex * Now we want to group the results by hour. Since we have the visit times as unix timestamps in second resolution, we need to extract the hour component of the timestamp. So we first add an APPLY step, that strips the sub-hour information from the timestamp and stores is as a new property, hour : FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour Now we want to group the results by hour, and count the distinct user ids in each hour. This is done by a GROUPBY/REDUCE step: FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users Now we'd like to sort the results by hour, ascending: FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users SORTBY 2 @hour ASC And as a final step, we can format the hour as a human readable timestamp. This is done by calling the transformation function timefmt that formats unix timestamps. You can specify a format to be passed to the system's strftime function ( see documentation ), but not specifying one is equivalent to specifying %FT%TZ to strftime . FT.AGGREGATE myIndex * APPLY @timestamp - (@timestamp % 3600) AS hour GROUPBY 1 @hour REDUCE COUNT_DISTINCT 1 @user_id AS num_users SORTBY 2 @hour ASC APPLY timefmt(@hour) AS hour","title":"Example 1: unique users by hour, ordered chronologically."},{"location":"Aggregations/#example_2_sort_visits_to_a_specific_url_by_day_and_country","text":"In this example we filter by the url, transform the timestamp to its day part, and group by the day and country, simply counting the number of visits per group. sorting by day ascending and country descending. FT.AGGREGATE myIndex @url:\\ about.html\\ APPLY @timestamp - (@timestamp % 86400) AS day GROUPBY 2 @day @country REDUCE count 0 AS num_visits SORTBY 4 @day ASC @country DESC","title":"Example 2: Sort visits to a specific URL by day and country:"},{"location":"Aggregations/#groupby_reducers","text":"GROUPBY step work similarly to SQL GROUP BY clauses, and create groups of results based on one or more properties in each record. For each group, we return the \"group keys\", or the values common to all records in the group, by which they were grouped together - along with the results of zero or more REDUCE clauses. Each GROUPBY step in the pipeline may be accompanied by zero or more REDUCE clauses. Reducers apply some accumulation function to each record in the group and reduce them into a single record representing the group. When we are finished processing all the records upstream of the GROUPBY step, each group emits its reduced record. For example, the simplest reducer is COUNT, which simply counts the number of records in each group. If multiple REDUCE clauses exist for a single GROUPBY step, each reducer works independently on each result and writes its final output once. Each reducer may have its own alias determined using the AS optional parameter. If AS is not specified, the alias is the reduce function and its parameters, e.g. count_distinct(foo,bar) .","title":"GROUPBY reducers"},{"location":"Aggregations/#supported_groupby_reducers","text":"","title":"Supported GROUPBY reducers"},{"location":"Aggregations/#count","text":"Format REDUCE COUNT 0 Description Count the number of records in each group","title":"COUNT"},{"location":"Aggregations/#count_distinct","text":"Format REDUCE COUNT_DISTINCT 1 {property} Description Count the number of distinct values for property . Note The reducer creates a hash-set per group, and hashes each record. This can be memory heavy if the groups are big.","title":"COUNT_DISTINCT"},{"location":"Aggregations/#count_distinctish","text":"Format REDUCE COUNT_DISTINCTISH 1 {property} Description Same as COUNT_DISTINCT - but provide an approximation instead of an exact count, at the expense of less memory and CPU in big groups. Note The reducer uses HyperLogLog counters per group, at ~3% error rate, and 1024 Bytes of constant space allocation per group. This means it is ideal for few huge groups and not ideal for many small groups. In the former case, it can be an order of magnitude faster and consume much less memory than COUNT_DISTINCT, but again, it does not fit every user case.","title":"COUNT_DISTINCTISH"},{"location":"Aggregations/#sum","text":"Format REDUCE SUM 1 {property} Description Return the sum of all numeric values of a given property in a group. Non numeric values if the group are counted as 0.","title":"SUM"},{"location":"Aggregations/#min","text":"Format REDUCE MIN 1 {property} Description Return the minimal value of a property, whether it is a string, number or NULL.","title":"MIN"},{"location":"Aggregations/#max","text":"Format REDUCE MAX 1 {property} Description Return the maximal value of a property, whether it is a string, number or NULL.","title":"MAX"},{"location":"Aggregations/#avg","text":"Format REDUCE AVG 1 {property} Description Return the average value of a numeric property. This is equivalent to reducing by sum and count, and later on applying the ratio of them as an APPLY step.","title":"AVG"},{"location":"Aggregations/#stddev","text":"Format REDUCE STDDEV 1 {property} Description Return the standard deviation of a numeric property in the group.","title":"STDDEV"},{"location":"Aggregations/#quantile","text":"Format REDUCE QUANTILE 2 {property} {quantile} Description Return the value of a numeric property at a given quantile of the results. Quantile is expressed as a number between 0 and 1. For example, the median can be expressed as the quantile at 0.5, e.g. REDUCE QUANTILE 2 @foo 0.5 AS median . If multiple quantiles are required, just repeat the QUANTILE reducer for each quantile. e.g. REDUCE QUANTILE 2 @foo 0.5 AS median REDUCE QUANTILE 2 @foo 0.99 AS p99","title":"QUANTILE"},{"location":"Aggregations/#tolist","text":"Format REDUCE TOLIST 1 {property} Description Merge all distinct values of a given property into a single array.","title":"TOLIST"},{"location":"Aggregations/#first_value","text":"Format REDUCE FIRST_VALUE {nargs} {property} [BY {property} [ASC|DESC]] Description Return the first or top value of a given property in the group, optionally by comparing that or another property. For example, you can extract the name of the oldest user in the group: REDUCE FIRST_VALUE 4 @name BY @age DESC If no BY is specified, we return the first value we encounter in the group. If you with to get the top or bottom value in the group sorted by the same value, you are better off using the MIN/MAX reducers, but the same effect will be achieved by doing REDUCE FIRST_VALUE 4 @foo BY @foo DESC .","title":"FIRST_VALUE"},{"location":"Aggregations/#random_sample","text":"Format REDUCE RANDOM_SAMPLE {nargs} {property} {sample_size} Description Perform a reservoir sampling of the group elements with a given size, and return an array of the sampled items with an even distribution.","title":"RANDOM_SAMPLE"},{"location":"Aggregations/#apply_expressions","text":"APPLY performs a 1-to-1 transformation on one or more properties in each record. It either stores the result as a new property down the pipeline, or replaces any property using this transformation. The transformations are expressed as a combination of arithmetic expressions and built in functions. Evaluating functions and expressions is recursively nested and can be composed without limit. For example: sqrt(log(foo) * floor(@bar/baz)) + (3^@qaz % 6) or simply @foo/@bar . If an expression or a function is applied to values that do not match the expected types, no error is emitted but a NULL value is set as the result. APPLY steps must have an explicit alias determined by the AS parameter.","title":"APPLY expressions"},{"location":"Aggregations/#literals_inside_expressions","text":"Numbers are expressed as integers or floating point numbers, i.e. 2 , 3.141 , -34 , etc. inf and -inf are acceptable as well. Strings are quoted with either single or double quotes. Single quotes are acceptable inside strings quoted with double quotes and vice versa. Punctuation marks can be escaped with backslashes. e.g. \"foo's bar\" , 'foo\\'s bar' , \"foo \\\"bar\\\"\" . Any literal or sub expression can be wrapped in parentheses to resolve ambiguities of operator precedence.","title":"Literals inside expressions"},{"location":"Aggregations/#arithmetic_operations","text":"For numeric expressions and properties, we support addition ( + ), subtraction ( - ), multiplication ( * ), division ( / ), modulo ( % ) and power ( ^ ). We currently do not support bitwise logical operators. Note that these operators apply only to numeric values and numeric sub expressions. Any attempt to multiply a string by a number, for instance, will result in a NULL output.","title":"Arithmetic operations"},{"location":"Aggregations/#list_of_numeric_apply_functions","text":"Function Description Example log(x) Return the logarithm of a number, property or sub-expression log(@foo) abs(x) Return the absolute number of a numeric expression abs(@foo-@bar) ceil(x) Round to the smallest value not less than x ceil(@foo/3.14) floor(x) Round to largest value not greater than x floor(@foo/3.14) log2(x) Return the logarithm of x to base 2 log2(2^@foo) exp(x) Return the exponent of x, i.e. e^x exp(@foo) sqrt(x) Return the square root of x sqrt(@foo)","title":"List of numeric APPLY functions"},{"location":"Aggregations/#list_of_string_apply_functions","text":"Function upper(s) Return the uppercase conversion of s upper('hello world') lower(s) Return the lowercase conversion of 2 lower(\"HELLO WORLD\") substr(s, offset, count) Return the substring of s, starting at offset and having count characters. If offset is negative, it represents the distance from the end of the string. If count is -1, it means \"the rest of the string starting at offset\". substr(\"hello\", 0, 3) substr(\"hello\", -2, -1) format( fmt, ...) Use the arguments following fmt to format a string. Currently the only format argument supported is %s and it applies to all types of arguments. format(\"Hello, %s, you are %s years old\", @name, @age) matched_terms([max_terms=100]) Return the query terms that matched for each record (up to 100), as a list. If a limit is specified, we will return the first N matches we find - based on query order. matched_terms() split(s, [sep=\",\"], [strip=\" \"]) Split a string by any character in the string sep, and strip any characters in strip. If only s is specified, we split by commas and strip spaces. The output is an array. split(\"foo,bar\")","title":"List of string APPLY functions"},{"location":"Aggregations/#list_of_datetime_apply_functions","text":"Function Description timefmt(x, [fmt]) Return a formatted time string based on a numeric timestamp value x. See strftime for formatting options. Not specifying fmt is equivalent to %FT%TZ . parsetime(timesharing, [fmt]) The opposite of timefmt() - parse a time format using a given format string day(timestamp) Round a Unix timestamp to midnight (00:00) start of the current day. hour(timestamp) Round a Unix timestamp to the beginning of the current hour. minute(timestamp) Round a Unix timestamp to the beginning of the current minute. month(timestamp) Round a unix timestamp to the beginning of the current month. dayofweek(timestamp) Convert a Unix timestamp to the day number (Sunday = 0). dayofmonth(timestamp) Convert a Unix timestamp to the day of month number (1 .. 31). dayofyear(timestamp) Convert a Unix timestamp to the day of year number (0 .. 365). year(timestamp) Convert a Unix timestamp to the current year (e.g. 2018). monthofyear(timestamp) Convert a Unix timestamp to the current month (0 .. 11).","title":"List of date/time APPLY functions"},{"location":"Aggregations/#filter_expressions","text":"FILTER expressions filter the results using predicates relating to values in the result set. The FILTER expressions are evaluated post-query and relate to the current state of the pipeline. Thus they can be useful to prune the results based on group calculations. Note that the filters are not indexed and will not speed the processing per se. Filter expressions follow the syntax of APPLY expressions, with the addition of the conditions == , != , , = , , = . Two or more predicates can be combined with logical AND ( ) and OR ( || ). A single predicate can be negated with a NOT prefix ( ! ). For example, filtering all results where the user name is 'foo' and the age is less than 20 is expressed as: FT.AGGREGATE ... FILTER @name== foo @age 20 ... Several filter steps can be added, although at the same stage in the pipeline, it is more efficient to combine several predicates into a single filter step.","title":"FILTER expressions"},{"location":"Aggregations/#cursor_api","text":"FT.AGGREGATE ... WITHCURSOR [COUNT {read size} MAXIDLE {idle timeout}] FT.CURSOR READ {idx} {cid} [COUNT {read size}] FT.CURSOR DEL {idx} {cid} You can use cursors with FT.AGGREGATE , with the WITHCURSOR keyword. Cursors allow you to consume only part of the response, allowing you to fetch additional results as needed. This is much quicker than using LIMIT with offset, since the query is executed only once, and its state is stored on the server. To use cursors, specify the WITHCURSOR keyword in FT.AGGREGATE , e.g. FT.AGGREGATE idx * WITHCURSOR This will return a response of an array with two elements. The first element is the actual (partial) results, and the second is the cursor ID. The cursor ID can then be fed to FT.CURSOR READ repeatedly, until the cursor ID is 0, in which case all results have been returned. To read from an existing cursor, use FT.CURSOR READ , e.g. FT.CURSOR READ idx 342459320 Assuming 342459320 is the cursor ID returned from the FT.AGGREGATE request. Here is an example in pseudo-code: response, cursor = FT.AGGREGATE idx redis WITHCURSOR ; while (1) { processResponse(response) if (!cursor) { break; } response, cursor = FT.CURSOR read idx cursor } Note that even if the cursor is 0, a partial result may still be returned.","title":"Cursor API"},{"location":"Aggregations/#cursor_settings","text":"","title":"Cursor settings"},{"location":"Aggregations/#read_size","text":"You can control how many rows are read per each cursor fetch by using the COUNT parameter. This parameter can be specified both in FT.AGGREGATE (immediately after WITHCURSOR ) or in FT.CURSOR READ . FT.AGGREGATE idx query WITHCURSOR COUNT 10 Will read 10 rows at a time. You can override this setting by also specifying COUNT in CURSOR READ , e.g. FT.CURSOR READ idx 342459320 COUNT 50 Will return at most 50 results. The default read size is 1000","title":"Read size"},{"location":"Aggregations/#timeouts_and_limits","text":"Because cursors are stateful resources which occupy memory on the server, they have a limited lifetime. In order to safeguard against orphaned/stale cursors, cursors have an idle timeout value. If no activity occurs on the cursor before the idle timeout, the cursor is deleted. The idle timer resets to 0 whenever the cursor is read from using CURSOR READ . The default idle timeout is 30000 milliseconds (or 30 seconds). You can modify the idle timeout using the MAXIDLE keyword when creating the cursor. Note that the value cannot exceed the default 30s. FT.AGGREGATE idx query WITHCURSOR MAXIDLE 10000 Will set the limit for 10 seconds.","title":"Timeouts and limits"},{"location":"Aggregations/#other_cursor_commands","text":"Cursors can be explicity deleted using the CURSOR DEL command, e.g. FT.CURSOR DEL idx 342459320 Note that cursors are automatically deleted if all their results have been returned, or if they have been timed out. All idle cursors can be forcefully purged at once using FT.CURSOR GC idx 0 command. By default, RediSearch uses a lazy throttled approach to garbage collection, which collects idle cursors every 500 operations, or every second - whichever is later.","title":"Other cursor commands"},{"location":"Chinese/","text":"Chinese support in RediSearch Support for adding documents in Chinese is available starting at version 0.99.0. Chinese support allows Chinese documents to be added and tokenized using segmentation rather than simple tokenization using whitespace and/or punctuation. Indexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese. Chinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match. RediSearch makes use of the Friso chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required. Example: Using Chinese in RediSearch In pseudo-code: FT.CREATE idx SCHEMA txt TEXT FT.ADD idx docCn 1.0 LANGUAGE chinese FIELDS txt Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\uff0c\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] FT.SEARCH idx \u6570\u636e LANGUAGE chinese HIGHLIGHT SUMMARIZE # Outputs: # b \u6570\u636e /b ?... b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8... Using the Python Client: # -*- coding: utf-8 -*- from redisearch.client import Client , Query from redisearch import TextField client = Client ( idx ) try : client . drop_index () except : pass client . create_index ([ TextField ( txt )]) # Add a document client . add_document ( docCn1 , txt = Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] , language = chinese ) print client . search ( Query ( \u6570\u636e ) . summarize () . highlight () . language ( chinese )) . docs [ 0 ] . txt Prints: b \u6570\u636e /b ?... b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8... Using custom dictionaries If you wish to use a custom dictionary, you can do so at the module level when loading the module. The FRISOINI setting can point to the location of a friso.ini file which contains the relevant settings and paths to the dictionary files. Note that there is no \"default\" friso.ini file location. RediSearch comes with its own friso.ini and dictionary files which are compiled into the module binary at build-time.","title":"Chinese Support"},{"location":"Chinese/#chinese_support_in_redisearch","text":"Support for adding documents in Chinese is available starting at version 0.99.0. Chinese support allows Chinese documents to be added and tokenized using segmentation rather than simple tokenization using whitespace and/or punctuation. Indexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese. Chinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match. RediSearch makes use of the Friso chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required.","title":"Chinese support in RediSearch"},{"location":"Chinese/#example_using_chinese_in_redisearch","text":"In pseudo-code: FT.CREATE idx SCHEMA txt TEXT FT.ADD idx docCn 1.0 LANGUAGE chinese FIELDS txt Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\uff0c\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] FT.SEARCH idx \u6570\u636e LANGUAGE chinese HIGHLIGHT SUMMARIZE # Outputs: # b \u6570\u636e /b ?... b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8... Using the Python Client: # -*- coding: utf-8 -*- from redisearch.client import Client , Query from redisearch import TextField client = Client ( idx ) try : client . drop_index () except : pass client . create_index ([ TextField ( txt )]) # Add a document client . add_document ( docCn1 , txt = Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] , language = chinese ) print client . search ( Query ( \u6570\u636e ) . summarize () . highlight () . language ( chinese )) . docs [ 0 ] . txt Prints: b \u6570\u636e /b ?... b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8...","title":"Example: Using Chinese in RediSearch"},{"location":"Chinese/#using_custom_dictionaries","text":"If you wish to use a custom dictionary, you can do so at the module level when loading the module. The FRISOINI setting can point to the location of a friso.ini file which contains the relevant settings and paths to the dictionary files. Note that there is no \"default\" friso.ini file location. RediSearch comes with its own friso.ini and dictionary files which are compiled into the module binary at build-time.","title":"Using custom dictionaries"},{"location":"Clients/","text":"RediSearch Client Libraries RediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages. While it is possible and simple to use the raw Redis commands API, in most cases it's easier to just use a client library abstracting it. Currently available Libraries Language Library Author License Comments Python redisearch-py Redis Labs BSD Usually the most up-to-date client library Java JRediSearch Redis Labs BSD Go redisearch-go Redis Labs BSD Incomplete API JavaScript RedRediSearch Kyle J. Davis MIT Partial API, compatible with Reds C# NRediSearch Marc Gravell MIT Part of StackExchange.Redis PHP redisearch-php Ethan Hann MIT Ruby on Rails redi_search_rails Dmitry Polyakovsky MIT Ruby redisearch-rb Victor Ruiz MIT","title":"Client Libraries"},{"location":"Clients/#redisearch_client_libraries","text":"RediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages. While it is possible and simple to use the raw Redis commands API, in most cases it's easier to just use a client library abstracting it.","title":"RediSearch Client Libraries"},{"location":"Clients/#currently_available_libraries","text":"Language Library Author License Comments Python redisearch-py Redis Labs BSD Usually the most up-to-date client library Java JRediSearch Redis Labs BSD Go redisearch-go Redis Labs BSD Incomplete API JavaScript RedRediSearch Kyle J. Davis MIT Partial API, compatible with Reds C# NRediSearch Marc Gravell MIT Part of StackExchange.Redis PHP redisearch-php Ethan Hann MIT Ruby on Rails redi_search_rails Dmitry Polyakovsky MIT Ruby redisearch-rb Victor Ruiz MIT","title":"Currently available Libraries"},{"location":"Commands/","text":"RediSearch Full Command Documentation FT.CREATE Format FT.CREATE {index} [MAXTEXTFIELDS] [NOOFFSETS] [NOHL] [NOFIELDS] [NOFREQS] [STOPWORDS {num} {stopword} ...] SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] [PHONETIC {matcher}] | NUMERIC | GEO | TAG [SEPARATOR {sep}] ] [SORTABLE][NOINDEX] ... Description Creates an index with the given spec. The index name will be used in all the key names so keep it short! Note on field number limits RediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields. On 32 bit builds, at most 64 fields can be TEXT fields. Note that the more fields you have, the larger your index will be, as each additional 8 fields require one extra byte per index record to encode. You can always use the NOFIELDS option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields. Parameters index : the index name to create. If it exists the old spec will be overwritten MAXTEXTFIELDS : For efficiency, RediSearch encodes indexes differently if they are created with less than 32 text fields. This option forces RediSearch to encode indexes as if there were more than 32 text fields, which allows you to add additional fields (beyond 32) using FT.ALTER . NOOFFSETS : If set, we do not store term offsets for documents (saves memory, does not allow exact searches or highlighting). Implies NOHL . NOHL : Conserves storage space and memory by disabling highlighting support. If set, we do not store corresponding byte offsets for term positions. NOHL is also implied by NOOFFSETS . NOFIELDS : If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields. NOFREQS : If set, we avoid saving the term frequencies in the index. This saves memory but does not allow sorting based on the frequencies of a given term within the document. STOPWORDS : If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}. If not set, we take the default list of stopwords. If {num} is set to 0, the index will not have stopwords. SCHEMA {field} {options...} : After the SCHEMA keyword we define the index fields. They can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0. Field Options SORTABLE Numeric, tag or text field can have the optional SORTABLE argument that allows the user to later sort the results by the value of this field (this adds memory overhead so do not declare it on large text fields). NOSTEM Text fields can have the NOSTEM argument which will disable stemming when indexing its values. This may be ideal for things like proper names. NOINDEX Fields can have the NOINDEX option, which means they will not be indexed. This is useful in conjunction with SORTABLE , to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index. PHONETIC {matcher} Declaring a text field as PHONETIC will perform phonetic matching on it in searches by default. The obligatory {matcher} argument specifies the phonetic algorithm and language used. The following matchers are supported: dm:en - Double Metaphone for English dm:fr - Double Metaphone for French dm:pt - Double Metaphone for Portuguese dm:es - Double Metaphone for Spanish For more details see Phonetic Matching . Complexity O(1) Returns OK or an error FT.ADD Format FT.ADD {index} {docId} {score} [NOSAVE] [REPLACE [PARTIAL]] [LANGUAGE {language}] [PAYLOAD {payload}] [IF {condition}] FIELDS {field} {value} [{field} {value}...] Description Adds a document to the index. Parameters index : The Fulltext index name. The index must be first created with FT.CREATE docId : The document's id that will be returned from searches. Notes on docId The same docId cannot be added twice to the same index. The same docId can be added to multiple indices, but a single document with that docId is saved in the database. score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. If you don't have a score just set it to 1 NOSAVE : If set to true, we will not save the actual document in the database and only index it. REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists. PARTIAL (only applicable with REPLACE): If set, you do not have to specify all fields for reindexing. Fields not given to the command will be loaded from the current version of the document. Also, if only non-indexable fields, score or payload are set - we do not do a full re-indexing of the document, and this will be a lot faster. FIELDS : Following the FIELDS specifier, we are looking for pairs of {field} {value} to be indexed. Each field will be scored based on the index spec given in FT.CREATE . Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set PAYLOAD {payload} : Optionally set a binary safe payload string to the document, that can be evaluated at query time by a custom scoring function, or retrieved to the client. IF {condition} : (Applicable only in conjunction with REPLACE and optionally PARTIAL ). Update the document only if a boolean expression applies to the document before the update , e.g. FT.ADD idx doc 1 REPLACE IF \"@timestamp 23323234234\" . The expression is evaluated atomically before the update, ensuring that the update will happen only if it is true. See Aggregations for more details on the expression language. LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Default to English. If an unsupported language is sent, the command returns an error. The supported languages are: \"arabic\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\", \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\", \"russian\", \"spanish\", \"swedish\", \"tamil\", \"turkish\" \"chinese\" If indexing a Chinese language document, you must set the language to chinese in order for Chinese characters to be tokenized properly. Adding Chinese Documents When adding Chinese-language documents, LANGUAGE chinese should be set in order for the indexer to properly tokenize the terms. If the default language is used then search terms will be extracted based on punctuation characters and whitespace. The Chinese language tokenizer makes use of a segmentation algorithm (via Friso ) which segments texts and checks it against a predefined dictionary. See Stemming for more information. Complexity O(n), where n is the number of tokens in the document Returns OK on success, or an error if something went wrong. A special status NOADD is returned if an IF condition evaluated to false. FT.ADD with REPLACE and PARTIAL By default, FT.ADD does not allow updating the document, and will fail if it already exists in the index. However, updating the document is possible with the REPLACE and REPLACE PARTIAL options. REPLACE : On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document. REPLACE PARTIAL : When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage. FT.ADDHASH Format FT . ADDHASH { index } { docId } { score } [ LANGUAGE language ] [ REPLACE ] Description Adds a document to the index from an existing HASH key in Redis. Parameters index : The Fulltext index name. The index must be first created with FT.CREATE docId : The document's id. This has to be an existing HASH key in Redis that will hold the fields the index needs. score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. If you don't have a score just set it to 1 REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists. LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Defaults to English. If an unsupported language is sent, the command returns an error. The supported languages are: \"arabic\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\", \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\", \"russian\", \"spanish\", \"swedish\", \"tamil\", \"turkish\" Complexity O(n), where n is the number of tokens in the document Returns OK on success, or an error if something went wrong. FT.ALTER Format FT.ALTER {index} SCHEMA ADD {field} {options} ... Description Alters an existing index. Currently, adding fields to the index is the only supported alteration. Adding a field to the index will cause any future document updates to use the new field when indexing. Existing documents will not be reindexed. Note Depending on how the index was created, you may be limited by the amount of additional text fields which can be added to an existing index. If the current index contains less than 32 text fields, then SCHEMA ADD will only be able to add up to 32 fields (meaning that the index will only ever be able to contain 32 total text fields). If you wish for the index to contain more than 32 fields, create it with the MAXTEXTFIELDS option. Parameters index : the index name. field : the field name. options : the field options - refer to FT.CREATE for more information. Complexity O(1) Returns OK or an error. FT.INFO Format FT.INFO {index} Description Returns information and statistics on the index. Returned values include: Number of documents. Number of distinct terms. Average bytes per record. Size and capacity of the index buffers. Example: 127.0.0.1:6379 ft.info wik{0} 1) index_name 2) wikipedia 3) fields 4) 1) 1) title 2) type 3) FULLTEXT 4) weight 5) 1 2) 1) body 2) type 3) FULLTEXT 4) weight 5) 1 5) num_docs 6) 502694 7) num_terms 8) 439158 9) num_records 10) 8098583 11) inverted_sz_mb 12) 45.58 13) inverted_cap_mb 14) 56.61 15) inverted_cap_ovh 16) 0.19 17) offset_vectors_sz_mb 18) 9.27 19) skip_index_size_mb 20) 7.35 21) score_index_size_mb 22) 30.8 23) records_per_doc_avg 24) 16.1 25) bytes_per_record_avg 26) 5.90 27) offsets_per_term_avg 28) 1.20 29) offset_bits_per_record_avg 30) 8.00 Parameters index : The Fulltext index name. The index must be first created with FT.CREATE Complexity O(1) Returns Array Response. A nested array of keys and values. FT.SEARCH Format FT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS] [FILTER {numeric_field} {min} {max}] ... [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft] [INKEYS {num} {key} ... ] [INFIELDS {num} {field} ... ] [RETURN {num} {field} ... ] [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]] [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]] [SLOP {slop}] [INORDER] [LANGUAGE {language}] [EXPANDER {expander}] [SCORER {scorer}] [PAYLOAD {payload}] [SORTBY {field} [ASC|DESC]] [LIMIT offset num] Description Searches the index with a textual query, returning either documents or just ids. Parameters index : The index name. The index must be first created with FT.CREATE . query : the text query to search. If it's more than a single word, put it in quotes. Refer to query syntax for more details. NOCONTENT : If it appears after the query, we only return the document ids and not the content. This is useful if RediSearch is only an index on an external document collection VERBATIM : if set, we do not try to use stemming for query expansion but search the query terms verbatim. NOSTOPWORDS : If set, we do not filter stopwords from the query. WITHSCORES : If set, we also return the relative internal score of each document. this can be used to merge results from multiple instances WITHPAYLOADS : If set, we retrieve optional document payloads (see FT.ADD). the payloads follow the document id, and if WITHSCORES was set, follow the scores. WITHSORTKEYS : Only relevant in conjunction with SORTBY . Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes. FILTER numeric_field min max : If set, and numeric_field is defined as a numeric field in FT.CREATE, we will limit results to those having numeric values ranging between min and max. min and max follow ZRANGE syntax, and can be -inf , +inf and use ( for exclusive ranges. Multiple numeric filters for different fields are supported in one query. GEOFILTER {geo_field} {lon} {lat} {radius} m|km|mi|ft : If set, we filter the results to a given radius from lon and lat. Radius is given as a number and units. See GEORADIUS for more details. INKEYS {num} {field} ... : If set, we limit the result to a given set of keys specified in the list. the first argument must be the length of the list, and greater than zero. Non-existent keys are ignored - unless all the keys are non-existent. INFIELDS {num} {field} ... : If set, filter the results to ones appearing only in specific fields of the document, like title or URL. num is the number of specified field arguments RETURN {num} {field} ... : Use this keyword to limit which fields from the document are returned. num is the number of fields following the keyword. If num is 0, it acts like NOCONTENT . SUMMARIZE ... : Use this option to return only the sections of the field which contain the matched text. See Highlighting for more details HIGHLIGHT ... : Use this option to format occurrences of matched text. See Highligting for more details SLOP {slop} : If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0) INORDER : If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them. LANGUAGE {language} : If set, we use a stemmer for the supplied language during search for query expansion. If querying documents in Chinese, this should be set to chinese in order to properly tokenize the query terms. Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages. EXPANDER {expander} : If set, we will use a custom query expander instead of the stemmer. See Extensions . SCORER {scorer} : If set, we will use a custom scoring function defined by the user. See Extensions . PAYLOAD {payload} : Add an arbitrary, binary safe payload that will be exposed to custom scoring functions. See Extensions . SORTBY {field} [ASC|DESC] : If specified, and field is a sortable field , the results are ordered by the value of this field. This applies to both text and numeric fields. LIMIT first num : If the parameters appear after the query, we limit the results to the offset and number of results given. The default is 0 10 Complexity O(n) for single word queries (though for popular words we save a cache of the top 50 results). Complexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them. Returns Array reply, where the first element is the total number of results, and then pairs of document id, and a nested array of field/value. If NOCONTENT was given, we return an array where the first element is the total number of results, and the rest of the members are document ids. FT.AGGREGATE Format FT.AGGREGATE {index_name} {query_string} [WITHSCHEMA] [VERBATIM] [LOAD {nargs} {property} ...] [GROUPBY {nargs} {property} ... REDUCE {func} {nargs} {arg} ... [AS {name:string}] ... ] ... [SORTBY {nargs} {property} [ASC|DESC] ... [MAX {num}]] [APPLY {expr} AS {alias}] ... [LIMIT {offset} {num}] ... [FILTER {expr}] ... Description Runs a search query on an index, and performs aggregate transformations on the results, extracting statistics etc from them. See the full documentation on aggregations for further details. Parameters index_name : The index the query is executed against. query_string : The base filtering query that retrieves the documents. It follows the exact same syntax as the search query, including filters, unions, not, optional, etc. LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as SORTABLE , where they are available to the aggregation pipeline with very load latency. LOAD hurts the performance of aggregate queries considerably, since every processed record needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to very high processing times. GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them, or performing multiple aggregate operations (see below). REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers. The reducers can have their own property names using the AS {name} optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property @foo , the resulting name will be count_distinct(@foo) . SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but ASC or DESC can be added for each property. nargs is the number of sorting parameters, including ASC and DESC. for example: SORTBY 4 @foo ASC @bar DESC . MAX is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to LIMIT , you usually need just SORTBY \u2026 MAX for common queries. APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation. expr is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example: APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline. LIMIT {offset} {num} . Limit the number of results to return just num results starting at index offset (zero-based). AS mentioned above, it is much more efficient to use SORTBY \u2026 MAX if you are interested in just limiting the output of a sort operation. However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting all the records and then paging over results 50-100. FILTER {expr} . Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline. Complexity Non-deterministic. Depends on the query and aggregations performed, but it is usually linear to the number of results returned. Returns Array Response. Each row is an array and represents a single aggregate result. Example output Here we are counting GitHub events by user (actor), to produce the most active users: 127.0.0.1:6379 FT.AGGREGATE gh * GROUPBY 1 @actor REDUCE COUNT 0 AS num SORTBY 2 @num DESC MAX 10 1) (integer) 284784 2) 1) actor 2) lombiqbot 3) num 4) 22197 3) 1) actor 2) codepipeline-test 3) num 4) 17746 4) 1) actor 2) direwolf-github 3) num 4) 10683 5) 1) actor 2) ogate 3) num 4) 6449 6) 1) actor 2) openlocalizationtest 3) num 4) 4759 7) 1) actor 2) digimatic 3) num 4) 3809 8) 1) actor 2) gugod 3) num 4) 3512 9) 1) actor 2) xdzou 3) num 4) 3216 10) 1) actor 2) opstest 3) num 4) 2863 11) 1) actor 2) jikker 3) num 4) 2794 (0.59s) FT.EXPLAIN Format FT.EXPLAIN {index} {query} Description Returns the execution plan for a complex query. In the returned response, a + on a term is an indication of stemming. Example: $ redis-cli --raw 127 .0.0.1:6379 FT.EXPLAIN rd (foo bar)|(hello world) @date:[100 200]|@date:[500 +inf] INTERSECT { UNION { INTERSECT { foo bar } INTERSECT { hello world } } UNION { NUMERIC { 100 .000000 = x = 200 .000000 } NUMERIC { 500 .000000 = x = inf } } } Parameters index : The index name. The index must be first created with FT.CREATE query : The query string, as if sent to FT.SEARCH Complexity O(1) Returns String Response. A string representing the execution plan (see above example). Note : You should use redis-cli --raw to properly read line-breaks in the returned response. FT.DEL Format FT.DEL {index} {doc_id} [DD] Description Deletes a document from the index. Returns 1 if the document was in the index, or 0 if not. After deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV. FT.DEL does not delete the actual document By default! Since RediSearch regards documents as separate entities to the index and allows things like adding existing documents or indexing without saving the document - by default FT.DEL only deletes the reference to the document from the index, not the actual Redis HASH key where the document is stored. Specifying DD (Delete Document) after the document ID, will make RediSearch also delete the actual document if it is in the index . Alternatively, you can just send an extra DEL {doc_id} to redis and delete the document directly. You can run both of them in a MULTI transaction. Parameters index : The index name. The index must be first created with FT.CREATE doc_id : the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed. Complexity O(1) Returns Integer Reply: 1 if the document was deleted, 0 if not. FT.GET Format FT.GET {index} {doc id} Description Returns the full contents of a document. Currently it is equivalent to HGETALL, but this is future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode. If the document does not exist or is not a HASH object, we return a NULL reply Parameters index : The index name. The index must be first created with FT.CREATE documentId : The id of the document as inserted to the index Returns Array Reply: Key-value pairs of field names and values of the document FT.MGET Format FT.MGET {index} {docId} ... Description Returns the full contents of multiple documents. Currently it is equivalent to calling multiple HGETALL commands, although faster. This command is also future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode. We return an array with exactly the same number of elements as the number of keys sent to the command. Each element, in turn, is an array of key-value pairs representing the document. If a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object. Parameters index : The Fulltext index name. The index must be first created with FT.CREATE documentIds : The ids of the requested documents as inserted to the index Returns Array Reply: An array with exactly the same number of elements as the number of keys sent to the command. Each element in it is either an array representing the document or Null if it was not found. FT.DROP Format FT.DROP {index} [KEEPDOCS] Description Deletes all the keys associated with the index. By default, DROP deletes the document hashes as well, but adding the KEEPDOCS option keeps the documents in place, ready for re-indexing. If no other data is on the Redis instance, this is equivalent to FLUSHDB, apart from the fact that the index specification is not deleted. Parameters index : The Fulltext index name. The index must be first created with FT.CREATE KEEPDOCS : If set, the drop operation will not delete the actual document hashes. Returns Status Reply: OK on success. FT.TAGVALS Format FT.TAGVALS {index} {field_name} Description Returns the distinct tags indexed in a Tag field . This is useful if your tag field indexes things like cities, categories, etc. Limitations There is no paging or sorting, the tags are not alphabetically sorted. This command only operates on Tag fields . The strings return lower-cased and stripped of whitespaces, but otherwise unchanged. Parameters index : The Fulltext index name. The index must be first created with FT.CREATE filed_name : The name of a Tag file defined in the schema. Returns Array Reply: All the distinct tags in the tag index. Complexity O(n), n being the cardinality of the tag field. FT.SUGADD Format FT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}] Description Adds a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the index definitions, and leaves creating and updating suggestions dictionaries to the user. Parameters key : the suggestion dictionary key. string : the suggestion string we index score : a floating point number of the suggestion string's weight INCR : if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time PAYLOAD {payload} : If set, we save an extra payload with the suggestion, that can be fetched by adding the WITHPAYLOADS argument to FT.SUGGET . Returns Integer Reply: the current size of the suggestion dictionary. FT.SUGGET Format FT . SUGGET { key } { prefix } [ FUZZY ] [ WITHSCORES ] [ WITHPAYLOADS ] [ MAX num ] Description Gets completion suggestions for a prefix. Parameters key : the suggestion dictionary key. prefix : the prefix to complete on FUZZY : if set, we do a fuzzy prefix search, including prefixes at Levenshtein distance of 1 from the prefix sent MAX num : If set, we limit the results to a maximum of num (default: 5). WITHSCORES : If set, we also return the score of each suggestion. this can be used to merge results from multiple instances WITHPAYLOADS : If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply. Returns Array Reply: a list of the top suggestions matching the prefix, optionally with score after each entry FT.SUGDEL Format FT.SUGDEL {key} {string} Description Deletes a string from a suggestion index. Parameters key : the suggestion dictionary key. string : the string to delete Returns Integer Reply: 1 if the string was found and deleted, 0 otherwise. FT.SUGLEN Format FT.SUGLEN {key} Description Gets the size of an auto-complete suggestion dictionary Parameters key : the suggestion dictionary key. Returns Integer Reply: the current size of the suggestion dictionary. FT.OPTIMIZE This command is deprecated Index optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command and remove it if they haven't already. Format FT.OPTIMIZE {index} Description This command is deprecated. FT.SYNADD Format FT.SYNADD index name term1 term2 ... Description Adds a synonym group. The command is used to create a new synonyms group. The command returns the synonym group id which can later be used to add additional terms to that synonym group. Only documents which was indexed after the adding operation will be effected. FT.SYNUPDATE Format FT.SYNUPDATE index name synonym group id term1 term2 ... Description Updates a synonym group. The command is used to update an existing synonym group with additional terms. Only documents which was indexed after the update will be effected. FT.SYNDUMP Format FT.SYNDUMP index name Description Dumps the contents of a synonym group. The command is used to dump the synonyms data structure. Returns a list of synonym terms and their synonym group ids. FT.SPELLCHECK Format FT . SPELLCHECK { index } { query } [ DISTANCE dist ] [ TERMS { INCLUDE | EXCLUDE } { dict } [ TERMS ...]] Description Performs spelling correction on a query, returning suggestions for misspelled terms. See Query Spelling Correction for more details. Parameters index : the index with the indexed terms. query : the search query. TERMS : specifies an inclusion ( INCLUDE ) or exclusion ( EXCLUDE ) custom dictionary named {dict} . Refer to FT.DICTADD , FT.DICTDEL and FT.DICTDUMP for managing custom dictionaries. DISTANCE : the maximal Levenshtein distance for spelling suggestions (default: 1, max: 4). Returns An array, in which each element represents a misspelled term from the query. The misspelled terms are ordered by their order of appearance in the query. Each misspelled term, in turn, is a 3-element array consisting of the constant string \"TERM\", the term itself and an array of suggestions for spelling corrections. Each element in the spelling corrections array consists of the score of the suggestion and the suggestion itself. The suggestions array, per misspelled term, is ordered in descending order by score. Example output 1) 1) TERM 2) {term1} 3) 1) 1) {score1} 2) {suggestion1} 2) 1) {score2} 2) {suggestion2} . . . 2) 1) TERM 2) {term2} 3) 1) 1) {score1} 2) {suggestion1} 2) 1) {score2} 2) {suggestion2} . . . . . . FT.DICTADD Format FT.DICTADD {dict} {term} [{term} ...] Description Adds terms to a dictionary. Parameters dict : the dictionary name. term : the term to add to the dictionary. Returns Returns int, specifically the number of new terms that were added. FT.DICTDEL Format FT.DICTDEL {dict} {term} [{term} ...] Description Deletes terms from a dictionary. Parameters dict : the dictionary name. term : the term to delete from the dictionary. Returns Returns int, specifically the number of terms that were deleted. FT.DICTDUMP Format FT.DICTDUMP {dict} Description Dumps all terms in the given dictionary. Parameters dict : the dictionary name. Returns Returns an array, where each element is term (string). FT.CONFIG Format FT.CONFIG GET|HELP {option} FT.CONFIG SET {option} {value} Description Retrieves, describes and sets runtime configuration options. Parameters option : the name of the configuration option, or '*' for all. value : a value for the configuration option. For details about the configuration options refer to Configuring . Setting values in runtime is supported for these configuration options: NOGC MINPREFIX MAXEXPANSIONS TIMEOUT ON_TIMEOUT MIN_PHONETIC_TERM_LEN Returns When provided with a valid option name, the GET subcommand returns a string with the current option's value. An array containing an array for each configuration option, consisting of the option's name and current value, is returned when '*' is provided. The SET subcommand returns 'OK' for valid runtime-settable option names and values.","title":"Command Reference"},{"location":"Commands/#redisearch_full_command_documentation","text":"","title":"RediSearch Full Command Documentation"},{"location":"Commands/#ftcreate","text":"","title":"FT.CREATE"},{"location":"Commands/#format","text":"FT.CREATE {index} [MAXTEXTFIELDS] [NOOFFSETS] [NOHL] [NOFIELDS] [NOFREQS] [STOPWORDS {num} {stopword} ...] SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] [PHONETIC {matcher}] | NUMERIC | GEO | TAG [SEPARATOR {sep}] ] [SORTABLE][NOINDEX] ...","title":"Format"},{"location":"Commands/#description","text":"Creates an index with the given spec. The index name will be used in all the key names so keep it short! Note on field number limits RediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields. On 32 bit builds, at most 64 fields can be TEXT fields. Note that the more fields you have, the larger your index will be, as each additional 8 fields require one extra byte per index record to encode. You can always use the NOFIELDS option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields.","title":"Description"},{"location":"Commands/#parameters","text":"index : the index name to create. If it exists the old spec will be overwritten MAXTEXTFIELDS : For efficiency, RediSearch encodes indexes differently if they are created with less than 32 text fields. This option forces RediSearch to encode indexes as if there were more than 32 text fields, which allows you to add additional fields (beyond 32) using FT.ALTER . NOOFFSETS : If set, we do not store term offsets for documents (saves memory, does not allow exact searches or highlighting). Implies NOHL . NOHL : Conserves storage space and memory by disabling highlighting support. If set, we do not store corresponding byte offsets for term positions. NOHL is also implied by NOOFFSETS . NOFIELDS : If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields. NOFREQS : If set, we avoid saving the term frequencies in the index. This saves memory but does not allow sorting based on the frequencies of a given term within the document. STOPWORDS : If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}. If not set, we take the default list of stopwords. If {num} is set to 0, the index will not have stopwords. SCHEMA {field} {options...} : After the SCHEMA keyword we define the index fields. They can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0.","title":"Parameters"},{"location":"Commands/#field_options","text":"SORTABLE Numeric, tag or text field can have the optional SORTABLE argument that allows the user to later sort the results by the value of this field (this adds memory overhead so do not declare it on large text fields). NOSTEM Text fields can have the NOSTEM argument which will disable stemming when indexing its values. This may be ideal for things like proper names. NOINDEX Fields can have the NOINDEX option, which means they will not be indexed. This is useful in conjunction with SORTABLE , to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index. PHONETIC {matcher} Declaring a text field as PHONETIC will perform phonetic matching on it in searches by default. The obligatory {matcher} argument specifies the phonetic algorithm and language used. The following matchers are supported: dm:en - Double Metaphone for English dm:fr - Double Metaphone for French dm:pt - Double Metaphone for Portuguese dm:es - Double Metaphone for Spanish For more details see Phonetic Matching .","title":"Field Options"},{"location":"Commands/#complexity","text":"O(1)","title":"Complexity"},{"location":"Commands/#returns","text":"OK or an error","title":"Returns"},{"location":"Commands/#ftadd","text":"","title":"FT.ADD"},{"location":"Commands/#format_1","text":"FT.ADD {index} {docId} {score} [NOSAVE] [REPLACE [PARTIAL]] [LANGUAGE {language}] [PAYLOAD {payload}] [IF {condition}] FIELDS {field} {value} [{field} {value}...]","title":"Format"},{"location":"Commands/#description_1","text":"Adds a document to the index.","title":"Description"},{"location":"Commands/#parameters_1","text":"index : The Fulltext index name. The index must be first created with FT.CREATE docId : The document's id that will be returned from searches. Notes on docId The same docId cannot be added twice to the same index. The same docId can be added to multiple indices, but a single document with that docId is saved in the database. score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. If you don't have a score just set it to 1 NOSAVE : If set to true, we will not save the actual document in the database and only index it. REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists. PARTIAL (only applicable with REPLACE): If set, you do not have to specify all fields for reindexing. Fields not given to the command will be loaded from the current version of the document. Also, if only non-indexable fields, score or payload are set - we do not do a full re-indexing of the document, and this will be a lot faster. FIELDS : Following the FIELDS specifier, we are looking for pairs of {field} {value} to be indexed. Each field will be scored based on the index spec given in FT.CREATE . Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set PAYLOAD {payload} : Optionally set a binary safe payload string to the document, that can be evaluated at query time by a custom scoring function, or retrieved to the client. IF {condition} : (Applicable only in conjunction with REPLACE and optionally PARTIAL ). Update the document only if a boolean expression applies to the document before the update , e.g. FT.ADD idx doc 1 REPLACE IF \"@timestamp 23323234234\" . The expression is evaluated atomically before the update, ensuring that the update will happen only if it is true. See Aggregations for more details on the expression language. LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Default to English. If an unsupported language is sent, the command returns an error. The supported languages are: \"arabic\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\", \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\", \"russian\", \"spanish\", \"swedish\", \"tamil\", \"turkish\" \"chinese\" If indexing a Chinese language document, you must set the language to chinese in order for Chinese characters to be tokenized properly.","title":"Parameters"},{"location":"Commands/#adding_chinese_documents","text":"When adding Chinese-language documents, LANGUAGE chinese should be set in order for the indexer to properly tokenize the terms. If the default language is used then search terms will be extracted based on punctuation characters and whitespace. The Chinese language tokenizer makes use of a segmentation algorithm (via Friso ) which segments texts and checks it against a predefined dictionary. See Stemming for more information.","title":"Adding Chinese Documents"},{"location":"Commands/#complexity_1","text":"O(n), where n is the number of tokens in the document","title":"Complexity"},{"location":"Commands/#returns_1","text":"OK on success, or an error if something went wrong. A special status NOADD is returned if an IF condition evaluated to false. FT.ADD with REPLACE and PARTIAL By default, FT.ADD does not allow updating the document, and will fail if it already exists in the index. However, updating the document is possible with the REPLACE and REPLACE PARTIAL options. REPLACE : On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document. REPLACE PARTIAL : When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage.","title":"Returns"},{"location":"Commands/#ftaddhash","text":"","title":"FT.ADDHASH"},{"location":"Commands/#format_2","text":"FT . ADDHASH { index } { docId } { score } [ LANGUAGE language ] [ REPLACE ]","title":"Format"},{"location":"Commands/#description_2","text":"Adds a document to the index from an existing HASH key in Redis.","title":"Description"},{"location":"Commands/#parameters_2","text":"index : The Fulltext index name. The index must be first created with FT.CREATE docId : The document's id. This has to be an existing HASH key in Redis that will hold the fields the index needs. score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. If you don't have a score just set it to 1 REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists. LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Defaults to English. If an unsupported language is sent, the command returns an error. The supported languages are: \"arabic\", \"danish\", \"dutch\", \"english\", \"finnish\", \"french\", \"german\", \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\", \"russian\", \"spanish\", \"swedish\", \"tamil\", \"turkish\"","title":"Parameters"},{"location":"Commands/#complexity_2","text":"O(n), where n is the number of tokens in the document","title":"Complexity"},{"location":"Commands/#returns_2","text":"OK on success, or an error if something went wrong.","title":"Returns"},{"location":"Commands/#ftalter","text":"","title":"FT.ALTER"},{"location":"Commands/#format_3","text":"FT.ALTER {index} SCHEMA ADD {field} {options} ...","title":"Format"},{"location":"Commands/#description_3","text":"Alters an existing index. Currently, adding fields to the index is the only supported alteration. Adding a field to the index will cause any future document updates to use the new field when indexing. Existing documents will not be reindexed. Note Depending on how the index was created, you may be limited by the amount of additional text fields which can be added to an existing index. If the current index contains less than 32 text fields, then SCHEMA ADD will only be able to add up to 32 fields (meaning that the index will only ever be able to contain 32 total text fields). If you wish for the index to contain more than 32 fields, create it with the MAXTEXTFIELDS option.","title":"Description"},{"location":"Commands/#parameters_3","text":"index : the index name. field : the field name. options : the field options - refer to FT.CREATE for more information.","title":"Parameters"},{"location":"Commands/#complexity_3","text":"O(1)","title":"Complexity"},{"location":"Commands/#returns_3","text":"OK or an error.","title":"Returns"},{"location":"Commands/#ftinfo","text":"","title":"FT.INFO"},{"location":"Commands/#format_4","text":"FT.INFO {index}","title":"Format"},{"location":"Commands/#description_4","text":"Returns information and statistics on the index. Returned values include: Number of documents. Number of distinct terms. Average bytes per record. Size and capacity of the index buffers. Example: 127.0.0.1:6379 ft.info wik{0} 1) index_name 2) wikipedia 3) fields 4) 1) 1) title 2) type 3) FULLTEXT 4) weight 5) 1 2) 1) body 2) type 3) FULLTEXT 4) weight 5) 1 5) num_docs 6) 502694 7) num_terms 8) 439158 9) num_records 10) 8098583 11) inverted_sz_mb 12) 45.58 13) inverted_cap_mb 14) 56.61 15) inverted_cap_ovh 16) 0.19 17) offset_vectors_sz_mb 18) 9.27 19) skip_index_size_mb 20) 7.35 21) score_index_size_mb 22) 30.8 23) records_per_doc_avg 24) 16.1 25) bytes_per_record_avg 26) 5.90 27) offsets_per_term_avg 28) 1.20 29) offset_bits_per_record_avg 30) 8.00","title":"Description"},{"location":"Commands/#parameters_4","text":"index : The Fulltext index name. The index must be first created with FT.CREATE","title":"Parameters"},{"location":"Commands/#complexity_4","text":"O(1)","title":"Complexity"},{"location":"Commands/#returns_4","text":"Array Response. A nested array of keys and values.","title":"Returns"},{"location":"Commands/#ftsearch","text":"","title":"FT.SEARCH"},{"location":"Commands/#format_5","text":"FT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS] [FILTER {numeric_field} {min} {max}] ... [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft] [INKEYS {num} {key} ... ] [INFIELDS {num} {field} ... ] [RETURN {num} {field} ... ] [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]] [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]] [SLOP {slop}] [INORDER] [LANGUAGE {language}] [EXPANDER {expander}] [SCORER {scorer}] [PAYLOAD {payload}] [SORTBY {field} [ASC|DESC]] [LIMIT offset num]","title":"Format"},{"location":"Commands/#description_5","text":"Searches the index with a textual query, returning either documents or just ids.","title":"Description"},{"location":"Commands/#parameters_5","text":"index : The index name. The index must be first created with FT.CREATE . query : the text query to search. If it's more than a single word, put it in quotes. Refer to query syntax for more details. NOCONTENT : If it appears after the query, we only return the document ids and not the content. This is useful if RediSearch is only an index on an external document collection VERBATIM : if set, we do not try to use stemming for query expansion but search the query terms verbatim. NOSTOPWORDS : If set, we do not filter stopwords from the query. WITHSCORES : If set, we also return the relative internal score of each document. this can be used to merge results from multiple instances WITHPAYLOADS : If set, we retrieve optional document payloads (see FT.ADD). the payloads follow the document id, and if WITHSCORES was set, follow the scores. WITHSORTKEYS : Only relevant in conjunction with SORTBY . Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes. FILTER numeric_field min max : If set, and numeric_field is defined as a numeric field in FT.CREATE, we will limit results to those having numeric values ranging between min and max. min and max follow ZRANGE syntax, and can be -inf , +inf and use ( for exclusive ranges. Multiple numeric filters for different fields are supported in one query. GEOFILTER {geo_field} {lon} {lat} {radius} m|km|mi|ft : If set, we filter the results to a given radius from lon and lat. Radius is given as a number and units. See GEORADIUS for more details. INKEYS {num} {field} ... : If set, we limit the result to a given set of keys specified in the list. the first argument must be the length of the list, and greater than zero. Non-existent keys are ignored - unless all the keys are non-existent. INFIELDS {num} {field} ... : If set, filter the results to ones appearing only in specific fields of the document, like title or URL. num is the number of specified field arguments RETURN {num} {field} ... : Use this keyword to limit which fields from the document are returned. num is the number of fields following the keyword. If num is 0, it acts like NOCONTENT . SUMMARIZE ... : Use this option to return only the sections of the field which contain the matched text. See Highlighting for more details HIGHLIGHT ... : Use this option to format occurrences of matched text. See Highligting for more details SLOP {slop} : If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0) INORDER : If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them. LANGUAGE {language} : If set, we use a stemmer for the supplied language during search for query expansion. If querying documents in Chinese, this should be set to chinese in order to properly tokenize the query terms. Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages. EXPANDER {expander} : If set, we will use a custom query expander instead of the stemmer. See Extensions . SCORER {scorer} : If set, we will use a custom scoring function defined by the user. See Extensions . PAYLOAD {payload} : Add an arbitrary, binary safe payload that will be exposed to custom scoring functions. See Extensions . SORTBY {field} [ASC|DESC] : If specified, and field is a sortable field , the results are ordered by the value of this field. This applies to both text and numeric fields. LIMIT first num : If the parameters appear after the query, we limit the results to the offset and number of results given. The default is 0 10","title":"Parameters"},{"location":"Commands/#complexity_5","text":"O(n) for single word queries (though for popular words we save a cache of the top 50 results). Complexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.","title":"Complexity"},{"location":"Commands/#returns_5","text":"Array reply, where the first element is the total number of results, and then pairs of document id, and a nested array of field/value. If NOCONTENT was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.","title":"Returns"},{"location":"Commands/#ftaggregate","text":"","title":"FT.AGGREGATE"},{"location":"Commands/#format_6","text":"FT.AGGREGATE {index_name} {query_string} [WITHSCHEMA] [VERBATIM] [LOAD {nargs} {property} ...] [GROUPBY {nargs} {property} ... REDUCE {func} {nargs} {arg} ... [AS {name:string}] ... ] ... [SORTBY {nargs} {property} [ASC|DESC] ... [MAX {num}]] [APPLY {expr} AS {alias}] ... [LIMIT {offset} {num}] ... [FILTER {expr}] ...","title":"Format"},{"location":"Commands/#description_6","text":"Runs a search query on an index, and performs aggregate transformations on the results, extracting statistics etc from them. See the full documentation on aggregations for further details.","title":"Description"},{"location":"Commands/#parameters_6","text":"index_name : The index the query is executed against. query_string : The base filtering query that retrieves the documents. It follows the exact same syntax as the search query, including filters, unions, not, optional, etc. LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as SORTABLE , where they are available to the aggregation pipeline with very load latency. LOAD hurts the performance of aggregate queries considerably, since every processed record needs to execute the equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to very high processing times. GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them, or performing multiple aggregate operations (see below). REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers. The reducers can have their own property names using the AS {name} optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property @foo , the resulting name will be count_distinct(@foo) . SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but ASC or DESC can be added for each property. nargs is the number of sorting parameters, including ASC and DESC. for example: SORTBY 4 @foo ASC @bar DESC . MAX is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to LIMIT , you usually need just SORTBY \u2026 MAX for common queries. APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation. expr is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example: APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline. LIMIT {offset} {num} . Limit the number of results to return just num results starting at index offset (zero-based). AS mentioned above, it is much more efficient to use SORTBY \u2026 MAX if you are interested in just limiting the output of a sort operation. However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting all the records and then paging over results 50-100. FILTER {expr} . Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline.","title":"Parameters"},{"location":"Commands/#complexity_6","text":"Non-deterministic. Depends on the query and aggregations performed, but it is usually linear to the number of results returned.","title":"Complexity"},{"location":"Commands/#returns_6","text":"Array Response. Each row is an array and represents a single aggregate result.","title":"Returns"},{"location":"Commands/#example_output","text":"Here we are counting GitHub events by user (actor), to produce the most active users: 127.0.0.1:6379 FT.AGGREGATE gh * GROUPBY 1 @actor REDUCE COUNT 0 AS num SORTBY 2 @num DESC MAX 10 1) (integer) 284784 2) 1) actor 2) lombiqbot 3) num 4) 22197 3) 1) actor 2) codepipeline-test 3) num 4) 17746 4) 1) actor 2) direwolf-github 3) num 4) 10683 5) 1) actor 2) ogate 3) num 4) 6449 6) 1) actor 2) openlocalizationtest 3) num 4) 4759 7) 1) actor 2) digimatic 3) num 4) 3809 8) 1) actor 2) gugod 3) num 4) 3512 9) 1) actor 2) xdzou 3) num 4) 3216 10) 1) actor 2) opstest 3) num 4) 2863 11) 1) actor 2) jikker 3) num 4) 2794 (0.59s)","title":"Example output"},{"location":"Commands/#ftexplain","text":"","title":"FT.EXPLAIN"},{"location":"Commands/#format_7","text":"FT.EXPLAIN {index} {query}","title":"Format"},{"location":"Commands/#description_7","text":"Returns the execution plan for a complex query. In the returned response, a + on a term is an indication of stemming. Example: $ redis-cli --raw 127 .0.0.1:6379 FT.EXPLAIN rd (foo bar)|(hello world) @date:[100 200]|@date:[500 +inf] INTERSECT { UNION { INTERSECT { foo bar } INTERSECT { hello world } } UNION { NUMERIC { 100 .000000 = x = 200 .000000 } NUMERIC { 500 .000000 = x = inf } } }","title":"Description"},{"location":"Commands/#parameters_7","text":"index : The index name. The index must be first created with FT.CREATE query : The query string, as if sent to FT.SEARCH","title":"Parameters"},{"location":"Commands/#complexity_7","text":"O(1)","title":"Complexity"},{"location":"Commands/#returns_7","text":"String Response. A string representing the execution plan (see above example). Note : You should use redis-cli --raw to properly read line-breaks in the returned response.","title":"Returns"},{"location":"Commands/#ftdel","text":"","title":"FT.DEL"},{"location":"Commands/#format_8","text":"FT.DEL {index} {doc_id} [DD]","title":"Format"},{"location":"Commands/#description_8","text":"Deletes a document from the index. Returns 1 if the document was in the index, or 0 if not. After deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV. FT.DEL does not delete the actual document By default! Since RediSearch regards documents as separate entities to the index and allows things like adding existing documents or indexing without saving the document - by default FT.DEL only deletes the reference to the document from the index, not the actual Redis HASH key where the document is stored. Specifying DD (Delete Document) after the document ID, will make RediSearch also delete the actual document if it is in the index . Alternatively, you can just send an extra DEL {doc_id} to redis and delete the document directly. You can run both of them in a MULTI transaction.","title":"Description"},{"location":"Commands/#parameters_8","text":"index : The index name. The index must be first created with FT.CREATE doc_id : the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed.","title":"Parameters"},{"location":"Commands/#complexity_8","text":"O(1)","title":"Complexity"},{"location":"Commands/#returns_8","text":"Integer Reply: 1 if the document was deleted, 0 if not.","title":"Returns"},{"location":"Commands/#ftget","text":"","title":"FT.GET"},{"location":"Commands/#format_9","text":"FT.GET {index} {doc id}","title":"Format"},{"location":"Commands/#description_9","text":"Returns the full contents of a document. Currently it is equivalent to HGETALL, but this is future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode. If the document does not exist or is not a HASH object, we return a NULL reply","title":"Description"},{"location":"Commands/#parameters_9","text":"index : The index name. The index must be first created with FT.CREATE documentId : The id of the document as inserted to the index","title":"Parameters"},{"location":"Commands/#returns_9","text":"Array Reply: Key-value pairs of field names and values of the document","title":"Returns"},{"location":"Commands/#ftmget","text":"","title":"FT.MGET"},{"location":"Commands/#format_10","text":"FT.MGET {index} {docId} ...","title":"Format"},{"location":"Commands/#description_10","text":"Returns the full contents of multiple documents. Currently it is equivalent to calling multiple HGETALL commands, although faster. This command is also future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode. We return an array with exactly the same number of elements as the number of keys sent to the command. Each element, in turn, is an array of key-value pairs representing the document. If a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object.","title":"Description"},{"location":"Commands/#parameters_10","text":"index : The Fulltext index name. The index must be first created with FT.CREATE documentIds : The ids of the requested documents as inserted to the index","title":"Parameters"},{"location":"Commands/#returns_10","text":"Array Reply: An array with exactly the same number of elements as the number of keys sent to the command. Each element in it is either an array representing the document or Null if it was not found.","title":"Returns"},{"location":"Commands/#ftdrop","text":"","title":"FT.DROP"},{"location":"Commands/#format_11","text":"FT.DROP {index} [KEEPDOCS]","title":"Format"},{"location":"Commands/#description_11","text":"Deletes all the keys associated with the index. By default, DROP deletes the document hashes as well, but adding the KEEPDOCS option keeps the documents in place, ready for re-indexing. If no other data is on the Redis instance, this is equivalent to FLUSHDB, apart from the fact that the index specification is not deleted.","title":"Description"},{"location":"Commands/#parameters_11","text":"index : The Fulltext index name. The index must be first created with FT.CREATE KEEPDOCS : If set, the drop operation will not delete the actual document hashes.","title":"Parameters"},{"location":"Commands/#returns_11","text":"Status Reply: OK on success.","title":"Returns"},{"location":"Commands/#fttagvals","text":"","title":"FT.TAGVALS"},{"location":"Commands/#format_12","text":"FT.TAGVALS {index} {field_name}","title":"Format"},{"location":"Commands/#description_12","text":"Returns the distinct tags indexed in a Tag field . This is useful if your tag field indexes things like cities, categories, etc. Limitations There is no paging or sorting, the tags are not alphabetically sorted. This command only operates on Tag fields . The strings return lower-cased and stripped of whitespaces, but otherwise unchanged.","title":"Description"},{"location":"Commands/#parameters_12","text":"index : The Fulltext index name. The index must be first created with FT.CREATE filed_name : The name of a Tag file defined in the schema.","title":"Parameters"},{"location":"Commands/#returns_12","text":"Array Reply: All the distinct tags in the tag index.","title":"Returns"},{"location":"Commands/#complexity_9","text":"O(n), n being the cardinality of the tag field.","title":"Complexity"},{"location":"Commands/#ftsugadd","text":"","title":"FT.SUGADD"},{"location":"Commands/#format_13","text":"FT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]","title":"Format"},{"location":"Commands/#description_13","text":"Adds a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the index definitions, and leaves creating and updating suggestions dictionaries to the user.","title":"Description"},{"location":"Commands/#parameters_13","text":"key : the suggestion dictionary key. string : the suggestion string we index score : a floating point number of the suggestion string's weight INCR : if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time PAYLOAD {payload} : If set, we save an extra payload with the suggestion, that can be fetched by adding the WITHPAYLOADS argument to FT.SUGGET .","title":"Parameters"},{"location":"Commands/#returns_13","text":"Integer Reply: the current size of the suggestion dictionary.","title":"Returns"},{"location":"Commands/#ftsugget","text":"","title":"FT.SUGGET"},{"location":"Commands/#format_14","text":"FT . SUGGET { key } { prefix } [ FUZZY ] [ WITHSCORES ] [ WITHPAYLOADS ] [ MAX num ]","title":"Format"},{"location":"Commands/#description_14","text":"Gets completion suggestions for a prefix.","title":"Description"},{"location":"Commands/#parameters_14","text":"key : the suggestion dictionary key. prefix : the prefix to complete on FUZZY : if set, we do a fuzzy prefix search, including prefixes at Levenshtein distance of 1 from the prefix sent MAX num : If set, we limit the results to a maximum of num (default: 5). WITHSCORES : If set, we also return the score of each suggestion. this can be used to merge results from multiple instances WITHPAYLOADS : If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply.","title":"Parameters"},{"location":"Commands/#returns_14","text":"Array Reply: a list of the top suggestions matching the prefix, optionally with score after each entry","title":"Returns"},{"location":"Commands/#ftsugdel","text":"","title":"FT.SUGDEL"},{"location":"Commands/#format_15","text":"FT.SUGDEL {key} {string}","title":"Format"},{"location":"Commands/#description_15","text":"Deletes a string from a suggestion index.","title":"Description"},{"location":"Commands/#parameters_15","text":"key : the suggestion dictionary key. string : the string to delete","title":"Parameters"},{"location":"Commands/#returns_15","text":"Integer Reply: 1 if the string was found and deleted, 0 otherwise.","title":"Returns"},{"location":"Commands/#ftsuglen","text":"","title":"FT.SUGLEN"},{"location":"Commands/#format_16","text":"FT.SUGLEN {key}","title":"Format"},{"location":"Commands/#description_16","text":"Gets the size of an auto-complete suggestion dictionary","title":"Description"},{"location":"Commands/#parameters_16","text":"key : the suggestion dictionary key.","title":"Parameters"},{"location":"Commands/#returns_16","text":"Integer Reply: the current size of the suggestion dictionary.","title":"Returns"},{"location":"Commands/#ftoptimize","text":"This command is deprecated Index optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command and remove it if they haven't already.","title":"FT.OPTIMIZE"},{"location":"Commands/#format_17","text":"FT.OPTIMIZE {index}","title":"Format"},{"location":"Commands/#description_17","text":"This command is deprecated.","title":"Description"},{"location":"Commands/#ftsynadd","text":"","title":"FT.SYNADD"},{"location":"Commands/#format_18","text":"FT.SYNADD index name term1 term2 ...","title":"Format"},{"location":"Commands/#description_18","text":"Adds a synonym group. The command is used to create a new synonyms group. The command returns the synonym group id which can later be used to add additional terms to that synonym group. Only documents which was indexed after the adding operation will be effected.","title":"Description"},{"location":"Commands/#ftsynupdate","text":"","title":"FT.SYNUPDATE"},{"location":"Commands/#format_19","text":"FT.SYNUPDATE index name synonym group id term1 term2 ...","title":"Format"},{"location":"Commands/#description_19","text":"Updates a synonym group. The command is used to update an existing synonym group with additional terms. Only documents which was indexed after the update will be effected.","title":"Description"},{"location":"Commands/#ftsyndump","text":"","title":"FT.SYNDUMP"},{"location":"Commands/#format_20","text":"FT.SYNDUMP index name","title":"Format"},{"location":"Commands/#description_20","text":"Dumps the contents of a synonym group. The command is used to dump the synonyms data structure. Returns a list of synonym terms and their synonym group ids.","title":"Description"},{"location":"Commands/#ftspellcheck","text":"","title":"FT.SPELLCHECK"},{"location":"Commands/#format_21","text":"FT . SPELLCHECK { index } { query } [ DISTANCE dist ] [ TERMS { INCLUDE | EXCLUDE } { dict } [ TERMS ...]]","title":"Format"},{"location":"Commands/#description_21","text":"Performs spelling correction on a query, returning suggestions for misspelled terms. See Query Spelling Correction for more details.","title":"Description"},{"location":"Commands/#parameters_17","text":"index : the index with the indexed terms. query : the search query. TERMS : specifies an inclusion ( INCLUDE ) or exclusion ( EXCLUDE ) custom dictionary named {dict} . Refer to FT.DICTADD , FT.DICTDEL and FT.DICTDUMP for managing custom dictionaries. DISTANCE : the maximal Levenshtein distance for spelling suggestions (default: 1, max: 4).","title":"Parameters"},{"location":"Commands/#returns_17","text":"An array, in which each element represents a misspelled term from the query. The misspelled terms are ordered by their order of appearance in the query. Each misspelled term, in turn, is a 3-element array consisting of the constant string \"TERM\", the term itself and an array of suggestions for spelling corrections. Each element in the spelling corrections array consists of the score of the suggestion and the suggestion itself. The suggestions array, per misspelled term, is ordered in descending order by score.","title":"Returns"},{"location":"Commands/#example_output_1","text":"1) 1) TERM 2) {term1} 3) 1) 1) {score1} 2) {suggestion1} 2) 1) {score2} 2) {suggestion2} . . . 2) 1) TERM 2) {term2} 3) 1) 1) {score1} 2) {suggestion1} 2) 1) {score2} 2) {suggestion2} . . . . . .","title":"Example output"},{"location":"Commands/#ftdictadd","text":"","title":"FT.DICTADD"},{"location":"Commands/#format_22","text":"FT.DICTADD {dict} {term} [{term} ...]","title":"Format"},{"location":"Commands/#description_22","text":"Adds terms to a dictionary.","title":"Description"},{"location":"Commands/#parameters_18","text":"dict : the dictionary name. term : the term to add to the dictionary.","title":"Parameters"},{"location":"Commands/#returns_18","text":"Returns int, specifically the number of new terms that were added.","title":"Returns"},{"location":"Commands/#ftdictdel","text":"","title":"FT.DICTDEL"},{"location":"Commands/#format_23","text":"FT.DICTDEL {dict} {term} [{term} ...]","title":"Format"},{"location":"Commands/#description_23","text":"Deletes terms from a dictionary.","title":"Description"},{"location":"Commands/#parameters_19","text":"dict : the dictionary name. term : the term to delete from the dictionary.","title":"Parameters"},{"location":"Commands/#returns_19","text":"Returns int, specifically the number of terms that were deleted.","title":"Returns"},{"location":"Commands/#ftdictdump","text":"","title":"FT.DICTDUMP"},{"location":"Commands/#format_24","text":"FT.DICTDUMP {dict}","title":"Format"},{"location":"Commands/#description_24","text":"Dumps all terms in the given dictionary.","title":"Description"},{"location":"Commands/#parameters_20","text":"dict : the dictionary name.","title":"Parameters"},{"location":"Commands/#returns_20","text":"Returns an array, where each element is term (string).","title":"Returns"},{"location":"Commands/#ftconfig","text":"","title":"FT.CONFIG"},{"location":"Commands/#format_25","text":"FT.CONFIG GET|HELP {option} FT.CONFIG SET {option} {value}","title":"Format"},{"location":"Commands/#description_25","text":"Retrieves, describes and sets runtime configuration options.","title":"Description"},{"location":"Commands/#parameters_21","text":"option : the name of the configuration option, or '*' for all. value : a value for the configuration option. For details about the configuration options refer to Configuring . Setting values in runtime is supported for these configuration options: NOGC MINPREFIX MAXEXPANSIONS TIMEOUT ON_TIMEOUT MIN_PHONETIC_TERM_LEN","title":"Parameters"},{"location":"Commands/#returns_21","text":"When provided with a valid option name, the GET subcommand returns a string with the current option's value. An array containing an array for each configuration option, consisting of the option's name and current value, is returned when '*' is provided. The SET subcommand returns 'OK' for valid runtime-settable option names and values.","title":"Returns"},{"location":"Configuring/","text":"Run-time configuration RediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added. Passing Configuration Options During Loading In general, passing configuration options is done by appending arguments after the --loadmodule argument in the command line, loadmodule configuration directive in a Redis config file, or the MODULE LOAD command. For example: In redis.conf: loadmodule redisearch.so OPT1 OPT2 From redis-cli: 127.0.0.6379 MODULE load redisearch.so OPT1 OPT2 From command line: $ redis-server --loadmodule ./redisearch.so OPT1 OPT2 Setting Configuration Options In Run-Time As of v1.4.1, the FT.CONFIG allows setting some options during runtime. In addition, the command can be used to view the current run-time configuration options. RediSearch configuration options TIMEOUT The maximum amount of time in milliseconds that a search query is allowed to run. If this time is exceeded we return the top results accumulated so far, or an error depending on the policy set with ON_TIMEOUT . The timeout can be disabled by setting it to 0. Note This works only in concurrent mode, so enabling SAFEMODE disables this option. Default 500 Example $ redis-server --loadmodule ./redisearch.so TIMEOUT 100 ON_TIMEOUT {policy} The response policy for queries that exceed the TIMEOUT setting. The policy can be one of the following: RETURN : this policy will return the top results accumulated by the query until it timed out. FAIL : will return an error when the query exeeds the timeout value. Default RETURN Example $ redis-server --loadmodule ./redisearch.so ON_TIMEOUT fail SAFEMODE If present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread. This is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily inconsistent results (i.e. documents that were valid during the invocation of the query are not returned because they were deleted during query processing). Default Off (not present) Example $ redis-server --loadmodule ./redisearch.so SAFEMODE EXTLOAD {file_name} If present, we try to load a RediSearch extension dynamic library from the specified file path. See Extensions for details. Default None Example $ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so NOGC If set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users. Default Not set Example $ redis-server --loadmodule ./redisearch.so NOGC MINPREFIX The minimum number of characters we allow for prefix queries (e.g. hel* ). Setting it to 1 can hurt performance. Default 2 Example $ redis-server --loadmodule ./redisearch.so MINPREFIX 3 MAXEXPANSIONS The maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues. Default 200 Example $ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS 1000 MAXDOCTABLESIZE The maximum size of the internal hash table used for storing the documents. Default 1000000 Example $ redis-server --loadmodule ./redisearch.so MAXDOCTABLESIZE 3000000 FRISOINI {file_name} If present, we load the custom Chinese dictionary from the specified path. See Using custom dictionaries for more details. Default Not set Example $ redis-server --loadmodule ./redisearch.so FRISOINI /opt/dict/friso.ini GC_SCANSIZE The garbage collection bulk size of the internal gc used for cleaning up the indexes. Default 100 Example $ redis-server --loadmodule ./redisearch.so GC_SCANSIZE 10 GC_POLICY The policy for the garbage collector. Supported policies are: DEFAULT : the default policy. FORK : uses a forked thread for garbage collection (v1.4.1 and above). The FORK garbage collection policy is considered an experimental feature, and should be used responsibly. Default \"default\" Example $ redis-server --loadmodule ./redisearch.so GC_POLICY DEFAULT","title":"Configuration"},{"location":"Configuring/#run-time_configuration","text":"RediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added.","title":"Run-time configuration"},{"location":"Configuring/#passing_configuration_options_during_loading","text":"In general, passing configuration options is done by appending arguments after the --loadmodule argument in the command line, loadmodule configuration directive in a Redis config file, or the MODULE LOAD command. For example: In redis.conf: loadmodule redisearch.so OPT1 OPT2 From redis-cli: 127.0.0.6379 MODULE load redisearch.so OPT1 OPT2 From command line: $ redis-server --loadmodule ./redisearch.so OPT1 OPT2","title":"Passing Configuration Options During Loading"},{"location":"Configuring/#setting_configuration_options_in_run-time","text":"As of v1.4.1, the FT.CONFIG allows setting some options during runtime. In addition, the command can be used to view the current run-time configuration options.","title":"Setting Configuration Options In Run-Time"},{"location":"Configuring/#redisearch_configuration_options","text":"","title":"RediSearch configuration options"},{"location":"Configuring/#timeout","text":"The maximum amount of time in milliseconds that a search query is allowed to run. If this time is exceeded we return the top results accumulated so far, or an error depending on the policy set with ON_TIMEOUT . The timeout can be disabled by setting it to 0. Note This works only in concurrent mode, so enabling SAFEMODE disables this option.","title":"TIMEOUT"},{"location":"Configuring/#default","text":"500","title":"Default"},{"location":"Configuring/#example","text":"$ redis-server --loadmodule ./redisearch.so TIMEOUT 100","title":"Example"},{"location":"Configuring/#on_timeout_policy","text":"The response policy for queries that exceed the TIMEOUT setting. The policy can be one of the following: RETURN : this policy will return the top results accumulated by the query until it timed out. FAIL : will return an error when the query exeeds the timeout value.","title":"ON_TIMEOUT {policy}"},{"location":"Configuring/#default_1","text":"RETURN","title":"Default"},{"location":"Configuring/#example_1","text":"$ redis-server --loadmodule ./redisearch.so ON_TIMEOUT fail","title":"Example"},{"location":"Configuring/#safemode","text":"If present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread. This is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily inconsistent results (i.e. documents that were valid during the invocation of the query are not returned because they were deleted during query processing).","title":"SAFEMODE"},{"location":"Configuring/#default_2","text":"Off (not present)","title":"Default"},{"location":"Configuring/#example_2","text":"$ redis-server --loadmodule ./redisearch.so SAFEMODE","title":"Example"},{"location":"Configuring/#extload_file_name","text":"If present, we try to load a RediSearch extension dynamic library from the specified file path. See Extensions for details.","title":"EXTLOAD {file_name}"},{"location":"Configuring/#default_3","text":"None","title":"Default"},{"location":"Configuring/#example_3","text":"$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so","title":"Example"},{"location":"Configuring/#nogc","text":"If set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.","title":"NOGC"},{"location":"Configuring/#default_4","text":"Not set","title":"Default"},{"location":"Configuring/#example_4","text":"$ redis-server --loadmodule ./redisearch.so NOGC","title":"Example"},{"location":"Configuring/#minprefix","text":"The minimum number of characters we allow for prefix queries (e.g. hel* ). Setting it to 1 can hurt performance.","title":"MINPREFIX"},{"location":"Configuring/#default_5","text":"2","title":"Default"},{"location":"Configuring/#example_5","text":"$ redis-server --loadmodule ./redisearch.so MINPREFIX 3","title":"Example"},{"location":"Configuring/#maxexpansions","text":"The maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues.","title":"MAXEXPANSIONS"},{"location":"Configuring/#default_6","text":"200","title":"Default"},{"location":"Configuring/#example_6","text":"$ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS 1000","title":"Example"},{"location":"Configuring/#maxdoctablesize","text":"The maximum size of the internal hash table used for storing the documents.","title":"MAXDOCTABLESIZE"},{"location":"Configuring/#default_7","text":"1000000","title":"Default"},{"location":"Configuring/#example_7","text":"$ redis-server --loadmodule ./redisearch.so MAXDOCTABLESIZE 3000000","title":"Example"},{"location":"Configuring/#frisoini_file_name","text":"If present, we load the custom Chinese dictionary from the specified path. See Using custom dictionaries for more details.","title":"FRISOINI {file_name}"},{"location":"Configuring/#default_8","text":"Not set","title":"Default"},{"location":"Configuring/#example_8","text":"$ redis-server --loadmodule ./redisearch.so FRISOINI /opt/dict/friso.ini","title":"Example"},{"location":"Configuring/#gc_scansize","text":"The garbage collection bulk size of the internal gc used for cleaning up the indexes.","title":"GC_SCANSIZE"},{"location":"Configuring/#default_9","text":"100","title":"Default"},{"location":"Configuring/#example_9","text":"$ redis-server --loadmodule ./redisearch.so GC_SCANSIZE 10","title":"Example"},{"location":"Configuring/#gc_policy","text":"The policy for the garbage collector. Supported policies are: DEFAULT : the default policy. FORK : uses a forked thread for garbage collection (v1.4.1 and above). The FORK garbage collection policy is considered an experimental feature, and should be used responsibly.","title":"GC_POLICY"},{"location":"Configuring/#default_10","text":"\"default\"","title":"Default"},{"location":"Configuring/#example_10","text":"$ redis-server --loadmodule ./redisearch.so GC_POLICY DEFAULT","title":"Example"},{"location":"DESIGN/","text":"RediSearch internal design RediSearch implements inverted indexes on top of Redis, but unlike previous implementations of Redis inverted indexes, it uses custom data encoding, that allows more memory and CPU efficient searches, and more advanced search features. This document details some of the design choices and how these features are implemented. Intro: Redis String DMA The main feature that this module takes advantage of, is Redis Modules Strings DMA, or Direct Memory Access. This feature is simple yet very powerful. It basically allows modules to allocate data on Redis string keys,then get a direct pointer to the data allocated by this key, without copying or serializing it. This allows very fast access to huge amounts of memory, and since from the module's perspective, the string value is exposed simply as char * , it can be cast to any data structure. You simply call RedisModule_StringTruncate to resize a memory chunk to the size needed, and RedisModule_StringDMA to get direct access to the memory in that key. See https://github.com/RedisLabs/RedisModulesSDK/blob/master/FUNCTIONS.md#redismodule_stringdma We use this API in the module mainly to encode inverted indexes, and for other auxiliary data structures besides that. A generic \"Buffer\" implementation using DMA strings can be found in redis_buffer.c . It automatically resizes the Redis string it uses as raw memory when the capacity needs to grow. Inverted index encoding An Inverted Index is the data structure at the heart of all search engines. The idea is simple - per each word or search term, we save a list of all the documents it appears in, and other data, such as term frequency, the offsets where the term appeared in the document, and more. Offsets are used for \"exact match\" type searches, or for ranking of results. When a search is performed, we need to either traverse such an index, or intersect or union two or more indexes. Classic Redis implementations of search engines use sorted sets as inverted indexes. This works but has significant memory overhead, and also does not allow for encoding of offsets, as explained above. RediSearch uses String DMA (see above) to efficiently encode inverted indexes. It combines Delta Encoding and Varint Encoding to encode entries, minimizing space used for indexes, while keeping decompression and traversal efficient. For each \"hit\" (document/word entry), we encode: The document Id as a delta from the previous document. The term frequency, factored by the document's rank (see below) Flags, that can be used to filter only specific fields or other user-defined properties. An Offset Vector, of all the document offsets of the word. Note Document ids as entered by the user are converted to internal incremental document ids, that allow delta encoding to be efficient, and let the inverted indexes be sorted by document id. This allows for a single index hit entry to be encoded in as little as 6 bytes (Note that this is the best case. depending on the number of occurrences of the word in the document, this can get much higher). To optimize searches, we keep two additional auxiliary data structures in different DMA string keys: Skip Index : We keep a table of the index offset of 1/50 of the index entries. This allows faster lookup when intersecting inverted indexes, as not the entire list must be traversed. Score Index : In simple single-word searches, there is no real need to traverse all the results, just the top N results the user is interested in. So we keep an auxiliary index of the top 20 or so entries for each term and use them when applicable. Document and result ranking Each document entered to the engine using FT.ADD , has a user assigned rank, between 0 and 1.0. This is used in combination with TF-IDF scoring of each word, to rank the results. As an optimization, each inverted index hit is encoded with TF*Document_rank as its score, and only IDF is applied during searches. This may change in the future. On top of that, in the case of intersection queries, we take the minimal distance between the terms in the query, and factor that into the ranking. The closest the terms are to each other, the better the result. When searching, we keep a priority queue of the top N results requested, and eventually return them, sorted by rank. Index Specs and field weights When creating an \"index\" using FT.CREATE , the user specifies the fields to be indexed, and their respective weights. This can be used to give some document fields, like a title, more weight in ranking results. For example: FT.CREATE my_index title 10.0 body 1.0 url 2.0 Will create an index on fields named title, body and url, with scores of 10, 1 and 2 respectively. When documents are indexed, the weights are taken from the saved Index Spec , that is stored in a special redis key, and only fields that are specified in this spec are indexed. Document data storage It is not mandatory to save the document data when indexing a document (specifying NOSAVE for FT.ADD will cause the document to be indexed but not saved). If the user does save the document, we simply create a HASH key in Redis, containing all fields (including ones not indexed), and upon search, we simply perform an HGETALL query on each retrieved document, returning its entire data. TODO : Document snippets should be implemented down the road, Query Execution Engine We use a chained-iterator based approach to query execution, similar to Python generators in concept. We simply chain iterators that yield index hits. Those can be: Read Iterators , reading hits one by one from an inverted index. i.e. hello Intersect Iterators , aggregating two or more iterators, yielding only their intersection points. i.e. hello AND world Exact Intersect Iterators - same as above, but yielding results only if the intersection is an exact phrase. i.e. hello NEAR world Union Iterators - combining two or more iterators, and yielding a union of their hits. i.e. hello OR world These are combined based on the query as an execution plan that is evaluated lazily. For example: hello == read( hello ) hello world == intersect( read( hello ), read( world ) ) hello world == exact_intersect( read( hello ), read( world ) ) hello world foo == intersect( exact_intersect( read( hello ), read( world ) ), read( foo ) ) All these iterators are lazy evaluated, entry by entry, with constant memory overhead. The \"root\" iterator is read by the query execution engine, and filtered for the top N results in it. Numeric Filters We support defining a field in the index schema as \"NUMERIC\", meaning you will be able to limit search results only to ones where the given value falls within a specific range. Filtering is done by adding FILTER predicates (more than one is supported) to your query. e.g.: FT.SEARCH products hd tv FILTER price 100 (300 The filter syntax follows the ZRANGEBYSCORE semantics of Redis, meaning -inf and +inf are supported, and prepending ( to a number means an exclusive range. As of release 0.6, the implementation uses a multi-level range tree, saving ranges at multiple resolutions, to allow efficient range scanning. Adding numeric filters can accelerate slow queries if the numeric range is small relative to the entire span of the filtered field. For example, a filter on dates focusing on a few days out of years of data, can speed a heavy query by an order of magnitude. Auto-Complete and Fuzzy Suggestions Another important feature for RediSearch is its auto-complete or suggest commands. It allows you to create dictionaries of weighted terms, and then query them for completion suggestions to a given user prefix. For example, if we put the term \u201clcd tv\u201d into a dictionary, sending the prefix \u201clc\u201d will return it as a result. The dictionary is modelled as a compressed trie (prefix tree) with weights, that is traversed to find the top suffixes of a prefix. RediSearch also allows for Fuzzy Suggestions, meaning you can get suggestions to user prefixes even if the user has a typo in the prefix. This is enabled using a Levenshtein Automaton, allowing efficient searching of the dictionary for all terms within a maximal Levenshtein distance of a term or prefix. Then suggested are weighted based on both their original score and distance from the prefix typed by the user. Currently we support (for performance reasons) only suggestions where the prefix is up to 1 Levenshtein distance away from the typed prefix. However, since searching for fuzzy prefixes, especially very short ones, will traverse an enormous amount of suggestions (in fact, fuzzy suggestions for any single letter will traverse the entire dictionary!), it is recommended to use this feature carefully, and only when considering the performance penalty it incurs. Since Redis is single threaded, blocking it for any amount of time means no other queries can be processed at that time. To support unicode fuzzy matching, we use 16-bit \"runes\" inside the trie and not bytes. This increases memory consumption if the text is purely ASCII, but allows completion with the same level of support to all modern languages. This is done in the following manner: We assume all input to FT.SUG* commands is valid utf-8. We convert the input strings to 32-bit Unicode, optionally normalizing, case-folding and removing accents on the way. If the conversion fails it's because the input is not valid utf-8. We trim the 32-bit runes to 16-bit runes using the lower 16 bits. These can be used for insertion, deletion, and search. We convert the output of searches back to utf-8.","title":"RediSearch internal design"},{"location":"DESIGN/#redisearch_internal_design","text":"RediSearch implements inverted indexes on top of Redis, but unlike previous implementations of Redis inverted indexes, it uses custom data encoding, that allows more memory and CPU efficient searches, and more advanced search features. This document details some of the design choices and how these features are implemented.","title":"RediSearch internal design"},{"location":"DESIGN/#intro_redis_string_dma","text":"The main feature that this module takes advantage of, is Redis Modules Strings DMA, or Direct Memory Access. This feature is simple yet very powerful. It basically allows modules to allocate data on Redis string keys,then get a direct pointer to the data allocated by this key, without copying or serializing it. This allows very fast access to huge amounts of memory, and since from the module's perspective, the string value is exposed simply as char * , it can be cast to any data structure. You simply call RedisModule_StringTruncate to resize a memory chunk to the size needed, and RedisModule_StringDMA to get direct access to the memory in that key. See https://github.com/RedisLabs/RedisModulesSDK/blob/master/FUNCTIONS.md#redismodule_stringdma We use this API in the module mainly to encode inverted indexes, and for other auxiliary data structures besides that. A generic \"Buffer\" implementation using DMA strings can be found in redis_buffer.c . It automatically resizes the Redis string it uses as raw memory when the capacity needs to grow.","title":"Intro: Redis String DMA"},{"location":"DESIGN/#inverted_index_encoding","text":"An Inverted Index is the data structure at the heart of all search engines. The idea is simple - per each word or search term, we save a list of all the documents it appears in, and other data, such as term frequency, the offsets where the term appeared in the document, and more. Offsets are used for \"exact match\" type searches, or for ranking of results. When a search is performed, we need to either traverse such an index, or intersect or union two or more indexes. Classic Redis implementations of search engines use sorted sets as inverted indexes. This works but has significant memory overhead, and also does not allow for encoding of offsets, as explained above. RediSearch uses String DMA (see above) to efficiently encode inverted indexes. It combines Delta Encoding and Varint Encoding to encode entries, minimizing space used for indexes, while keeping decompression and traversal efficient. For each \"hit\" (document/word entry), we encode: The document Id as a delta from the previous document. The term frequency, factored by the document's rank (see below) Flags, that can be used to filter only specific fields or other user-defined properties. An Offset Vector, of all the document offsets of the word. Note Document ids as entered by the user are converted to internal incremental document ids, that allow delta encoding to be efficient, and let the inverted indexes be sorted by document id. This allows for a single index hit entry to be encoded in as little as 6 bytes (Note that this is the best case. depending on the number of occurrences of the word in the document, this can get much higher). To optimize searches, we keep two additional auxiliary data structures in different DMA string keys: Skip Index : We keep a table of the index offset of 1/50 of the index entries. This allows faster lookup when intersecting inverted indexes, as not the entire list must be traversed. Score Index : In simple single-word searches, there is no real need to traverse all the results, just the top N results the user is interested in. So we keep an auxiliary index of the top 20 or so entries for each term and use them when applicable.","title":"Inverted index encoding"},{"location":"DESIGN/#document_and_result_ranking","text":"Each document entered to the engine using FT.ADD , has a user assigned rank, between 0 and 1.0. This is used in combination with TF-IDF scoring of each word, to rank the results. As an optimization, each inverted index hit is encoded with TF*Document_rank as its score, and only IDF is applied during searches. This may change in the future. On top of that, in the case of intersection queries, we take the minimal distance between the terms in the query, and factor that into the ranking. The closest the terms are to each other, the better the result. When searching, we keep a priority queue of the top N results requested, and eventually return them, sorted by rank.","title":"Document and result ranking"},{"location":"DESIGN/#index_specs_and_field_weights","text":"When creating an \"index\" using FT.CREATE , the user specifies the fields to be indexed, and their respective weights. This can be used to give some document fields, like a title, more weight in ranking results. For example: FT.CREATE my_index title 10.0 body 1.0 url 2.0 Will create an index on fields named title, body and url, with scores of 10, 1 and 2 respectively. When documents are indexed, the weights are taken from the saved Index Spec , that is stored in a special redis key, and only fields that are specified in this spec are indexed.","title":"Index Specs and field weights"},{"location":"DESIGN/#document_data_storage","text":"It is not mandatory to save the document data when indexing a document (specifying NOSAVE for FT.ADD will cause the document to be indexed but not saved). If the user does save the document, we simply create a HASH key in Redis, containing all fields (including ones not indexed), and upon search, we simply perform an HGETALL query on each retrieved document, returning its entire data. TODO : Document snippets should be implemented down the road,","title":"Document data storage"},{"location":"DESIGN/#query_execution_engine","text":"We use a chained-iterator based approach to query execution, similar to Python generators in concept. We simply chain iterators that yield index hits. Those can be: Read Iterators , reading hits one by one from an inverted index. i.e. hello Intersect Iterators , aggregating two or more iterators, yielding only their intersection points. i.e. hello AND world Exact Intersect Iterators - same as above, but yielding results only if the intersection is an exact phrase. i.e. hello NEAR world Union Iterators - combining two or more iterators, and yielding a union of their hits. i.e. hello OR world These are combined based on the query as an execution plan that is evaluated lazily. For example: hello == read( hello ) hello world == intersect( read( hello ), read( world ) ) hello world == exact_intersect( read( hello ), read( world ) ) hello world foo == intersect( exact_intersect( read( hello ), read( world ) ), read( foo ) ) All these iterators are lazy evaluated, entry by entry, with constant memory overhead. The \"root\" iterator is read by the query execution engine, and filtered for the top N results in it.","title":"Query Execution Engine"},{"location":"DESIGN/#numeric_filters","text":"We support defining a field in the index schema as \"NUMERIC\", meaning you will be able to limit search results only to ones where the given value falls within a specific range. Filtering is done by adding FILTER predicates (more than one is supported) to your query. e.g.: FT.SEARCH products hd tv FILTER price 100 (300 The filter syntax follows the ZRANGEBYSCORE semantics of Redis, meaning -inf and +inf are supported, and prepending ( to a number means an exclusive range. As of release 0.6, the implementation uses a multi-level range tree, saving ranges at multiple resolutions, to allow efficient range scanning. Adding numeric filters can accelerate slow queries if the numeric range is small relative to the entire span of the filtered field. For example, a filter on dates focusing on a few days out of years of data, can speed a heavy query by an order of magnitude.","title":"Numeric Filters"},{"location":"DESIGN/#auto-complete_and_fuzzy_suggestions","text":"Another important feature for RediSearch is its auto-complete or suggest commands. It allows you to create dictionaries of weighted terms, and then query them for completion suggestions to a given user prefix. For example, if we put the term \u201clcd tv\u201d into a dictionary, sending the prefix \u201clc\u201d will return it as a result. The dictionary is modelled as a compressed trie (prefix tree) with weights, that is traversed to find the top suffixes of a prefix. RediSearch also allows for Fuzzy Suggestions, meaning you can get suggestions to user prefixes even if the user has a typo in the prefix. This is enabled using a Levenshtein Automaton, allowing efficient searching of the dictionary for all terms within a maximal Levenshtein distance of a term or prefix. Then suggested are weighted based on both their original score and distance from the prefix typed by the user. Currently we support (for performance reasons) only suggestions where the prefix is up to 1 Levenshtein distance away from the typed prefix. However, since searching for fuzzy prefixes, especially very short ones, will traverse an enormous amount of suggestions (in fact, fuzzy suggestions for any single letter will traverse the entire dictionary!), it is recommended to use this feature carefully, and only when considering the performance penalty it incurs. Since Redis is single threaded, blocking it for any amount of time means no other queries can be processed at that time. To support unicode fuzzy matching, we use 16-bit \"runes\" inside the trie and not bytes. This increases memory consumption if the text is purely ASCII, but allows completion with the same level of support to all modern languages. This is done in the following manner: We assume all input to FT.SUG* commands is valid utf-8. We convert the input strings to 32-bit Unicode, optionally normalizing, case-folding and removing accents on the way. If the conversion fails it's because the input is not valid utf-8. We trim the 32-bit runes to 16-bit runes using the lower 16 bits. These can be used for insertion, deletion, and search. We convert the output of searches back to utf-8.","title":"Auto-Complete and Fuzzy Suggestions"},{"location":"Escaping/","text":"Controlling Text Tokenization and Escaping At the moment, RediSearch uses a very simple tokenizer for documents and a slightly more sophisticated tokenizer for queries. Both allow a degree of control over string escaping and tokenization. Note: There is a different mechanism for tokenizing text and tag fields, this document refers only to text fields. For tag fields please refer to the Tag Fields documentation. The rules of text field tokenization All punctuation marks and whitespaces (besides underscores) separate the document and queries into tokens. e.g. any character of ,. {}[]\"':;!@#$%^ *()-+=~ will break the text into terms. So the text foo-bar.baz...bag will be tokenized into [foo, bar, baz, bag] Escaping separators in both queries and documents is done by prepending a backslash to any separator. e.g. the text hello\\-world hello-world will be tokenized as [hello-world, hello, world] . NOTE that in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\\\-world . Underscores ( _ ) are not used as separators in either document or query. So the text hello_world will remain as is after tokenization. Repeating spaces or punctuation marks are stripped. In Latin characters, everything gets converted to lowercase.","title":"Tokenization and Escaping"},{"location":"Escaping/#controlling_text_tokenization_and_escaping","text":"At the moment, RediSearch uses a very simple tokenizer for documents and a slightly more sophisticated tokenizer for queries. Both allow a degree of control over string escaping and tokenization. Note: There is a different mechanism for tokenizing text and tag fields, this document refers only to text fields. For tag fields please refer to the Tag Fields documentation.","title":"Controlling Text Tokenization and Escaping"},{"location":"Escaping/#the_rules_of_text_field_tokenization","text":"All punctuation marks and whitespaces (besides underscores) separate the document and queries into tokens. e.g. any character of ,. {}[]\"':;!@#$%^ *()-+=~ will break the text into terms. So the text foo-bar.baz...bag will be tokenized into [foo, bar, baz, bag] Escaping separators in both queries and documents is done by prepending a backslash to any separator. e.g. the text hello\\-world hello-world will be tokenized as [hello-world, hello, world] . NOTE that in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\\\-world . Underscores ( _ ) are not used as separators in either document or query. So the text hello_world will remain as is after tokenization. Repeating spaces or punctuation marks are stripped. In Latin characters, everything gets converted to lowercase.","title":"The rules of text field tokenization"},{"location":"Extensions/","text":"Extending RediSearch RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time. There are two kinds of extension APIs at the moment: Query Expanders , whose role is to expand query tokens (i.e. stemmers). Scoring Functions , whose role is to rank search results in query time. Registering and loading extensions Extensions should be compiled into .so files, and loaded into RediSearch on initialization of the module. Compiling Extensions should be compiled and linked as dynamic libraries. An example Makefile for an extension can be found here . That folder also contains an example extension that is used for testing and can be taken as a skeleton for implementing your own extension. Loading Loading an extension is done by apending EXTLOAD {path/to/ext.so} after the loadmodule configuration directive when loading RediSearch. For example: sh $ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so This causes RediSearch to automatically load the extension and register its expanders and scorers. Initializing an extension The entry point of an extension is a function with the signature: int RS_ExtensionInit ( RSExtensionCtx * ctx ); When loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers. It should return REDISEARCH_ERR on error or REDISEARCH_OK on success. Example init function #include redisearch.h //must be in the include path int RS_ExtensionInit ( RSExtensionCtx * ctx ) { /* Register a scoring function with an alias my_scorer and no special private data and free function */ if ( ctx - RegisterScoringFunction ( my_scorer , MyCustomScorer , NULL , NULL ) == REDISEARCH_ERR ) { return REDISEARCH_ERR ; } /* Register a query expander */ if ( ctx - RegisterQueryExpander ( my_expander , MyExpander , NULL , NULL ) == REDISEARCH_ERR ) { return REDISEARCH_ERR ; } return REDISEARCH_OK ; } Calling your custom functions When performing a query, you can tell RediSearch to use your scorers or expanders by specifying the SCORER or EXPANDER arguments, with the given alias. e.g.: FT.SEARCH my_index foo bar EXPANDER my_expander SCORER my_scorer NOTE : Expander and scorer aliases are case sensitive . The query expander API At the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time. The API for an expander is the following: #include redisearch.h //must be in the include path void MyQueryExpander ( RSQueryExpanderCtx * ctx , RSToken * token ) { ... } RSQueryExpanderCtx RSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as: typedef struct RSQueryExpanderCtx { /* Opaque query object used internally by the engine, and should not be accessed */ struct RSQuery * query ; /* Opaque query node object used internally by the engine, and should not be accessed */ struct RSQueryNode ** currentNode ; /* Private data of the extension, set on extension initialization */ void * privdata ; /* The language of the query, defaults to english */ const char * language ; /* ExpandToken allows the user to add an expansion of the token in the query, that will be * union-merged with the given token in query time. str is the expanded string, len is its length, * and flags is a 32 bit flag mask that can be used by the extension to set private information on * the token */ void ( * ExpandToken )( struct RSQueryExpanderCtx * ctx , const char * str , size_t len , RSTokenFlags flags ); /* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token) */ void ( * SetPayload )( struct RSQueryExpanderCtx * ctx , RSPayload payload ); } RSQueryExpanderCtx ; RSToken RSToken represents a single query token to be expanded and is defined as: /* A token in the query. The expanders receive query tokens and can expand the query with more query * tokens */ typedef struct { /* The token string - which may or may not be NULL terminated */ const char * str ; /* The token length */ size_t len ; /* 1 if the token is the result of query expansion */ uint8_t expanded : 1 ; /* Extension specific token flags that can be examined later by the scoring function */ RSTokenFlags flags ; } RSToken ; The scoring function API A scoring function receives each document being evaluated by the query, for final ranking. It has access to all the query terms that brought up the document,and to metadata about the document such as its a-priory score, length, etc. Since the scoring function is evaluated per each document, potentially millions of times, and since redis is single threaded - it is important that it works as fast as possible and be heavily optimized. A scoring function is applied to each potential result (per document) and is implemented with the following signature: double MyScoringFunction ( RSScoringFunctionCtx * ctx , RSIndexResult * res , RSDocumentMetadata * dmd , double minScore ); RSScoringFunctionCtx is a context that implements some helper methods. RSIndexResult is the result information - containing the document id, frequency, terms, and offsets. RSDocumentMetadata is an object holding global information about the document, such as its a-priory score. minSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start. The return value of the function is double representing the final score of the result. Returning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. To completely filter out a result and not count it in the totals, the scorer should return the special value RS_SCORE_FILTEROUT (which is internally set to negative infinity, or -1/0). RSScoringFunctionCtx This is an object containing the following members: void *privdata : a pointer to an object set by the extension on initialization time. RSPayload payload : A Payload object set either by the query expander or the client. int GetSlop(RSIndexResult *res) : A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other. RSIndexResult This is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result. See redisearch.h for details RSDocumentMetadata This is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function. Example query expander This example query expander expands each token with the term foo: #include redisearch.h //must be in the include path void DummyExpander ( RSQueryExpanderCtx * ctx , RSToken * token ) { ctx - ExpandToken ( ctx , strdup ( foo ), strlen ( foo ), 0x1337 ); } Example scoring function This is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop: #include redisearch.h //must be in the include path double TFIDFScorer ( RSScoringFunctionCtx * ctx , RSIndexResult * h , RSDocumentMetadata * dmd , double minScore ) { // no need to evaluate documents with score 0 if ( dmd - score == 0 ) return 0 ; // calculate sum(tf-idf) for each term in the result double tfidf = 0 ; for ( int i = 0 ; i h - numRecords ; i ++ ) { // take the term frequency and multiply by the term IDF, add that to the total tfidf += ( float ) h - records [ i ]. freq * ( h - records [ i ]. term ? h - records [ i ]. term - idf : 0 ); } // normalize by the maximal frequency of any term in the document tfidf /= ( double ) dmd - maxFreq ; // multiply by the document score (between 0 and 1) tfidf *= dmd - score ; // no need to factor the slop if tfidf is already below minimal score if ( tfidf minScore ) { return 0 ; } // get the slop and divide the result by it, making sure we prefer results with closer terms tfidf /= ( double ) ctx - GetSlop ( h ); return tfidf ; }","title":"Extension API"},{"location":"Extensions/#extending_redisearch","text":"RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time. There are two kinds of extension APIs at the moment: Query Expanders , whose role is to expand query tokens (i.e. stemmers). Scoring Functions , whose role is to rank search results in query time.","title":"Extending RediSearch"},{"location":"Extensions/#registering_and_loading_extensions","text":"Extensions should be compiled into .so files, and loaded into RediSearch on initialization of the module. Compiling Extensions should be compiled and linked as dynamic libraries. An example Makefile for an extension can be found here . That folder also contains an example extension that is used for testing and can be taken as a skeleton for implementing your own extension. Loading Loading an extension is done by apending EXTLOAD {path/to/ext.so} after the loadmodule configuration directive when loading RediSearch. For example: sh $ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so This causes RediSearch to automatically load the extension and register its expanders and scorers.","title":"Registering and loading extensions"},{"location":"Extensions/#initializing_an_extension","text":"The entry point of an extension is a function with the signature: int RS_ExtensionInit ( RSExtensionCtx * ctx ); When loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers. It should return REDISEARCH_ERR on error or REDISEARCH_OK on success.","title":"Initializing an extension"},{"location":"Extensions/#example_init_function","text":"#include redisearch.h //must be in the include path int RS_ExtensionInit ( RSExtensionCtx * ctx ) { /* Register a scoring function with an alias my_scorer and no special private data and free function */ if ( ctx - RegisterScoringFunction ( my_scorer , MyCustomScorer , NULL , NULL ) == REDISEARCH_ERR ) { return REDISEARCH_ERR ; } /* Register a query expander */ if ( ctx - RegisterQueryExpander ( my_expander , MyExpander , NULL , NULL ) == REDISEARCH_ERR ) { return REDISEARCH_ERR ; } return REDISEARCH_OK ; }","title":"Example init function"},{"location":"Extensions/#calling_your_custom_functions","text":"When performing a query, you can tell RediSearch to use your scorers or expanders by specifying the SCORER or EXPANDER arguments, with the given alias. e.g.: FT.SEARCH my_index foo bar EXPANDER my_expander SCORER my_scorer NOTE : Expander and scorer aliases are case sensitive .","title":"Calling your custom functions"},{"location":"Extensions/#the_query_expander_api","text":"At the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time. The API for an expander is the following: #include redisearch.h //must be in the include path void MyQueryExpander ( RSQueryExpanderCtx * ctx , RSToken * token ) { ... }","title":"The query expander API"},{"location":"Extensions/#rsqueryexpanderctx","text":"RSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as: typedef struct RSQueryExpanderCtx { /* Opaque query object used internally by the engine, and should not be accessed */ struct RSQuery * query ; /* Opaque query node object used internally by the engine, and should not be accessed */ struct RSQueryNode ** currentNode ; /* Private data of the extension, set on extension initialization */ void * privdata ; /* The language of the query, defaults to english */ const char * language ; /* ExpandToken allows the user to add an expansion of the token in the query, that will be * union-merged with the given token in query time. str is the expanded string, len is its length, * and flags is a 32 bit flag mask that can be used by the extension to set private information on * the token */ void ( * ExpandToken )( struct RSQueryExpanderCtx * ctx , const char * str , size_t len , RSTokenFlags flags ); /* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token) */ void ( * SetPayload )( struct RSQueryExpanderCtx * ctx , RSPayload payload ); } RSQueryExpanderCtx ;","title":"RSQueryExpanderCtx"},{"location":"Extensions/#rstoken","text":"RSToken represents a single query token to be expanded and is defined as: /* A token in the query. The expanders receive query tokens and can expand the query with more query * tokens */ typedef struct { /* The token string - which may or may not be NULL terminated */ const char * str ; /* The token length */ size_t len ; /* 1 if the token is the result of query expansion */ uint8_t expanded : 1 ; /* Extension specific token flags that can be examined later by the scoring function */ RSTokenFlags flags ; } RSToken ;","title":"RSToken"},{"location":"Extensions/#the_scoring_function_api","text":"A scoring function receives each document being evaluated by the query, for final ranking. It has access to all the query terms that brought up the document,and to metadata about the document such as its a-priory score, length, etc. Since the scoring function is evaluated per each document, potentially millions of times, and since redis is single threaded - it is important that it works as fast as possible and be heavily optimized. A scoring function is applied to each potential result (per document) and is implemented with the following signature: double MyScoringFunction ( RSScoringFunctionCtx * ctx , RSIndexResult * res , RSDocumentMetadata * dmd , double minScore ); RSScoringFunctionCtx is a context that implements some helper methods. RSIndexResult is the result information - containing the document id, frequency, terms, and offsets. RSDocumentMetadata is an object holding global information about the document, such as its a-priory score. minSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start. The return value of the function is double representing the final score of the result. Returning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. To completely filter out a result and not count it in the totals, the scorer should return the special value RS_SCORE_FILTEROUT (which is internally set to negative infinity, or -1/0).","title":"The scoring function API"},{"location":"Extensions/#rsscoringfunctionctx","text":"This is an object containing the following members: void *privdata : a pointer to an object set by the extension on initialization time. RSPayload payload : A Payload object set either by the query expander or the client. int GetSlop(RSIndexResult *res) : A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.","title":"RSScoringFunctionCtx"},{"location":"Extensions/#rsindexresult","text":"This is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result. See redisearch.h for details","title":"RSIndexResult"},{"location":"Extensions/#rsdocumentmetadata","text":"This is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function.","title":"RSDocumentMetadata"},{"location":"Extensions/#example_query_expander","text":"This example query expander expands each token with the term foo: #include redisearch.h //must be in the include path void DummyExpander ( RSQueryExpanderCtx * ctx , RSToken * token ) { ctx - ExpandToken ( ctx , strdup ( foo ), strlen ( foo ), 0x1337 ); }","title":"Example query expander"},{"location":"Extensions/#example_scoring_function","text":"This is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop: #include redisearch.h //must be in the include path double TFIDFScorer ( RSScoringFunctionCtx * ctx , RSIndexResult * h , RSDocumentMetadata * dmd , double minScore ) { // no need to evaluate documents with score 0 if ( dmd - score == 0 ) return 0 ; // calculate sum(tf-idf) for each term in the result double tfidf = 0 ; for ( int i = 0 ; i h - numRecords ; i ++ ) { // take the term frequency and multiply by the term IDF, add that to the total tfidf += ( float ) h - records [ i ]. freq * ( h - records [ i ]. term ? h - records [ i ]. term - idf : 0 ); } // normalize by the maximal frequency of any term in the document tfidf /= ( double ) dmd - maxFreq ; // multiply by the document score (between 0 and 1) tfidf *= dmd - score ; // no need to factor the slop if tfidf is already below minimal score if ( tfidf minScore ) { return 0 ; } // get the slop and divide the result by it, making sure we prefer results with closer terms tfidf /= ( double ) ctx - GetSlop ( h ); return tfidf ; }","title":"Example scoring function"},{"location":"Highlight/","text":"Highlighting API The highlighting API allows you to have only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters. RediSearch implements high performance highlighting and summarization algorithms, with the following API: Command syntax FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}] HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] There are two sub-commands commands used for highlighting. One is HIGHLIGHT which surrounds matching text with an open and/or close tag, and the other is SUMMARIZE which splits a field into contextual fragments surrounding the found terms. It is possible to summarize a field, highlight a field, or perform both actions in the same query. Summarization FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}] Summarization will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context. RediSearch can perform summarization using the SUMMARIZE keyword. If no additional arguments are passed, all returned fields are summarized using built-in defaults. The SUMMARIZE keyword accepts the following arguments: FIELDS : If present, must be the first argument. This should be followed by the number of fields to summarize, which itself is followed by a list of fields. Each field present is summarized. If no FIELDS directive is passed, then all fields returned are summarized. FRAGS : How many fragments should be returned. If not specified, a default of 3 is used. LEN The number of context words each fragment should contain. Context words surround the found term. A higher value will return a larger block of text. If not specified, the default value is 20. SEPARATOR The string used to divide between individual summary snippets. The default is ... which is common among search engines; but you may override this with any other string if you desire to programmatically divide them later on. You may use a newline sequence, as newlines are stripped from the result body anyway (thus, it will not be conflated with an embedded newline in the text) Highlighting FT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] Highlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently. RediSearch can perform highlighting using the HIGHLIGHT keyword. If no additional arguments are passed, all returned fields are highlighted using built-in defaults. The HIGHLIGHT keyword accepts the following arguments: FIELDS If present, must be the first argument. This should be followed by the number of fields to highlight, which itself is followed by a list of fields. Each field present is highlighted. If no FIELDS directive is passed, then all fields returned are highlighted. TAGS If present, must be followed by two strings; the first is prepended to each term match, and the second is appended to it. If no TAGS are specified, a built-in tag value is appended and prepended. Field selection If no specific fields are passed to the RETURN , SUMMARIZE , or HIGHLIGHT keywords, then all of a document's fields are returned. However, if any of these keywords contain a FIELD directive, then the SEARCH command will only return the sum total of all fields enumerated in any of those directives. The RETURN keyword is treated specially, as it overrides any fields specified in SUMMARIZE or HIGHLIGHT . In the command RETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz , the fields foo is returned as-is, while bar and baz are not returned, because RETURN was specified, but did not include those fields. In the command SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz , bar is returned summarized and baz is returned highlighted.","title":"Highlighting Results"},{"location":"Highlight/#highlighting_api","text":"The highlighting API allows you to have only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters. RediSearch implements high performance highlighting and summarization algorithms, with the following API:","title":"Highlighting API"},{"location":"Highlight/#command_syntax","text":"FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}] HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] There are two sub-commands commands used for highlighting. One is HIGHLIGHT which surrounds matching text with an open and/or close tag, and the other is SUMMARIZE which splits a field into contextual fragments surrounding the found terms. It is possible to summarize a field, highlight a field, or perform both actions in the same query.","title":"Command syntax"},{"location":"Highlight/#summarization","text":"FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}] Summarization will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context. RediSearch can perform summarization using the SUMMARIZE keyword. If no additional arguments are passed, all returned fields are summarized using built-in defaults. The SUMMARIZE keyword accepts the following arguments: FIELDS : If present, must be the first argument. This should be followed by the number of fields to summarize, which itself is followed by a list of fields. Each field present is summarized. If no FIELDS directive is passed, then all fields returned are summarized. FRAGS : How many fragments should be returned. If not specified, a default of 3 is used. LEN The number of context words each fragment should contain. Context words surround the found term. A higher value will return a larger block of text. If not specified, the default value is 20. SEPARATOR The string used to divide between individual summary snippets. The default is ... which is common among search engines; but you may override this with any other string if you desire to programmatically divide them later on. You may use a newline sequence, as newlines are stripped from the result body anyway (thus, it will not be conflated with an embedded newline in the text)","title":"Summarization"},{"location":"Highlight/#highlighting","text":"FT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] Highlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently. RediSearch can perform highlighting using the HIGHLIGHT keyword. If no additional arguments are passed, all returned fields are highlighted using built-in defaults. The HIGHLIGHT keyword accepts the following arguments: FIELDS If present, must be the first argument. This should be followed by the number of fields to highlight, which itself is followed by a list of fields. Each field present is highlighted. If no FIELDS directive is passed, then all fields returned are highlighted. TAGS If present, must be followed by two strings; the first is prepended to each term match, and the second is appended to it. If no TAGS are specified, a built-in tag value is appended and prepended.","title":"Highlighting"},{"location":"Highlight/#field_selection","text":"If no specific fields are passed to the RETURN , SUMMARIZE , or HIGHLIGHT keywords, then all of a document's fields are returned. However, if any of these keywords contain a FIELD directive, then the SEARCH command will only return the sum total of all fields enumerated in any of those directives. The RETURN keyword is treated specially, as it overrides any fields specified in SUMMARIZE or HIGHLIGHT . In the command RETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz , the fields foo is returned as-is, while bar and baz are not returned, because RETURN was specified, but did not include those fields. In the command SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz , bar is returned summarized and baz is returned highlighted.","title":"Field selection"},{"location":"Overview/","text":"RediSearch Technical Overview Abstract RediSearch is a powerful text search and secondary indexing engine, built on top of Redis as a Redis Module. Unlike other Redis search libraries, it does not use the internal data structures of Redis like sorted sets. Using its own highly optimized data structures and algorithms, it allows for advanced search features, high performance, and low memory footprint. It can perform simple text searches, as well as complex structured queries, filtering by numeric properties and geographical distances. RediSearch supports continuous indexing with no performance degradation, maintaining concurrent loads of querying and indexing. This makes it ideal for searching frequently updated databases, without the need for batch indexing and service interrupts. RediSearch's Enterprise version supports scaling the search engine across many servers, allowing it to easily grow to billions of documents on hundreds of servers. All of this is done while taking advantage of Redis' robust architecture and infrastructure. Utilizing Redis' protocol, replication, persistence, clustering - RediSearch delivers a powerful yet simple to manage and maintain search and indexing engine, that can be used as a standalone database, or to augment existing Redis databases with advanced powerful indexing capabilities. Main features Full-Text indexing of multiple fields in a document, including: Exact phrase matching. Stemming in many languages. Chinese tokenization support. Prefix queries. Optional, negative and union queries. Distributed search on billions of documents. Numeric property indexing. Geographical indexing and radius filters. Incremental indexing without performance loss. A structured query language for advanced queries: Unions and intersections Optional and negative queries Tag filtering Prefix matching A powerful auto-complete engine with fuzzy matching. Multiple scoring models and sorting by values. Concurrent low-latency insertion and updates of documents. Concurrent searches allowing long-running queries without blocking Redis. An extension mechanism allowing custom scoring models and query extension. Support for indexing existing Hash objects in Redis databases. Indexing documents In order to search effectively, RediSearch needs to know how to index documents. A document may have several fields, each with its own weight (e.g. a title is usually more important than the text itself. The engine can also use numeric or geographical fields for filtering. Hence, the first step is to create the index definition, which tells RediSearch how to treat the documents we will add. For example, to define an index of products, indexing their title, description, brand, and price, the index creation would look like: FT.CREATE my_index SCHEMA title TEXT WEIGHT 5 description TEXT brand TEXT PRICE numeric When we will add a document to this index, for example: FT.ADD my_index doc1 1.0 FIELDS title Acme 42 inch LCD TV description 42 inch brand new Full-HD tv with smart tv capabilities brand Acme price 300 This tells RediSearch to take the document, break each field into its terms (\"tokenization\") and index it, by marking the index for each of the terms in the index as contained in this document. Thus, the product is added immediately to the index and can now be found in future searches Searching Now that we have added products to our index, searching is very simple: FT.SEARCH products full hd tv This will tell RediSearch to intersect the lists of documents for each term and return all documents containing the three terms. Of course, more complex queries can be performed, and the full syntax of the query language is detailed below. Data structures RediSearch uses its own custom data structures and uses Redis' native structures only for storing the actual document content (using Hash objects). Using specialized data structures allows faster searches and more memory effective storage of index records, utilizing compression techniques like delta encoding. These are the data structures RediSearch uses under the hood: Index and document metadata For each search index , there is a root data structure containing the schema, statistics, etc - but most importantly, little compact metadata about each document indexed. Internally, inside the index, RediSearch uses delta encoded lists of numeric, incremental, 32-bit document ids. This means that the user given keys or ids for documents, need to be replaced with the internal ids on indexing, and back to the original ids on search. For that, RediSearch saves two tables, mapping the two kinds of ids in two ways (one table uses a compact trie, the other is simply an array where the internal document ID is the array index). On top of that, for each document, we store its user given a priory score, some status bits, and an optional \"payload\" attached to the document by the user. Accessing the document metadata table is an order of magnitude faster than accessing the hash object where the document is actually saved, so scoring functions that need to access metadata about the document can operate fast enough. Inverted index For each term appearing in at least one document, we keep an inverted index, basically a list of all the documents where this term appears. The list is compressed using delta coding, and the document ids are always incrementing. When the user indexes the documents \"foo\", \"bar\" and \"baz\" for example, they are assigned incrementing ids, For example 1025, 1045, 1080 . When encoding them into the index, we only encode the first ID, followed by the deltas between each entry and the previous one, in this case, 1025, 20, 35 . Using variable-width encoding, we can use one byte to express numbers under 255, two bytes for numbers between 256 and 16383 and so on. This can compress the index by up to 75%. On top of the ids, we save the frequency of each term in each document, a bit mask representing the fields in which the term appeared in the document, and a list of the positions in which the term appeared. The structure of the default search record is as follows. Usually, all the entries are one byte long: +----------------+------------+------------------+-------------+------------------------+ | docId_delta | frequency | field mask | offsets len | offset, offset, .... | | (1-4 bytes) | (1-2 bytes)| (1-16 bytes) | (1-2 bytes)| (1-2 bytes per offset) | +----------------+------------+------------------+-------------+------------------------+ Optionally, we can choose not to save any one of those attributes besides the ID, degrading the features available to the engine. Numeric index Numeric properties are indexed in a special data structure that enables filtering by numeric ranges in an efficient way. One could view a numeric value as a term operating just like an inverted index. For example, all the products with the price $100 are in a specific list, that is intersected with the rest of the query (see Query Execution Engine). However, in order to filer by a range of prices, we would have to intersect the query with all the distinct prices within that range - or perform a union query. If the range has many values in it, this becomes highly inefficient. To avoid that, we group numeric entries with close values together, in a single \"range node\". These nodes are stored in binary range tree, that allows the engine to select the relevant nodes and union them together. Each entry in a range node contains a document Id, and the actual numeric value for that document. To further optimize, the tree uses an adaptive algorithm to try to merge as many nodes as possible within the same range node. Tag index Tag indexes are similar to full-text indexes, but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax. The main differences between tag fields and full-text fields are: An entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one. The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming. Tags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document. The index is much simpler and more compressed: we only store document ids in the index, usually resulting in 1-2 bytes per index entry. Geo index Geo indexes utilize Redis' own geo-indexing capabilities. In query time, the geographical part of the query (a radius filter) is sent to Redis, returning only the ids of documents that are within that radius. Auto-complete The auto-complete engine (see below for a fuller description) utilizes a compact trie or prefix tree, to encode terms and search them by prefix. Query language We support a simple syntax for complex queries, that can be combined together to express complex filtering and matching rules. The query is combined as a text string in the FT.SEARCH request and is parsed using a complex query parser. Multi-word phrases simply a list of tokens, e.g. foo bar baz , and imply intersection (AND) of the terms. Exact phrases are wrapped in quotes, e.g \"hello world\" . OR Unions (i.e word1 OR word2 ), are expressed with a pipe ( | ), e.g. hello|hallo|shalom|hola . NOT negation (i.e. word1 NOT word2 ) of expressions or sub-queries. e.g. hello -world . Prefix matches (all terms starting with a prefix) are expressed with a * following a 2-letter or longer prefix. Selection of specific fields using the syntax @field:hello world . Numeric Range matches on numeric fields with the syntax @field:[{min} {max}] . Geo radius matches on geo fields with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] Tag field filters with the syntax @field:{tag | tag | ...} . See the full documentation on tag fields. Optional terms or clauses: foo ~bar means bar is optional but documents with bar in them will rank higher. Complex queries example Expressions can be combined together to express complex rules. For example, let's assume we have a database of products, where each entity has the fields title , brand , tags and price . Expressing a generic search would be simply: lcd tv This would return documents containing these terms in any field. Limiting the search to specific fields (title only in this case) is expressed as: @title:(lcd tv) Numeric filters can be combined to filter price within a price range: @title:(lcd tv) @price:[100 500.2] Multiple text fields can be accessed in different query clauses, for example, to select products of multiple brands: @title:(lcd tv) @brand:(sony | samsung | lg) @price:[100 500.2] Tag fields can be used to index multi-term properties without actual full-text tokenization: @title:(lcd tv) @brand:(sony | samsung | lg) @tags:{42 inch | smart tv} @price:[100 500.2] And negative clauses can also be added, in this example to filter out plasma and CRT TVs: @title:(lcd tv) @brand:(sony | samsung | lg) @tags:{42 inch | smart tv} @price:[100 500.2] -@tags:{plasma | crt} Scoring model RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use sortable fields (see below). Scoring functions are specified by adding the SCORER {scorer_name} argument to a search request. If you prefer a custom scoring function, it is possible to add more functions using the Extension API . These are the pre-bundled scoring functions available in RediSearch: TFIDF (Default) Basic TF-IDF scoring with document score and proximity boosting factored in. TFIDF.DOCNORM Identical to the default TFIDF scorer, with one important distinction: BM25 A variation on the basic TF-IDF scorer, see this Wikipedia article for more info . DISMAX A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. DOCSCORE A scoring function that just returns the priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further. Sortable fields It is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by the last name. When creating the index with FT.CREATE, you can declare TEXT and NUMERIC properties to be SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the following schema: FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE Would allow the following query: FT.SEARCH users john lennon SORTBY age DESC Result highlighting and summarisation Highlighting allows users to only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters. RediSearch implements high performance highlighting and summarization algorithms, with the following API: FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {separator}] HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] Summarisation will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context. Highlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently. Auto-completion Another important feature for RediSearch is its auto-complete engine. This allows users to create dictionaries of weighted terms, and then query them for completion suggestions to a given user prefix. Completions can have \"payloads\" - a user-provided piece of data that can be used for display. For example, completing the names of users, it is possible to add extra metadata about users to be displayed al For example, if a user starts to put the term \u201clcd tv\u201d into a dictionary, sending the prefix \u201clc\u201d will return the full term as a result. The dictionary is modeled as a compact trie (prefix tree) with weights, which is traversed to find the top suffixes of a prefix. RediSearch also allows for Fuzzy Suggestions, meaning you can get suggestions to prefixes even if the user makes a typo in their prefix. This is enabled using a Levenshtein Automaton, allowing efficient searching of the dictionary for all terms within a maximal Levenshtein Distance of a term or prefix. Then suggestions are weighted based on both their original score and their distance from the prefix typed by the user. However, searching for fuzzy prefixes (especially very short ones) will traverse an enormous number of suggestions. In fact, fuzzy suggestions for any single letter will traverse the entire dictionary, so we recommend using this feature carefully, in consideration of the performance penalty it incurs. RediSearch's auto-completer supports Unicode, allowing for fuzzy matches in non-latin languages as well. Search engine internals The Redis module API RediSearch utilizes the Redis Module API and is loaded into Redis as an extension module. Redis modules make possible to extend Redis functionality, implementing new Redis commands, data structures and capabilities with similar performance to native core Redis itself. Redis modules are dynamic libraries, that can be loaded into Redis at startup or using the MODULE LOAD command. Redis exports a C API, in the form of a single C header file called redismodule.h. This means that while the logic of RediSearch and its algorithms are mostly independent, and it could, in theory, be ported quite easily to run as a stand-alone server - it still \"stands on the shoulders\" of giants and takes advantage of Redis as a robust infrastructure for a database server. Building on top of Redis means that by default the module operates: * A high performance network protocol server. * Robust replication. * Highly durable persistence as snapshots of transaction logs. * Cluster mode. * etc. Query execution engine RediSearch uses a high-performance flexible query processing engine, that can evaluate very complex queries in real time. The above query language is compiled into an execution plan that consists of a tree of \"index iterators\" or \"filters\". These can be any of: Numeric filter Tag filter Text filter Geo filter Intersection operation (combining 2 or more filters) Union operation (combining 2 or more filters) NOT operation (negating the results of an underlying filter) Optional operation (wrapping an underlying filter in an optional matching filter) The query parser generates a tree of these filters. For example, a multi-word search would be resolved into an intersect operation of multiple text filters, each traversing an inverted index of a different term. Simple optimizations such as removing redundant layers in the tree are applied. Each of the filters in the resulting tree evaluates one match at a time. This means that at any given moment, the query processor is busy evaluating and scoring one matching document. This means that very little memory allocation is done at run-time, resulting in higher performance. The resulting matching documents are then fed to a post-processing chain of \"result processors\", responsible for scoring them, extracting the top-N results, loading the documents from storage and sending them to the client. That chain is dynamic as well, and changes based on the attributes of the query. For example, a query that only needs to return document ids, will not include a stage for loading documents from storage. Concurrent updates and searches While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. Think, for example, of a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration. RediSearch utilizes the Redis Module API's concurrency features to avoid stalling the server for long periods of time. The idea is simple - while Redis in itself still remains single-threaded, a module can run many threads - and any one of them can acquire the Global Lock when it needs to access Redis data, operate on it, and release it. We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long-running query will give other queries time to properly run by yielding this lock from time to time. To allow concurrency, we adopted the following design: RediSearch has a thread pool for running concurrent search queries. When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue. The thread pool runs a query processing function in its own thread. The function locks the Redis Global lock and starts executing the query. Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own). If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread. When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\") and continue work from the previous state. Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iterations to finish, but will allow other queries to run concurrently. Index garbage collection RediSearch is optimized for high write, update and delete throughput. One of the main design choices dictated by this goal is that deleting and updating documents do not actually delete anything from the index: Deletion simply marks the document deleted in a global document metadata table, using a single bit. Updating, on the other hand, marks the document as deleted, assigns it a new incremental document ID, and re-indexes the document under a new ID, without performing a comparison of the change. What this means, is that index entries belonging to deleted documents are not removed from the index, and can be seen as \"garbage\". Over time, an index with many deletes and updates will contain mostly garbage - both slowing things down and consuming unnecessary memory. To overcome this, RediSearch employs a background Garbage Collection mechanism: during normal operation of the index, a special thread randomly samples indexes, traverses them and looks for garbage. Index sections containing garbage are \"cleaned\" and memory is reclaimed. This is done in a none intrusive way, operating on very small amounts of data per scan, and utilizing Redis' concurrency mechanism (see above) to avoid interrupting the searches and indexing. The algorithm also tries to adapt to the state of the index, increasing the garbage collector's frequency if the index contains a lot of garbage, and decreasing it if it doesn't, to the point of hardly scanning if the index does not contain garbage. Extension model RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time. There are two kinds of extension APIs at the moment: Query Expanders , whose role is to expand query tokens (i.e. stemmers). Scoring Functions , whose role is to rank search results in query time. Extensions are compiled into dynamic libraries and loaded into RediSearch on initialization of the module. In fact, the mechanism is based on the code of Redis' own module system, albeit far simpler. Scalable Distributed Search While RediSearch is very fast and memory efficient, if an index is big enough, at some point it will be too slow or consume too much memory. Then, it will have to be scaled out and partitioned over several machines - meaning every machine will hold a small part of the complete search index. Traditional clusters map different keys to different \u201cshards\u201d to achieve this. However, in search indexes, this approach is not practical. If we mapped each word\u2019s index to a different shard, we would end up needing to intersect records from different servers for multi-term queries. The way to address this challenge is to employ a technique called Index Partitioning, which is very simple at its core: The index is split across many machines/partitions by document ID. Every such partition has a complete index of all the documents mapped to it. We query all shards concurrently and merge the results from all of them into a single result. To enable that, a new component is added to the cluster, called a Coordinator. When searching for documents, the coordinator receves the query, and sends it to N partitions, each holding a sub-index of 1/N documents. Since we\u2019re only interested in the top K results of all partitions, each partition returns just its own top K results. We then merge the N lists of K elements and extract the top K elements from the merged list. This feature is currently available only with the Enterprise version of RediSearch.","title":"RediSearch Technical Overview"},{"location":"Overview/#redisearch_technical_overview","text":"","title":"RediSearch Technical Overview"},{"location":"Overview/#abstract","text":"RediSearch is a powerful text search and secondary indexing engine, built on top of Redis as a Redis Module. Unlike other Redis search libraries, it does not use the internal data structures of Redis like sorted sets. Using its own highly optimized data structures and algorithms, it allows for advanced search features, high performance, and low memory footprint. It can perform simple text searches, as well as complex structured queries, filtering by numeric properties and geographical distances. RediSearch supports continuous indexing with no performance degradation, maintaining concurrent loads of querying and indexing. This makes it ideal for searching frequently updated databases, without the need for batch indexing and service interrupts. RediSearch's Enterprise version supports scaling the search engine across many servers, allowing it to easily grow to billions of documents on hundreds of servers. All of this is done while taking advantage of Redis' robust architecture and infrastructure. Utilizing Redis' protocol, replication, persistence, clustering - RediSearch delivers a powerful yet simple to manage and maintain search and indexing engine, that can be used as a standalone database, or to augment existing Redis databases with advanced powerful indexing capabilities.","title":"Abstract"},{"location":"Overview/#main_features","text":"Full-Text indexing of multiple fields in a document, including: Exact phrase matching. Stemming in many languages. Chinese tokenization support. Prefix queries. Optional, negative and union queries. Distributed search on billions of documents. Numeric property indexing. Geographical indexing and radius filters. Incremental indexing without performance loss. A structured query language for advanced queries: Unions and intersections Optional and negative queries Tag filtering Prefix matching A powerful auto-complete engine with fuzzy matching. Multiple scoring models and sorting by values. Concurrent low-latency insertion and updates of documents. Concurrent searches allowing long-running queries without blocking Redis. An extension mechanism allowing custom scoring models and query extension. Support for indexing existing Hash objects in Redis databases.","title":"Main features"},{"location":"Overview/#indexing_documents","text":"In order to search effectively, RediSearch needs to know how to index documents. A document may have several fields, each with its own weight (e.g. a title is usually more important than the text itself. The engine can also use numeric or geographical fields for filtering. Hence, the first step is to create the index definition, which tells RediSearch how to treat the documents we will add. For example, to define an index of products, indexing their title, description, brand, and price, the index creation would look like: FT.CREATE my_index SCHEMA title TEXT WEIGHT 5 description TEXT brand TEXT PRICE numeric When we will add a document to this index, for example: FT.ADD my_index doc1 1.0 FIELDS title Acme 42 inch LCD TV description 42 inch brand new Full-HD tv with smart tv capabilities brand Acme price 300 This tells RediSearch to take the document, break each field into its terms (\"tokenization\") and index it, by marking the index for each of the terms in the index as contained in this document. Thus, the product is added immediately to the index and can now be found in future searches","title":"Indexing documents"},{"location":"Overview/#searching","text":"Now that we have added products to our index, searching is very simple: FT.SEARCH products full hd tv This will tell RediSearch to intersect the lists of documents for each term and return all documents containing the three terms. Of course, more complex queries can be performed, and the full syntax of the query language is detailed below.","title":"Searching"},{"location":"Overview/#data_structures","text":"RediSearch uses its own custom data structures and uses Redis' native structures only for storing the actual document content (using Hash objects). Using specialized data structures allows faster searches and more memory effective storage of index records, utilizing compression techniques like delta encoding. These are the data structures RediSearch uses under the hood:","title":"Data structures"},{"location":"Overview/#index_and_document_metadata","text":"For each search index , there is a root data structure containing the schema, statistics, etc - but most importantly, little compact metadata about each document indexed. Internally, inside the index, RediSearch uses delta encoded lists of numeric, incremental, 32-bit document ids. This means that the user given keys or ids for documents, need to be replaced with the internal ids on indexing, and back to the original ids on search. For that, RediSearch saves two tables, mapping the two kinds of ids in two ways (one table uses a compact trie, the other is simply an array where the internal document ID is the array index). On top of that, for each document, we store its user given a priory score, some status bits, and an optional \"payload\" attached to the document by the user. Accessing the document metadata table is an order of magnitude faster than accessing the hash object where the document is actually saved, so scoring functions that need to access metadata about the document can operate fast enough.","title":"Index and document metadata"},{"location":"Overview/#inverted_index","text":"For each term appearing in at least one document, we keep an inverted index, basically a list of all the documents where this term appears. The list is compressed using delta coding, and the document ids are always incrementing. When the user indexes the documents \"foo\", \"bar\" and \"baz\" for example, they are assigned incrementing ids, For example 1025, 1045, 1080 . When encoding them into the index, we only encode the first ID, followed by the deltas between each entry and the previous one, in this case, 1025, 20, 35 . Using variable-width encoding, we can use one byte to express numbers under 255, two bytes for numbers between 256 and 16383 and so on. This can compress the index by up to 75%. On top of the ids, we save the frequency of each term in each document, a bit mask representing the fields in which the term appeared in the document, and a list of the positions in which the term appeared. The structure of the default search record is as follows. Usually, all the entries are one byte long: +----------------+------------+------------------+-------------+------------------------+ | docId_delta | frequency | field mask | offsets len | offset, offset, .... | | (1-4 bytes) | (1-2 bytes)| (1-16 bytes) | (1-2 bytes)| (1-2 bytes per offset) | +----------------+------------+------------------+-------------+------------------------+ Optionally, we can choose not to save any one of those attributes besides the ID, degrading the features available to the engine.","title":"Inverted index"},{"location":"Overview/#numeric_index","text":"Numeric properties are indexed in a special data structure that enables filtering by numeric ranges in an efficient way. One could view a numeric value as a term operating just like an inverted index. For example, all the products with the price $100 are in a specific list, that is intersected with the rest of the query (see Query Execution Engine). However, in order to filer by a range of prices, we would have to intersect the query with all the distinct prices within that range - or perform a union query. If the range has many values in it, this becomes highly inefficient. To avoid that, we group numeric entries with close values together, in a single \"range node\". These nodes are stored in binary range tree, that allows the engine to select the relevant nodes and union them together. Each entry in a range node contains a document Id, and the actual numeric value for that document. To further optimize, the tree uses an adaptive algorithm to try to merge as many nodes as possible within the same range node.","title":"Numeric index"},{"location":"Overview/#tag_index","text":"Tag indexes are similar to full-text indexes, but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax. The main differences between tag fields and full-text fields are: An entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one. The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming. Tags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document. The index is much simpler and more compressed: we only store document ids in the index, usually resulting in 1-2 bytes per index entry.","title":"Tag index"},{"location":"Overview/#geo_index","text":"Geo indexes utilize Redis' own geo-indexing capabilities. In query time, the geographical part of the query (a radius filter) is sent to Redis, returning only the ids of documents that are within that radius.","title":"Geo index"},{"location":"Overview/#auto-complete","text":"The auto-complete engine (see below for a fuller description) utilizes a compact trie or prefix tree, to encode terms and search them by prefix.","title":"Auto-complete"},{"location":"Overview/#query_language","text":"We support a simple syntax for complex queries, that can be combined together to express complex filtering and matching rules. The query is combined as a text string in the FT.SEARCH request and is parsed using a complex query parser. Multi-word phrases simply a list of tokens, e.g. foo bar baz , and imply intersection (AND) of the terms. Exact phrases are wrapped in quotes, e.g \"hello world\" . OR Unions (i.e word1 OR word2 ), are expressed with a pipe ( | ), e.g. hello|hallo|shalom|hola . NOT negation (i.e. word1 NOT word2 ) of expressions or sub-queries. e.g. hello -world . Prefix matches (all terms starting with a prefix) are expressed with a * following a 2-letter or longer prefix. Selection of specific fields using the syntax @field:hello world . Numeric Range matches on numeric fields with the syntax @field:[{min} {max}] . Geo radius matches on geo fields with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] Tag field filters with the syntax @field:{tag | tag | ...} . See the full documentation on tag fields. Optional terms or clauses: foo ~bar means bar is optional but documents with bar in them will rank higher.","title":"Query language"},{"location":"Overview/#complex_queries_example","text":"Expressions can be combined together to express complex rules. For example, let's assume we have a database of products, where each entity has the fields title , brand , tags and price . Expressing a generic search would be simply: lcd tv This would return documents containing these terms in any field. Limiting the search to specific fields (title only in this case) is expressed as: @title:(lcd tv) Numeric filters can be combined to filter price within a price range: @title:(lcd tv) @price:[100 500.2] Multiple text fields can be accessed in different query clauses, for example, to select products of multiple brands: @title:(lcd tv) @brand:(sony | samsung | lg) @price:[100 500.2] Tag fields can be used to index multi-term properties without actual full-text tokenization: @title:(lcd tv) @brand:(sony | samsung | lg) @tags:{42 inch | smart tv} @price:[100 500.2] And negative clauses can also be added, in this example to filter out plasma and CRT TVs: @title:(lcd tv) @brand:(sony | samsung | lg) @tags:{42 inch | smart tv} @price:[100 500.2] -@tags:{plasma | crt}","title":"Complex queries example"},{"location":"Overview/#scoring_model","text":"RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use sortable fields (see below). Scoring functions are specified by adding the SCORER {scorer_name} argument to a search request. If you prefer a custom scoring function, it is possible to add more functions using the Extension API . These are the pre-bundled scoring functions available in RediSearch: TFIDF (Default) Basic TF-IDF scoring with document score and proximity boosting factored in. TFIDF.DOCNORM Identical to the default TFIDF scorer, with one important distinction: BM25 A variation on the basic TF-IDF scorer, see this Wikipedia article for more info . DISMAX A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. DOCSCORE A scoring function that just returns the priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further.","title":"Scoring model"},{"location":"Overview/#sortable_fields","text":"It is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by the last name. When creating the index with FT.CREATE, you can declare TEXT and NUMERIC properties to be SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the following schema: FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE Would allow the following query: FT.SEARCH users john lennon SORTBY age DESC","title":"Sortable fields"},{"location":"Overview/#result_highlighting_and_summarisation","text":"Highlighting allows users to only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters. RediSearch implements high performance highlighting and summarization algorithms, with the following API: FT.SEARCH ... SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {separator}] HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}] Summarisation will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context. Highlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently.","title":"Result highlighting and summarisation"},{"location":"Overview/#auto-completion","text":"Another important feature for RediSearch is its auto-complete engine. This allows users to create dictionaries of weighted terms, and then query them for completion suggestions to a given user prefix. Completions can have \"payloads\" - a user-provided piece of data that can be used for display. For example, completing the names of users, it is possible to add extra metadata about users to be displayed al For example, if a user starts to put the term \u201clcd tv\u201d into a dictionary, sending the prefix \u201clc\u201d will return the full term as a result. The dictionary is modeled as a compact trie (prefix tree) with weights, which is traversed to find the top suffixes of a prefix. RediSearch also allows for Fuzzy Suggestions, meaning you can get suggestions to prefixes even if the user makes a typo in their prefix. This is enabled using a Levenshtein Automaton, allowing efficient searching of the dictionary for all terms within a maximal Levenshtein Distance of a term or prefix. Then suggestions are weighted based on both their original score and their distance from the prefix typed by the user. However, searching for fuzzy prefixes (especially very short ones) will traverse an enormous number of suggestions. In fact, fuzzy suggestions for any single letter will traverse the entire dictionary, so we recommend using this feature carefully, in consideration of the performance penalty it incurs. RediSearch's auto-completer supports Unicode, allowing for fuzzy matches in non-latin languages as well.","title":"Auto-completion"},{"location":"Overview/#search_engine_internals","text":"","title":"Search engine internals"},{"location":"Overview/#the_redis_module_api","text":"RediSearch utilizes the Redis Module API and is loaded into Redis as an extension module. Redis modules make possible to extend Redis functionality, implementing new Redis commands, data structures and capabilities with similar performance to native core Redis itself. Redis modules are dynamic libraries, that can be loaded into Redis at startup or using the MODULE LOAD command. Redis exports a C API, in the form of a single C header file called redismodule.h. This means that while the logic of RediSearch and its algorithms are mostly independent, and it could, in theory, be ported quite easily to run as a stand-alone server - it still \"stands on the shoulders\" of giants and takes advantage of Redis as a robust infrastructure for a database server. Building on top of Redis means that by default the module operates: * A high performance network protocol server. * Robust replication. * Highly durable persistence as snapshots of transaction logs. * Cluster mode. * etc.","title":"The Redis module API"},{"location":"Overview/#query_execution_engine","text":"RediSearch uses a high-performance flexible query processing engine, that can evaluate very complex queries in real time. The above query language is compiled into an execution plan that consists of a tree of \"index iterators\" or \"filters\". These can be any of: Numeric filter Tag filter Text filter Geo filter Intersection operation (combining 2 or more filters) Union operation (combining 2 or more filters) NOT operation (negating the results of an underlying filter) Optional operation (wrapping an underlying filter in an optional matching filter) The query parser generates a tree of these filters. For example, a multi-word search would be resolved into an intersect operation of multiple text filters, each traversing an inverted index of a different term. Simple optimizations such as removing redundant layers in the tree are applied. Each of the filters in the resulting tree evaluates one match at a time. This means that at any given moment, the query processor is busy evaluating and scoring one matching document. This means that very little memory allocation is done at run-time, resulting in higher performance. The resulting matching documents are then fed to a post-processing chain of \"result processors\", responsible for scoring them, extracting the top-N results, loading the documents from storage and sending them to the client. That chain is dynamic as well, and changes based on the attributes of the query. For example, a query that only needs to return document ids, will not include a stage for loading documents from storage.","title":"Query execution engine"},{"location":"Overview/#concurrent_updates_and_searches","text":"While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. Think, for example, of a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration. RediSearch utilizes the Redis Module API's concurrency features to avoid stalling the server for long periods of time. The idea is simple - while Redis in itself still remains single-threaded, a module can run many threads - and any one of them can acquire the Global Lock when it needs to access Redis data, operate on it, and release it. We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long-running query will give other queries time to properly run by yielding this lock from time to time. To allow concurrency, we adopted the following design: RediSearch has a thread pool for running concurrent search queries. When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue. The thread pool runs a query processing function in its own thread. The function locks the Redis Global lock and starts executing the query. Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own). If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread. When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\") and continue work from the previous state. Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iterations to finish, but will allow other queries to run concurrently.","title":"Concurrent updates and searches"},{"location":"Overview/#index_garbage_collection","text":"RediSearch is optimized for high write, update and delete throughput. One of the main design choices dictated by this goal is that deleting and updating documents do not actually delete anything from the index: Deletion simply marks the document deleted in a global document metadata table, using a single bit. Updating, on the other hand, marks the document as deleted, assigns it a new incremental document ID, and re-indexes the document under a new ID, without performing a comparison of the change. What this means, is that index entries belonging to deleted documents are not removed from the index, and can be seen as \"garbage\". Over time, an index with many deletes and updates will contain mostly garbage - both slowing things down and consuming unnecessary memory. To overcome this, RediSearch employs a background Garbage Collection mechanism: during normal operation of the index, a special thread randomly samples indexes, traverses them and looks for garbage. Index sections containing garbage are \"cleaned\" and memory is reclaimed. This is done in a none intrusive way, operating on very small amounts of data per scan, and utilizing Redis' concurrency mechanism (see above) to avoid interrupting the searches and indexing. The algorithm also tries to adapt to the state of the index, increasing the garbage collector's frequency if the index contains a lot of garbage, and decreasing it if it doesn't, to the point of hardly scanning if the index does not contain garbage.","title":"Index garbage collection"},{"location":"Overview/#extension_model","text":"RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time. There are two kinds of extension APIs at the moment: Query Expanders , whose role is to expand query tokens (i.e. stemmers). Scoring Functions , whose role is to rank search results in query time. Extensions are compiled into dynamic libraries and loaded into RediSearch on initialization of the module. In fact, the mechanism is based on the code of Redis' own module system, albeit far simpler.","title":"Extension model"},{"location":"Overview/#scalable_distributed_search","text":"While RediSearch is very fast and memory efficient, if an index is big enough, at some point it will be too slow or consume too much memory. Then, it will have to be scaled out and partitioned over several machines - meaning every machine will hold a small part of the complete search index. Traditional clusters map different keys to different \u201cshards\u201d to achieve this. However, in search indexes, this approach is not practical. If we mapped each word\u2019s index to a different shard, we would end up needing to intersect records from different servers for multi-term queries. The way to address this challenge is to employ a technique called Index Partitioning, which is very simple at its core: The index is split across many machines/partitions by document ID. Every such partition has a complete index of all the documents mapped to it. We query all shards concurrently and merge the results from all of them into a single result. To enable that, a new component is added to the cluster, called a Coordinator. When searching for documents, the coordinator receves the query, and sends it to N partitions, each holding a sub-index of 1/N documents. Since we\u2019re only interested in the top K results of all partitions, each partition returns just its own top K results. We then merge the N lists of K elements and extract the top K elements from the merged list. This feature is currently available only with the Enterprise version of RediSearch.","title":"Scalable Distributed Search"},{"location":"Phonetic_Matching/","text":"Phonetic Matching Phonetic matching, a.k.a \"Jon or John\", allows searching for terms based on their pronounciation. This capability can be a useful tool when searching for names of people. Phonetic matching is based on the use of a phonetic algorithm. A phonetic algorithm transforms the input term to an approximate representation of its pronounciation. This allows indexing terms, and consequently searching, by their pronounciation. As of v1.4 RediSearch provides phonetic matching via the definition of text fields with the PHONETIC attribute. This causes the terms in such fields to be indexed both by their textual value as well as their phonetic approximation. Performing a search on PHONETIC fields will, by default, also return results for phonetically similar terms. This behavior can be controlled with the $phonetic query attribute . Phonetic algorithms support RediSearch currently supports a single phonetic algorithm, the Double Metaphone (DM). It uses the implementation at slacy/double-metaphone , which provides general support for Latin languages.","title":"Phonetic Matching"},{"location":"Phonetic_Matching/#phonetic_matching","text":"Phonetic matching, a.k.a \"Jon or John\", allows searching for terms based on their pronounciation. This capability can be a useful tool when searching for names of people. Phonetic matching is based on the use of a phonetic algorithm. A phonetic algorithm transforms the input term to an approximate representation of its pronounciation. This allows indexing terms, and consequently searching, by their pronounciation. As of v1.4 RediSearch provides phonetic matching via the definition of text fields with the PHONETIC attribute. This causes the terms in such fields to be indexed both by their textual value as well as their phonetic approximation. Performing a search on PHONETIC fields will, by default, also return results for phonetically similar terms. This behavior can be controlled with the $phonetic query attribute .","title":"Phonetic Matching"},{"location":"Phonetic_Matching/#phonetic_algorithms_support","text":"RediSearch currently supports a single phonetic algorithm, the Double Metaphone (DM). It uses the implementation at slacy/double-metaphone , which provides general support for Latin languages.","title":"Phonetic algorithms support"},{"location":"Query_Syntax/","text":"Search Query Syntax We support a simple syntax for complex queries with the following rules: Multi-word phrases simply a list of tokens, e.g. foo bar baz , and imply intersection (AND) of the terms. Exact phrases are wrapped in quotes, e.g \"hello world\" . OR Unions (i.e word1 OR word2 ), are expressed with a pipe ( | ), e.g. hello|hallo|shalom|hola . NOT negation (i.e. word1 NOT word2 ) of expressions or sub-queries. e.g. hello -world . As of version 0.19.3, purely negative queries (i.e. -foo or -@title:(foo|bar) ) are supported. Prefix matches (all terms starting with a prefix) are expressed with a * . For performance reasons, a minimum prefix length is enforced (2 by default, but is configurable) A special \"wildcard query\" that returns all results in the index - * (cannot be combined with anything else). Selection of specific fields using the syntax @field:hello world . Numeric Range matches on numeric fields with the syntax @field:[{min} {max}] . Geo radius matches on geo fields with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] Tag field filters with the syntax @field:{tag | tag | ...} . See the full documentation on [tag fields|/Tags]. Optional terms or clauses: foo ~bar means bar is optional but documents with bar in them will rank higher. Fuzzy matching on terms (as of v1.2.0): %hello% means all terms with Levenshtein distance of 1 from it. An expression in a query can be wrapped in parentheses to disambiguate, e.g. (hello|hella) (world|werld) . Query attributes can be applied to individual clauses, e.g. (foo bar) = { $weight: 2.0; $slop: 1; $inorder: false; } Combinations of the above can be used together, e.g hello (world|foo) \"bar baz\" bbbb Pure negative queries As of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g. -hello or -(@title:foo|bar) . The results will be all the documents NOT containing the query terms. Warning Any complex expression can be negated this way, however, caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption. Field modifiers As of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword. Per query expression or sub-expression, it is possible to specify which fields it matches, by prepending the expression with the @ symbol, the field name and a : (colon) symbol. If a field modifier precedes multiple words, they are considered to be a phrase with the same modifier. If a field modifier precedes an expression in parentheses, it applies only to the expression inside the parentheses. Multiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query: FT.SEARCH cars @country:korea @engine:(diesel|hybrid) @class:suv Multiple modifiers can be applied to the same term or grouped terms. e.g.: FT.SEARCH idx @title|body:(hello world) @url|image:mydomain This will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields. Numeric filters in query If a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the query. The syntax is @field:[{min} {max}] - e.g. @price:[100 200] . A few notes on numeric predicates It is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument. It is possible to intersect or union multiple numeric filters in the same query, be it for the same field or different ones. -inf , inf and +inf are acceptable numbers in a range. Thus greater-than 100 is expressed as [(100 inf] . Numeric filters are inclusive. Exclusive min or max are expressed with ( prepended to the number, e.g. [(100 (200] . It is possible to negate a numeric filter by prepending a - sign to the filter, e.g. returning a result where price differs from 100 is expressed as: @title:foo -@price:[100 100] . Tag filters RediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax: @field:{ tag | tag | ...} e.g. @cities:{ New York | Los Angeles | Barcelona } Tags can have multiple words or include other punctuation marks other than the field's separator ( , by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags. Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing all tags, you should repeat the tag filter several times, e.g.: # This will return all documents containing all three cities as tags: @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona } # This will return all documents containing either city: @cities:{ New York | Los Angeles | Barcelona } Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc. Geo filters in query As of version 0.21, it is possible to add geo radius queries directly into the query language with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] . This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own GEORADIUS command for more details as it is used internally for that). Radius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as: chinese restaurant @location:[-122.41 37.77 5 km] . Prefix matching On index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending * to a prefix token. For example: hel* world Will be expanded to cover (hello|help|helm|...) world . A few notes on prefix searches As prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffixes. As a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching: Prefixes are limited to 2 letters or more. You can change this number by using the MINPREFIX setting on the module command line. Expansion is limited to 200 terms or less. You can change this number by using the MAXEXPANSIONS setting on the module command line. Prefix matching fully supports Unicode and is case insensitive. Currently, there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap. Fuzzy matching As of v1.2.0, the dictionary of all terms in the index can also be used to perform Fuzzy Matching . Fuzzy matches are performed based on Levenshtein distance (LD). Fuzzy matching on a term is performed by surrounding the term with '%', for example: %hello % world Will perform fuzzy matching on 'hello' for all terms where LD is 1. As of v1.4.0, the LD of the fuzzy match can be set by the number of '%' surrounding it, so that %%hello%% will perform fuzzy matching on 'hello' for all terms where LD is 2. The maximal LD for fuzzy matching is 3. Wildcard queries As of version 1.1.0, we provide a special query to retrieve all the documents in an index. This is meant mostly for the aggregation engine. You can call it by specifying only a single star sign as the query string - i.e. FT.SEARCH myIndex * . This cannot be combined with any other filters, field modifiers or anything inside the query. It is technically possible to use the deprecated FILTER and GEOFILTER request parameters outside the query string in conjunction with a wildcard, but this makes the wildcard meaningless and only hurts performance. Query attributes As of version 1.2.0, it is possible to apply specific query modifying attributes to specific clauses of the query. The syntax is (foo bar) = { $attribute: value; $attribute:value; ...} , e.g: ( foo bar ) = { $ weight : 2.0 ; $ slop : 1 ; $ inorder : true ; } ~( bar baz ) = { $ weight : 0.5 ; } The supported attributes are: $weight : determines the weight of the sub-query or token in the overall ranking on the result (default: 1.0). $slop : determines the maximum allowed \"slop\" (space between terms) in the query clause (default: 0). $inorder : whether or not the terms in a query clause must appear in the same order as in the query, usually set alongside with $slop (default: false). $phonetic : whether or not to perform phonetic matching (default: true). Note: setting this attribute on for fields which were not creates as PHONETIC will produce an error. A few query examples Simple phrase query - hello AND world hello world Exact phrase query - hello FOLLOWED BY world hello world Union: documents containing either hello OR world hello|world Not: documents containing hello but not world hello -world Intersection of unions (hello|halo) (world|werld) Negation of union hello -(world|werld) Union inside phrase (barack|barrack) obama Optional terms with higher priority to ones containing more matches: obama ~barack ~michelle Exact phrase in one field, one word in another field: @title: barack obama @job:president Combined AND, OR with field specifiers: @title:hello world @body:(foo bar) @category:(articles|biographies) Prefix Queries: hello worl* hel* worl* hello -worl* Numeric Filtering - products named \"tv\" with a price range of 200-500: @name:tv @price:[200 500] Numeric Filtering - users with age greater than 18: @age:[(18 +inf] Mapping common SQL predicates to RediSearch SQL Condition RediSearch Equivalent Comments WHERE x='foo' AND y='bar' @x:foo @y:bar for less ambiguity use (@x:foo) (@y:bar) WHERE x='foo' AND y!='bar' @x:foo -@y:bar WHERE x='foo' OR y='bar' (@x:foo)|(@y:bar) WHERE x IN ('foo', 'bar','hello world') @x:(foo|bar|\"hello world\") quotes mean exact phrase WHERE y='foo' AND x NOT IN ('foo','bar') @y:foo (-@x:foo) (-@x:bar) WHERE x NOT IN ('foo','bar') -@x:(foo|bar) WHERE num BETWEEN 10 AND 20 @num:[10 20] WHERE num = 10 @num:[10 +inf] WHERE num 10 @num:[(10 +inf] WHERE num 10 @num:[-inf (10] WHERE num = 10 @num:[-inf 10] WHERE num 10 OR num 20 @num:[-inf (10] | @num:[(20 +inf] WHERE name LIKE 'john%' @name:john* Technical note The query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition at the git repo .","title":"Query Syntax"},{"location":"Query_Syntax/#search_query_syntax","text":"We support a simple syntax for complex queries with the following rules: Multi-word phrases simply a list of tokens, e.g. foo bar baz , and imply intersection (AND) of the terms. Exact phrases are wrapped in quotes, e.g \"hello world\" . OR Unions (i.e word1 OR word2 ), are expressed with a pipe ( | ), e.g. hello|hallo|shalom|hola . NOT negation (i.e. word1 NOT word2 ) of expressions or sub-queries. e.g. hello -world . As of version 0.19.3, purely negative queries (i.e. -foo or -@title:(foo|bar) ) are supported. Prefix matches (all terms starting with a prefix) are expressed with a * . For performance reasons, a minimum prefix length is enforced (2 by default, but is configurable) A special \"wildcard query\" that returns all results in the index - * (cannot be combined with anything else). Selection of specific fields using the syntax @field:hello world . Numeric Range matches on numeric fields with the syntax @field:[{min} {max}] . Geo radius matches on geo fields with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] Tag field filters with the syntax @field:{tag | tag | ...} . See the full documentation on [tag fields|/Tags]. Optional terms or clauses: foo ~bar means bar is optional but documents with bar in them will rank higher. Fuzzy matching on terms (as of v1.2.0): %hello% means all terms with Levenshtein distance of 1 from it. An expression in a query can be wrapped in parentheses to disambiguate, e.g. (hello|hella) (world|werld) . Query attributes can be applied to individual clauses, e.g. (foo bar) = { $weight: 2.0; $slop: 1; $inorder: false; } Combinations of the above can be used together, e.g hello (world|foo) \"bar baz\" bbbb","title":"Search Query Syntax"},{"location":"Query_Syntax/#pure_negative_queries","text":"As of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g. -hello or -(@title:foo|bar) . The results will be all the documents NOT containing the query terms. Warning Any complex expression can be negated this way, however, caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.","title":"Pure negative queries"},{"location":"Query_Syntax/#field_modifiers","text":"As of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword. Per query expression or sub-expression, it is possible to specify which fields it matches, by prepending the expression with the @ symbol, the field name and a : (colon) symbol. If a field modifier precedes multiple words, they are considered to be a phrase with the same modifier. If a field modifier precedes an expression in parentheses, it applies only to the expression inside the parentheses. Multiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query: FT.SEARCH cars @country:korea @engine:(diesel|hybrid) @class:suv Multiple modifiers can be applied to the same term or grouped terms. e.g.: FT.SEARCH idx @title|body:(hello world) @url|image:mydomain This will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.","title":"Field modifiers"},{"location":"Query_Syntax/#numeric_filters_in_query","text":"If a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the query. The syntax is @field:[{min} {max}] - e.g. @price:[100 200] .","title":"Numeric filters in query"},{"location":"Query_Syntax/#a_few_notes_on_numeric_predicates","text":"It is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument. It is possible to intersect or union multiple numeric filters in the same query, be it for the same field or different ones. -inf , inf and +inf are acceptable numbers in a range. Thus greater-than 100 is expressed as [(100 inf] . Numeric filters are inclusive. Exclusive min or max are expressed with ( prepended to the number, e.g. [(100 (200] . It is possible to negate a numeric filter by prepending a - sign to the filter, e.g. returning a result where price differs from 100 is expressed as: @title:foo -@price:[100 100] .","title":"A few notes on numeric predicates"},{"location":"Query_Syntax/#tag_filters","text":"RediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax: @field:{ tag | tag | ...} e.g. @cities:{ New York | Los Angeles | Barcelona } Tags can have multiple words or include other punctuation marks other than the field's separator ( , by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags. Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing all tags, you should repeat the tag filter several times, e.g.: # This will return all documents containing all three cities as tags: @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona } # This will return all documents containing either city: @cities:{ New York | Los Angeles | Barcelona } Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc.","title":"Tag filters"},{"location":"Query_Syntax/#geo_filters_in_query","text":"As of version 0.21, it is possible to add geo radius queries directly into the query language with the syntax @field:[{lon} {lat} {radius} {m|km|mi|ft}] . This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own GEORADIUS command for more details as it is used internally for that). Radius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as: chinese restaurant @location:[-122.41 37.77 5 km] .","title":"Geo filters in query"},{"location":"Query_Syntax/#prefix_matching","text":"On index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending * to a prefix token. For example: hel* world Will be expanded to cover (hello|help|helm|...) world .","title":"Prefix matching"},{"location":"Query_Syntax/#a_few_notes_on_prefix_searches","text":"As prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffixes. As a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching: Prefixes are limited to 2 letters or more. You can change this number by using the MINPREFIX setting on the module command line. Expansion is limited to 200 terms or less. You can change this number by using the MAXEXPANSIONS setting on the module command line. Prefix matching fully supports Unicode and is case insensitive. Currently, there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap.","title":"A few notes on prefix searches"},{"location":"Query_Syntax/#fuzzy_matching","text":"As of v1.2.0, the dictionary of all terms in the index can also be used to perform Fuzzy Matching . Fuzzy matches are performed based on Levenshtein distance (LD). Fuzzy matching on a term is performed by surrounding the term with '%', for example: %hello % world Will perform fuzzy matching on 'hello' for all terms where LD is 1. As of v1.4.0, the LD of the fuzzy match can be set by the number of '%' surrounding it, so that %%hello%% will perform fuzzy matching on 'hello' for all terms where LD is 2. The maximal LD for fuzzy matching is 3.","title":"Fuzzy matching"},{"location":"Query_Syntax/#wildcard_queries","text":"As of version 1.1.0, we provide a special query to retrieve all the documents in an index. This is meant mostly for the aggregation engine. You can call it by specifying only a single star sign as the query string - i.e. FT.SEARCH myIndex * . This cannot be combined with any other filters, field modifiers or anything inside the query. It is technically possible to use the deprecated FILTER and GEOFILTER request parameters outside the query string in conjunction with a wildcard, but this makes the wildcard meaningless and only hurts performance.","title":"Wildcard queries"},{"location":"Query_Syntax/#query_attributes","text":"As of version 1.2.0, it is possible to apply specific query modifying attributes to specific clauses of the query. The syntax is (foo bar) = { $attribute: value; $attribute:value; ...} , e.g: ( foo bar ) = { $ weight : 2.0 ; $ slop : 1 ; $ inorder : true ; } ~( bar baz ) = { $ weight : 0.5 ; } The supported attributes are: $weight : determines the weight of the sub-query or token in the overall ranking on the result (default: 1.0). $slop : determines the maximum allowed \"slop\" (space between terms) in the query clause (default: 0). $inorder : whether or not the terms in a query clause must appear in the same order as in the query, usually set alongside with $slop (default: false). $phonetic : whether or not to perform phonetic matching (default: true). Note: setting this attribute on for fields which were not creates as PHONETIC will produce an error.","title":"Query attributes"},{"location":"Query_Syntax/#a_few_query_examples","text":"Simple phrase query - hello AND world hello world Exact phrase query - hello FOLLOWED BY world hello world Union: documents containing either hello OR world hello|world Not: documents containing hello but not world hello -world Intersection of unions (hello|halo) (world|werld) Negation of union hello -(world|werld) Union inside phrase (barack|barrack) obama Optional terms with higher priority to ones containing more matches: obama ~barack ~michelle Exact phrase in one field, one word in another field: @title: barack obama @job:president Combined AND, OR with field specifiers: @title:hello world @body:(foo bar) @category:(articles|biographies) Prefix Queries: hello worl* hel* worl* hello -worl* Numeric Filtering - products named \"tv\" with a price range of 200-500: @name:tv @price:[200 500] Numeric Filtering - users with age greater than 18: @age:[(18 +inf]","title":"A few query examples"},{"location":"Query_Syntax/#mapping_common_sql_predicates_to_redisearch","text":"SQL Condition RediSearch Equivalent Comments WHERE x='foo' AND y='bar' @x:foo @y:bar for less ambiguity use (@x:foo) (@y:bar) WHERE x='foo' AND y!='bar' @x:foo -@y:bar WHERE x='foo' OR y='bar' (@x:foo)|(@y:bar) WHERE x IN ('foo', 'bar','hello world') @x:(foo|bar|\"hello world\") quotes mean exact phrase WHERE y='foo' AND x NOT IN ('foo','bar') @y:foo (-@x:foo) (-@x:bar) WHERE x NOT IN ('foo','bar') -@x:(foo|bar) WHERE num BETWEEN 10 AND 20 @num:[10 20] WHERE num = 10 @num:[10 +inf] WHERE num 10 @num:[(10 +inf] WHERE num 10 @num:[-inf (10] WHERE num = 10 @num:[-inf 10] WHERE num 10 OR num 20 @num:[-inf (10] | @num:[(20 +inf] WHERE name LIKE 'john%' @name:john*","title":"Mapping common SQL predicates to RediSearch"},{"location":"Query_Syntax/#technical_note","text":"The query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition at the git repo .","title":"Technical note"},{"location":"Quick_Start/","text":"Quick Start Guide for RediSearch Running with Docker docker run -p 6379 :6379 redislabs/redisearch:latest Building and running from source RediSearch uses CMake as its build system. CMake is available for almost every available platform. You can obtain cmake through your operating system's package manager. RediSearch requires CMake version 3 or greater. If your package repository does not contain CMake3, you can download a precompiled binary from CMake downloads . To build using CMake: git clone https://github.com/RedisLabsModules/RediSearch.git cd RediSearch mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = RelWithDebInfo make redis-server --loadmodule ./redisearch.so The resulting module will be in the current directory. You can also simply type make from the top level directory, this will take care of running cmake with the appropriate arguments, and provide you with a redisearch.so file in the src directory: git clone https://github.com/RedisLabsModules/RediSearch.git cd RediSearch make redis-server --loadmodule ./src/redisearch.so Creating an index with fields and weights (default weight is 1.0) 127.0.0.1:6379 FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT OK Adding documents to the index 127.0.0.1:6379 FT.ADD myIdx doc1 1.0 FIELDS title hello world body lorem ipsum url http://redis.io OK Searching the index 127.0.0.1:6379 FT.SEARCH myIdx hello world LIMIT 0 10 1) (integer) 1 2) doc1 3) 1) title 2) hello world 3) body 4) lorem ipsum 5) url 6) http://redis.io Note Input is expected to be valid utf-8 or ASCII. The engine cannot handle wide character unicode at the moment. Dropping the index 127.0.0.1:6379 FT.DROP myIdx OK Adding and getting Auto-complete suggestions 127.0.0.1:6379 FT.SUGADD autocomplete hello world 100 OK 127.0.0.1:6379 FT.SUGGET autocomplete he 1) hello world","title":"Quick Start"},{"location":"Quick_Start/#quick_start_guide_for_redisearch","text":"","title":"Quick Start Guide for RediSearch"},{"location":"Quick_Start/#running_with_docker","text":"docker run -p 6379 :6379 redislabs/redisearch:latest","title":"Running with Docker"},{"location":"Quick_Start/#building_and_running_from_source","text":"RediSearch uses CMake as its build system. CMake is available for almost every available platform. You can obtain cmake through your operating system's package manager. RediSearch requires CMake version 3 or greater. If your package repository does not contain CMake3, you can download a precompiled binary from CMake downloads . To build using CMake: git clone https://github.com/RedisLabsModules/RediSearch.git cd RediSearch mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = RelWithDebInfo make redis-server --loadmodule ./redisearch.so The resulting module will be in the current directory. You can also simply type make from the top level directory, this will take care of running cmake with the appropriate arguments, and provide you with a redisearch.so file in the src directory: git clone https://github.com/RedisLabsModules/RediSearch.git cd RediSearch make redis-server --loadmodule ./src/redisearch.so","title":"Building and running from source"},{"location":"Quick_Start/#creating_an_index_with_fields_and_weights_default_weight_is_10","text":"127.0.0.1:6379 FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT OK","title":"Creating an index with fields and weights (default weight is 1.0)"},{"location":"Quick_Start/#adding_documents_to_the_index","text":"127.0.0.1:6379 FT.ADD myIdx doc1 1.0 FIELDS title hello world body lorem ipsum url http://redis.io OK","title":"Adding documents to the index"},{"location":"Quick_Start/#searching_the_index","text":"127.0.0.1:6379 FT.SEARCH myIdx hello world LIMIT 0 10 1) (integer) 1 2) doc1 3) 1) title 2) hello world 3) body 4) lorem ipsum 5) url 6) http://redis.io Note Input is expected to be valid utf-8 or ASCII. The engine cannot handle wide character unicode at the moment.","title":"Searching the index"},{"location":"Quick_Start/#dropping_the_index","text":"127.0.0.1:6379 FT.DROP myIdx OK","title":"Dropping the index"},{"location":"Quick_Start/#adding_and_getting_auto-complete_suggestions","text":"127.0.0.1:6379 FT.SUGADD autocomplete hello world 100 OK 127.0.0.1:6379 FT.SUGGET autocomplete he 1) hello world","title":"Adding and getting Auto-complete suggestions"},{"location":"Scoring/","text":"Scoring in RediSearch RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use sortable fields . Scoring functions are specified by adding the SCORER {scorer_name} argument to a search query. If you prefer a custom scoring function, it is possible to add more functions using the Extension API . These are the pre-bundled scoring functions available in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a SCORER argument in FT.SEARCH . TFIDF (default) Basic TF-IDF scoring with a few extra features thrown inside: For each term in each result, we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is normalized by the highest term frequency in each document . We multiply the total TF-IDF for the query term by the a priory document score given on FT.ADD . We give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penalty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared - 1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...) . So for N terms in document D, T1...Tn , the resulting score could be described with this python function: def get_score ( terms , doc ): # the sum of tf-idf score = 0 # the distance penalty for all terms dist_penalty = 0 for i , term in enumerate ( terms ): # tf normalized by maximum frequency tf = doc . freq ( term ) / doc . max_freq # idf is global for the index, and not calculated each time in real life idf = log2 ( 1 + total_docs / docs_with_term ( term )) score += tf * idf # sum up the distance penalty if i 0 : dist_penalty += min_distance ( term , terms [ i - 1 ]) ** 2 # multiply the score by the document score score *= doc . score # divide the score by the root of the cumulative distance if len ( terms ) 1 : score /= sqrt ( dist_penalty ) return score TFIDF.DOCNORM Identical to the default TFIDF scorer, with one important distinction: Term frequencies are normalized by the length of the document (expressed as the total number of terms). The length is weighted, so that if a document contains two terms, one in a field that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2. FT.SEARCH myIndex foo SCORER TFIDF.DOCNORM BM25 A variation on the basic TF-IDF scorer, see this Wikipedia article for more info . We also multiply the relevance score for each document by the a priory document score and apply a penalty based on slop as in TFIDF. FT.SEARCH myIndex foo SCORER BM25 DISMAX A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied. It is not a 1 to 1 implementation of Solr's DISMAX algorithm but follows it in broad terms. FT.SEARCH myIndex foo SCORER DISMAX DOCSCORE A scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further. FT.SEARCH myIndex foo SCORER DOCSCORE HAMMING Scoring by the (inverse) Hamming Distance between the documents' payload and the query payload. Since we are interested in the nearest neighbors, we inverse the hamming distance ( 1/(1+d) ) so that a distance of 0 gives a perfect score of 1 and is the highest rank. This works only if: The document has a payload. The query has a payload. Both are exactly the same length . Payloads are binary-safe, and having payloads with a length that's a multiple of 64 bits yields slightly faster results. Example: 127.0.0.1:6379 FT.CREATE idx SCHEMA foo TEXT OK 127.0.0.1:6379 FT.ADD idx 1 1 PAYLOAD aaaabbbb FIELDS foo hello OK 127.0.0.1:6379 FT.ADD idx 2 1 PAYLOAD aaaacccc FIELDS foo bar OK 127.0.0.1:6379 FT.SEARCH idx * PAYLOAD aaaabbbc SCORER HAMMING WITHSCORES 1) (integer) 2 2) 1 3) 0.5 // hamming distance of 1 -- 1/(1+1) == 0.5 4) 1) foo 2) hello 5) 2 6) 0.25 // hamming distance of 3 -- 1/(1+3) == 0.25 7) 1) foo 2) bar","title":"Scoring Documents"},{"location":"Scoring/#scoring_in_redisearch","text":"RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use sortable fields . Scoring functions are specified by adding the SCORER {scorer_name} argument to a search query. If you prefer a custom scoring function, it is possible to add more functions using the Extension API . These are the pre-bundled scoring functions available in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a SCORER argument in FT.SEARCH .","title":"Scoring in RediSearch"},{"location":"Scoring/#tfidf_default","text":"Basic TF-IDF scoring with a few extra features thrown inside: For each term in each result, we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is normalized by the highest term frequency in each document . We multiply the total TF-IDF for the query term by the a priory document score given on FT.ADD . We give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penalty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared - 1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...) . So for N terms in document D, T1...Tn , the resulting score could be described with this python function: def get_score ( terms , doc ): # the sum of tf-idf score = 0 # the distance penalty for all terms dist_penalty = 0 for i , term in enumerate ( terms ): # tf normalized by maximum frequency tf = doc . freq ( term ) / doc . max_freq # idf is global for the index, and not calculated each time in real life idf = log2 ( 1 + total_docs / docs_with_term ( term )) score += tf * idf # sum up the distance penalty if i 0 : dist_penalty += min_distance ( term , terms [ i - 1 ]) ** 2 # multiply the score by the document score score *= doc . score # divide the score by the root of the cumulative distance if len ( terms ) 1 : score /= sqrt ( dist_penalty ) return score","title":"TFIDF (default)"},{"location":"Scoring/#tfidfdocnorm","text":"Identical to the default TFIDF scorer, with one important distinction: Term frequencies are normalized by the length of the document (expressed as the total number of terms). The length is weighted, so that if a document contains two terms, one in a field that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2. FT.SEARCH myIndex foo SCORER TFIDF.DOCNORM","title":"TFIDF.DOCNORM"},{"location":"Scoring/#bm25","text":"A variation on the basic TF-IDF scorer, see this Wikipedia article for more info . We also multiply the relevance score for each document by the a priory document score and apply a penalty based on slop as in TFIDF. FT.SEARCH myIndex foo SCORER BM25","title":"BM25"},{"location":"Scoring/#dismax","text":"A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied. It is not a 1 to 1 implementation of Solr's DISMAX algorithm but follows it in broad terms. FT.SEARCH myIndex foo SCORER DISMAX","title":"DISMAX"},{"location":"Scoring/#docscore","text":"A scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further. FT.SEARCH myIndex foo SCORER DOCSCORE","title":"DOCSCORE"},{"location":"Scoring/#hamming","text":"Scoring by the (inverse) Hamming Distance between the documents' payload and the query payload. Since we are interested in the nearest neighbors, we inverse the hamming distance ( 1/(1+d) ) so that a distance of 0 gives a perfect score of 1 and is the highest rank. This works only if: The document has a payload. The query has a payload. Both are exactly the same length . Payloads are binary-safe, and having payloads with a length that's a multiple of 64 bits yields slightly faster results. Example: 127.0.0.1:6379 FT.CREATE idx SCHEMA foo TEXT OK 127.0.0.1:6379 FT.ADD idx 1 1 PAYLOAD aaaabbbb FIELDS foo hello OK 127.0.0.1:6379 FT.ADD idx 2 1 PAYLOAD aaaacccc FIELDS foo bar OK 127.0.0.1:6379 FT.SEARCH idx * PAYLOAD aaaabbbc SCORER HAMMING WITHSCORES 1) (integer) 2 2) 1 3) 0.5 // hamming distance of 1 -- 1/(1+1) == 0.5 4) 1) foo 2) hello 5) 2 6) 0.25 // hamming distance of 3 -- 1/(1+3) == 0.25 7) 1) foo 2) bar","title":"HAMMING"},{"location":"Sorting/","text":"Sorting by Indexed Fields As of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name. Declaring Sortable Fields When creating the index with FT.CREATE , you can declare TEXT and NUMERIC properties to be SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the following schema: FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE The fields last_name and age are sortable, but first_name isn't. This means we can search by either first and/or last name, and sort by last name or age. Note on sortable TEXT fields In the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it. Also, note that text fields get normalized and lowercased in a Unicode-safe way when stored for sorting and currently there is no way to change this behaviour. This means that America and america are considered equal in terms of sorting. Specifying SORTBY If an index includes sortable fields, you can add the SORTBY parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If WITHSCORES is specified along with SORTBY , the scores returned are simply the relative position of each result in the result set. The syntax for SORTBY is: SORTBY {field_name} [ASC|DESC] field_name must be a sortable field defined in the schema. ASC means the order will be ascending, DESC that it will be descending. The default ordering is ASC if not specified otherwise. Quick example FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE # Add some users FT.ADD users user1 1.0 FIELDS first_name alice last_name jones age 35 FT.ADD users user2 1.0 FIELDS first_name bob last_name jones age 36 # Searching while sorting # Searching by last name and sorting by first name FT.SEARCH users @last_name:jones SORTBY first_name DESC # Searching by both first and last name, and sorting by age FT.SEARCH users alice jones SORTBY age ASC","title":"Sortable Values"},{"location":"Sorting/#sorting_by_indexed_fields","text":"As of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name.","title":"Sorting by Indexed Fields"},{"location":"Sorting/#declaring_sortable_fields","text":"When creating the index with FT.CREATE , you can declare TEXT and NUMERIC properties to be SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the following schema: FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE The fields last_name and age are sortable, but first_name isn't. This means we can search by either first and/or last name, and sort by last name or age.","title":"Declaring Sortable Fields"},{"location":"Sorting/#note_on_sortable_text_fields","text":"In the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it. Also, note that text fields get normalized and lowercased in a Unicode-safe way when stored for sorting and currently there is no way to change this behaviour. This means that America and america are considered equal in terms of sorting.","title":"Note on sortable TEXT fields"},{"location":"Sorting/#specifying_sortby","text":"If an index includes sortable fields, you can add the SORTBY parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If WITHSCORES is specified along with SORTBY , the scores returned are simply the relative position of each result in the result set. The syntax for SORTBY is: SORTBY {field_name} [ASC|DESC] field_name must be a sortable field defined in the schema. ASC means the order will be ascending, DESC that it will be descending. The default ordering is ASC if not specified otherwise.","title":"Specifying SORTBY"},{"location":"Sorting/#quick_example","text":"FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE # Add some users FT.ADD users user1 1.0 FIELDS first_name alice last_name jones age 35 FT.ADD users user2 1.0 FIELDS first_name bob last_name jones age 36 # Searching while sorting # Searching by last name and sorting by first name FT.SEARCH users @last_name:jones SORTBY first_name DESC # Searching by both first and last name, and sorting by age FT.SEARCH users alice jones SORTBY age ASC","title":"Quick example"},{"location":"Spellcheck/","text":"Query Spelling Correction Query spelling correction, a.k.a \"did you mean\", provides suggestions for misspelled search terms. For example, the term 'reids' may be a misspelled 'redis'. In such cases and as of v1.4 RediSearch can be used for generating alternatives to misspelled query terms. A misspelled term is a full text term (i.e., a word) that is: Not a stop word Not in the index At least 3 characters long The alternatives for a misspelled term are generated from the corpus of already-indexed terms and, optionally, one or more custom dictionaries. Alternatives become spelling suggestions based on their respective Levenshtein distances (LD) from the misspelled term. Each spelling suggestion is given a normalized score based on its occurances in the index. To obtain the spelling corrections for a query, refer to the documentation of the FT.SPELLCHECK command. Custom dictionaries A dictionary is a set of terms. Dictionaries can be added with terms, have terms deleted from them and have their entire contents dumped using the FT.DICTADD , FT.DICTDEL and FT.DICTDUMP commands, respectively. Dictionaries can be used to modify the behavior of RediSearch's query spelling correction, by including or excluding their contents from potential spelling correction suggestions. When used for term inclusion, the terms in a dictionary can be provided as spelling suggestions regardless their occurances (or lack of) in the index. Scores of suggestions from inclusion dictionaries are always 0. Conversely, terms in an exlusion dictionary will never be returned as spelling alternatives.","title":"Spelling Correction"},{"location":"Spellcheck/#query_spelling_correction","text":"Query spelling correction, a.k.a \"did you mean\", provides suggestions for misspelled search terms. For example, the term 'reids' may be a misspelled 'redis'. In such cases and as of v1.4 RediSearch can be used for generating alternatives to misspelled query terms. A misspelled term is a full text term (i.e., a word) that is: Not a stop word Not in the index At least 3 characters long The alternatives for a misspelled term are generated from the corpus of already-indexed terms and, optionally, one or more custom dictionaries. Alternatives become spelling suggestions based on their respective Levenshtein distances (LD) from the misspelled term. Each spelling suggestion is given a normalized score based on its occurances in the index. To obtain the spelling corrections for a query, refer to the documentation of the FT.SPELLCHECK command.","title":"Query Spelling Correction"},{"location":"Spellcheck/#custom_dictionaries","text":"A dictionary is a set of terms. Dictionaries can be added with terms, have terms deleted from them and have their entire contents dumped using the FT.DICTADD , FT.DICTDEL and FT.DICTDUMP commands, respectively. Dictionaries can be used to modify the behavior of RediSearch's query spelling correction, by including or excluding their contents from potential spelling correction suggestions. When used for term inclusion, the terms in a dictionary can be provided as spelling suggestions regardless their occurances (or lack of) in the index. Scores of suggestions from inclusion dictionaries are always 0. Conversely, terms in an exlusion dictionary will never be returned as spelling alternatives.","title":"Custom dictionaries"},{"location":"Stemming/","text":"Stemming Support RediSearch supports stemming - that is adding the base form of a word to the index. This allows the query for \"going\" to also return results for \"go\" and \"gone\", for example. The current stemming support is based on the Snowball stemmer library, which supports most European languages, as well as Arabic and other. We hope to include more languages soon (if you need a specific language support, please open an issue). For further details see the Snowball Stemmer website . Supported languages The following languages are supported and can be passed to the engine when indexing or querying, with lowercase letters: arabic danish dutch english finnish french german hungarian italian norwegian portuguese romanian russian spanish swedish tamil turkish chinese (see below) Chinese support Indexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese. Chinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match. RediSearch makes use of the Friso chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required. Using custom dictionaries If you wish to use a custom dictionary, you can do so at the module level when loading the module. The FRISOINI setting can point to the location of a friso.ini file which contains the relevant settings and paths to the dictionary files. Note that there is no \"default\" friso.ini file location. RedisSearch comes with its own friso.ini and dictionary files which are compiled into the module binary at build-time.","title":"Stemming Support"},{"location":"Stemming/#stemming_support","text":"RediSearch supports stemming - that is adding the base form of a word to the index. This allows the query for \"going\" to also return results for \"go\" and \"gone\", for example. The current stemming support is based on the Snowball stemmer library, which supports most European languages, as well as Arabic and other. We hope to include more languages soon (if you need a specific language support, please open an issue). For further details see the Snowball Stemmer website .","title":"Stemming Support"},{"location":"Stemming/#supported_languages","text":"The following languages are supported and can be passed to the engine when indexing or querying, with lowercase letters: arabic danish dutch english finnish french german hungarian italian norwegian portuguese romanian russian spanish swedish tamil turkish chinese (see below)","title":"Supported languages"},{"location":"Stemming/#chinese_support","text":"Indexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese. Chinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match. RediSearch makes use of the Friso chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required.","title":"Chinese support"},{"location":"Stemming/#using_custom_dictionaries","text":"If you wish to use a custom dictionary, you can do so at the module level when loading the module. The FRISOINI setting can point to the location of a friso.ini file which contains the relevant settings and paths to the dictionary files. Note that there is no \"default\" friso.ini file location. RedisSearch comes with its own friso.ini and dictionary files which are compiled into the module binary at build-time.","title":"Using custom dictionaries"},{"location":"Stopwords/","text":"Stop-Words RediSearch has a pre-defined default list of stop-words . These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index. When indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query. At the moment, the default stop-word list applies to all full-text indexes in all languages and can be overridden manually at index creation time. Default stop-word list The following words are treated as stop-words by default: a, is, the, an, and, are, as, at, be, but, by, for, if, in, into, it, no, not, of, on, or, such, that, their, then, there, these, they, this, to, was, will, with Overriding the default stop-words Stop-words for an index can be defined (or disabled completely) on index creation using the STOPWORDS argument in the FT.CREATE command. The format is STOPWORDS {number} {stopword} ... where number is the number of stopwords given. The STOPWORDS argument must come before the SCHEMA argument. For example: FT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT Disabling stop-words completely Disabling stopwords completely can be done by passing STOPWORDS 0 on FT.CREATE . Avoiding stop-word detection in search queries In rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.","title":"Stop-Words"},{"location":"Stopwords/#stop-words","text":"RediSearch has a pre-defined default list of stop-words . These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index. When indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query. At the moment, the default stop-word list applies to all full-text indexes in all languages and can be overridden manually at index creation time.","title":"Stop-Words"},{"location":"Stopwords/#default_stop-word_list","text":"The following words are treated as stop-words by default: a, is, the, an, and, are, as, at, be, but, by, for, if, in, into, it, no, not, of, on, or, such, that, their, then, there, these, they, this, to, was, will, with","title":"Default stop-word list"},{"location":"Stopwords/#overriding_the_default_stop-words","text":"Stop-words for an index can be defined (or disabled completely) on index creation using the STOPWORDS argument in the FT.CREATE command. The format is STOPWORDS {number} {stopword} ... where number is the number of stopwords given. The STOPWORDS argument must come before the SCHEMA argument. For example: FT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT","title":"Overriding the default stop-words"},{"location":"Stopwords/#disabling_stop-words_completely","text":"Disabling stopwords completely can be done by passing STOPWORDS 0 on FT.CREATE .","title":"Disabling stop-words completely"},{"location":"Stopwords/#avoiding_stop-word_detection_in_search_queries","text":"In rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.","title":"Avoiding stop-word detection in search queries"},{"location":"Synonyms/","text":"Synonyms Support Overview RediSearch supports synonyms - that is searching for synonyms words defined by the synonym data structure. The synonym data structure is a set of groups, each group contains synonym terms. For example, the following synonym data structure contains three groups, each group contains three synonym terms: {boy, child, baby} {girl, child, baby} {man, person, adult} When these three groups are located inside the synonym data structure, it is possible to search for 'child' and receive documents contains 'boy', 'girl', 'child' and 'baby'. The synonym search technique We use a simple HashMap to map between the terms and the group ids. During building the index, we check if the current term appears in the synonym map, and if it does we take all the group ids that the term belongs to. For each group id, we add another record to the inverted index called \"\\~\\ id>\" that contains the same information as the term itself. When performing a search, we check if the searched term appears in the synonym map, and if it does we take all the group ids the term is belong to. For each group id, we search for \"\\~\\ id>\" and return the combined results. This technique ensures that we return all the synonyms of a given term. Handling concurrency Since the indexing is performed in a separate thread, the synonyms map may change during the indexing, which in turn may cause data corruption or crashes during indexing/searches. To solve this issue, we create a read-only copy for indexing purposes. The read-only copy is maintained using ref count. As long as the synonyms map does not change, the original synonym map holds a reference to its read-only copy so it will not be freed. Once the data inside the synonyms map has changed, the synonyms map decreses the reference count of its read only copy. This ensures that when all the indexers are done using the read only copy, then the read only copy will automatically freed. Also it ensures that the next time an indexer asks for a read-only copy, the synonyms map will create a new copy (contains the new data) and return it.","title":"Synonyms Support"},{"location":"Synonyms/#synonyms_support","text":"","title":"Synonyms Support"},{"location":"Synonyms/#overview","text":"RediSearch supports synonyms - that is searching for synonyms words defined by the synonym data structure. The synonym data structure is a set of groups, each group contains synonym terms. For example, the following synonym data structure contains three groups, each group contains three synonym terms: {boy, child, baby} {girl, child, baby} {man, person, adult} When these three groups are located inside the synonym data structure, it is possible to search for 'child' and receive documents contains 'boy', 'girl', 'child' and 'baby'.","title":"Overview"},{"location":"Synonyms/#the_synonym_search_technique","text":"We use a simple HashMap to map between the terms and the group ids. During building the index, we check if the current term appears in the synonym map, and if it does we take all the group ids that the term belongs to. For each group id, we add another record to the inverted index called \"\\~\\ id>\" that contains the same information as the term itself. When performing a search, we check if the searched term appears in the synonym map, and if it does we take all the group ids the term is belong to. For each group id, we search for \"\\~\\ id>\" and return the combined results. This technique ensures that we return all the synonyms of a given term.","title":"The synonym search technique"},{"location":"Synonyms/#handling_concurrency","text":"Since the indexing is performed in a separate thread, the synonyms map may change during the indexing, which in turn may cause data corruption or crashes during indexing/searches. To solve this issue, we create a read-only copy for indexing purposes. The read-only copy is maintained using ref count. As long as the synonyms map does not change, the original synonym map holds a reference to its read-only copy so it will not be freed. Once the data inside the synonyms map has changed, the synonyms map decreses the reference count of its read only copy. This ensures that when all the indexers are done using the read only copy, then the read only copy will automatically freed. Also it ensures that the next time an indexer asks for a read-only copy, the synonyms map will create a new copy (contains the new data) and return it.","title":"Handling concurrency"},{"location":"Tags/","text":"Tag Fields RediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text fields but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax. The main differences between tag and full-text fields are: An entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one. We do not perform stemming on tag indexes. The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming. Tags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document. The index is much simpler and more compressed: We do not store frequencies, offset vectors of field flags. The index contains only document IDs encoded as deltas. This means that an entry in a tag index is usually one or two bytes long. This makes them very memory efficient and fast. An unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024. Creating a tag field Tag fields can be added to the schema in FT.ADD with the following syntax: FT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}] SEPARATOR defaults to a comma ( , ), and can be any printable ASCII character. For example: FT.CREATE idx SCHEMA tags TAG SEPARATOR ; Querying tag fields As mentioned above, just searching for a tag without any modifiers will not retrieve documents containing it. The syntax for matching tags in a query is as follows (the curly braces are part of the syntax in this case): @ field_name :{ tag | tag | ...} e.g. @tags:{hello world | foo bar} Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc. For example: FT.SEARCH idx @title:hello @price:[0 100] @tags:{ foo bar | hello world } Multiple tags in a single filter Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing all tags, you should repeat the tag filter several times. For example, imagine an index of travellers, with a tag field for the cities each traveller has visited: FT.CREATE myIndex SCHEMA name TEXT cities TAG FT.ADD myIndex user1 1.0 FIELDS name John Doe cities New York, Barcelona, San Francisco For this index, the following query will return all the people who visited at least one of the following cities: FT.SEARCH myIndex @cities:{ New York | Los Angeles | Barcelona } But the next query will return all people who have visited all three cities : @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona } Multi-word tags And escaping Tags can be composed multiple words, or include other punctuation marks other than the field's separator ( , by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). NOTE: in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\-world. It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags. The following are identical: @tags:{foo\\ bar\\ baz | hello\\ world} @tags:{foo bar baz | hello world }","title":"Tag Fields"},{"location":"Tags/#tag_fields","text":"RediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text fields but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax. The main differences between tag and full-text fields are: An entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one. We do not perform stemming on tag indexes. The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming. Tags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document. The index is much simpler and more compressed: We do not store frequencies, offset vectors of field flags. The index contains only document IDs encoded as deltas. This means that an entry in a tag index is usually one or two bytes long. This makes them very memory efficient and fast. An unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024.","title":"Tag Fields"},{"location":"Tags/#creating_a_tag_field","text":"Tag fields can be added to the schema in FT.ADD with the following syntax: FT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}] SEPARATOR defaults to a comma ( , ), and can be any printable ASCII character. For example: FT.CREATE idx SCHEMA tags TAG SEPARATOR ;","title":"Creating a tag field"},{"location":"Tags/#querying_tag_fields","text":"As mentioned above, just searching for a tag without any modifiers will not retrieve documents containing it. The syntax for matching tags in a query is as follows (the curly braces are part of the syntax in this case): @ field_name :{ tag | tag | ...} e.g. @tags:{hello world | foo bar} Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc. For example: FT.SEARCH idx @title:hello @price:[0 100] @tags:{ foo bar | hello world }","title":"Querying tag fields"},{"location":"Tags/#multiple_tags_in_a_single_filter","text":"Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing all tags, you should repeat the tag filter several times. For example, imagine an index of travellers, with a tag field for the cities each traveller has visited: FT.CREATE myIndex SCHEMA name TEXT cities TAG FT.ADD myIndex user1 1.0 FIELDS name John Doe cities New York, Barcelona, San Francisco For this index, the following query will return all the people who visited at least one of the following cities: FT.SEARCH myIndex @cities:{ New York | Los Angeles | Barcelona } But the next query will return all people who have visited all three cities : @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }","title":"Multiple tags in a single filter"},{"location":"Tags/#multi-word_tags_and_escaping","text":"Tags can be composed multiple words, or include other punctuation marks other than the field's separator ( , by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). NOTE: in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\-world. It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags. The following are identical: @tags:{foo\\ bar\\ baz | hello\\ world} @tags:{foo bar baz | hello world }","title":"Multi-word tags And escaping"},{"location":"Threading/","text":"Multi-Threading in RediSearch By Dvir Volk, July 2017 1. One Thread To Rule Them All Redis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with. While keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like ZUNIONSTORE , LRANGE , SINTER and of course the infamous KEYS , can block Redis for seconds or minutes, depending on the size of data they are handling. 2. RediSearch and the Single Thread Issue RediSearch is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine. While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. Think, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration. So taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow. 3. Enter the Redis GIL Luckily, Redis BDFL Salvatore Sanfilippo has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API - Thread Safe Contexts and the Global Lock . The idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the Global Lock when it needs to access Redis data, operate on it, and release it. We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy. 4. Making Search Concurrent Up until now, the flow of a search query was simple - the query would arrive at a Command Handler callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result. To allow concurrency, we adapted the following design: RediSearch has a thread pool for running concurrent search queries. When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue. The thread pool runs a query processing function in its own thread. The function locks the Redis Global lock, and starts executing the query. Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own). If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread. When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state. Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently. Figure 1: Serial vs. Concurrent Search On the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster. The same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents. As a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release. 5. The Effect of Concurrency While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running KEYS * in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one! There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\". This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation. To enable safe mode and disable query concurrency, you can configure RediSearch at load time: redis-server --loadmodule redisearch.so SAFEMODE in command line, or by adding loadmodule redisearch.so SAFEMODE to your redis.conf - depending on how you load the module. 6. Some Numbers! I've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up. Benchmark Setup The data-set consists of about 1,000,000 Reddit comments. Two clients using Redis-benchmark were running - first separately, then in parallel: One client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections. One client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries). Both clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz. The Results: Note While we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries. 7. Parting Words This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more. For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.","title":"Multi-Threading in RediSearch"},{"location":"Threading/#multi-threading_in_redisearch","text":"By Dvir Volk, July 2017","title":"Multi-Threading in RediSearch"},{"location":"Threading/#1_one_thread_to_rule_them_all","text":"Redis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with. While keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like ZUNIONSTORE , LRANGE , SINTER and of course the infamous KEYS , can block Redis for seconds or minutes, depending on the size of data they are handling.","title":"1. One Thread To Rule Them All"},{"location":"Threading/#2_redisearch_and_the_single_thread_issue","text":"RediSearch is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine. While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. Think, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration. So taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.","title":"2. RediSearch and the Single Thread Issue"},{"location":"Threading/#3_enter_the_redis_gil","text":"Luckily, Redis BDFL Salvatore Sanfilippo has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API - Thread Safe Contexts and the Global Lock . The idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the Global Lock when it needs to access Redis data, operate on it, and release it. We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.","title":"3. Enter the Redis GIL"},{"location":"Threading/#4_making_search_concurrent","text":"Up until now, the flow of a search query was simple - the query would arrive at a Command Handler callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result. To allow concurrency, we adapted the following design: RediSearch has a thread pool for running concurrent search queries. When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue. The thread pool runs a query processing function in its own thread. The function locks the Redis Global lock, and starts executing the query. Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own). If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread. When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state. Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently.","title":"4. Making Search Concurrent"},{"location":"Threading/#figure_1_serial_vs_concurrent_search","text":"On the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster. The same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents. As a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.","title":"Figure 1: Serial vs. Concurrent Search"},{"location":"Threading/#5_the_effect_of_concurrency","text":"While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running KEYS * in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one! There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\". This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation. To enable safe mode and disable query concurrency, you can configure RediSearch at load time: redis-server --loadmodule redisearch.so SAFEMODE in command line, or by adding loadmodule redisearch.so SAFEMODE to your redis.conf - depending on how you load the module.","title":"5. The Effect of Concurrency"},{"location":"Threading/#6_some_numbers","text":"I've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up. Benchmark Setup The data-set consists of about 1,000,000 Reddit comments. Two clients using Redis-benchmark were running - first separately, then in parallel: One client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections. One client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries). Both clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.","title":"6. Some Numbers!"},{"location":"Threading/#the_results","text":"Note While we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries.","title":"The Results:"},{"location":"Threading/#7_parting_words","text":"This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more. For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.","title":"7. Parting Words"},{"location":"contrib/","text":"Contributor agreement Please refer to the following page for the agreement: Redis Labs Software Grant and Contributor License Agreement","title":"Contributor agreement"},{"location":"contrib/#contributor_agreement","text":"Please refer to the following page for the agreement: Redis Labs Software Grant and Contributor License Agreement","title":"Contributor agreement"},{"location":"go_client/","text":"redisearch -- import \"github.com/RedisLabs/redisearch-go/redisearch\" Package redisearch provides a Go client for the RediSearch search engine. For the full documentation of RediSearch, see https://oss.redislabs.com/redisearch Example Usage import ( github.com/RedisLabs/redisearch-go/redisearch log fmt ) func ExampleClient () { // Create a client. By default a client is schemaless // unless a schema is provided when creating the index c := createClient ( myIndex ) // Create a schema sc := redisearch . NewSchema ( redisearch . DefaultOptions ). AddField ( redisearch . NewTextField ( body )). AddField ( redisearch . NewTextFieldOptions ( title , redisearch . TextFieldOptions { Weight : 5.0 , Sortable : true })). AddField ( redisearch . NewNumericField ( date )) // Drop an existing index. If the index does not exist an error is returned c . Drop () // Create the index with the given schema if err := c . CreateIndex ( sc ); err != nil { log . Fatal ( err ) } // Create a document with an id and given score doc := redisearch . NewDocument ( doc1 , 1.0 ) doc . Set ( title , Hello world ). Set ( body , foo bar ). Set ( date , time . Now (). Unix ()) // Index the document. The API accepts multiple documents at a time if err := c . IndexOptions ( redisearch . DefaultIndexingOptions , doc ); err != nil { log . Fatal ( err ) } // Searching with limit and sorting docs , total , err := c . Search ( redisearch . NewQuery ( hello world ). Limit ( 0 , 2 ). SetReturnFields ( title )) fmt . Println ( docs [ 0 ]. Id , docs [ 0 ]. Properties [ title ], total , err ) // Output: doc1 Hello world 1 nil } Usage var DefaultIndexingOptions = IndexingOptions { Language : , NoSave : false , Replace : false , Partial : false , } DefaultIndexingOptions are the default options for document indexing var DefaultOptions = Options { NoSave : false , NoFieldFlags : false , NoFrequencies : false , NoOffsetVectors : false , Stopwords : nil , } DefaultOptions represents the default options type Autocompleter type Autocompleter struct { } Autocompleter implements a redisearch auto-completer API func NewAutocompleter func NewAutocompleter ( addr , name string ) * Autocompleter NewAutocompleter creates a new Autocompleter with the given host and key name func (*Autocompleter) AddTerms func ( a * Autocompleter ) AddTerms ( terms ... Suggestion ) error AddTerms pushes new term suggestions to the index func (*Autocompleter) Delete func ( a * Autocompleter ) Delete () error Delete deletes the Autocompleter key for this AC func (*Autocompleter) Suggest func ( a * Autocompleter ) Suggest ( prefix string , num int , fuzzy bool ) ([] Suggestion , error ) Suggest gets completion suggestions from the Autocompleter dictionary to the given prefix. If fuzzy is set, we also complete for prefixes that are in 1 Levenshten distance from the given prefix type Client type Client struct { } Client is an interface to redisearch's redis commands func NewClient func NewClient ( addr , name string ) * Client NewClient creates a new client connecting to the redis host, and using the given name as key prefix. Addr can be a single host:port pair, or a comma separated list of host:port,host:port... In the case of multiple hosts we create a multi-pool and select connections at random func (*Client) CreateIndex func ( i * Client ) CreateIndex ( s * Schema ) error CreateIndex configues the index and creates it on redis func (*Client) Drop func ( i * Client ) Drop () error Drop the Currentl just flushes the DB - note that this will delete EVERYTHING on the redis instance func (*Client) Explain func ( i * Client ) Explain ( q * Query ) ( string , error ) Explain Return a textual string explaining the query func (*Client) Index func ( i * Client ) Index ( docs ... Document ) error Index indexes a list of documents with the default options func (*Client) IndexOptions func ( i * Client ) IndexOptions ( opts IndexingOptions , docs ... Document ) error IndexOptions indexes multiple documents on the index, with optional Options passed to options func (*Client) Info func ( i * Client ) Info () ( * IndexInfo , error ) Info - Get information about the index. This can also be used to check if the index exists func (*Client) Search func ( i * Client ) Search ( q * Query ) ( docs [] Document , total int , err error ) Search searches the index for the given query, and returns documents, the total number of results, or an error if something went wrong type ConnPool type ConnPool interface { Get () redis . Conn } type Document type Document struct { Id string Score float32 Payload [] byte Properties map [ string ] interface {} } Document represents a single document to be indexed or returned from a query. Besides a score and id, the Properties are completely arbitrary func NewDocument func NewDocument ( id string , score float32 ) Document NewDocument creates a document with the specific id and score func (*Document) EstimateSize func ( d * Document ) EstimateSize () ( sz int ) func (Document) Set func ( d Document ) Set ( name string , value interface {}) Document Set sets a property and its value in the document func (*Document) SetPayload func ( d * Document ) SetPayload ( payload [] byte ) SetPayload Sets the document payload type DocumentList type DocumentList [] Document DocumentList is used to sort documents by descending score func (DocumentList) Len func ( l DocumentList ) Len () int func (DocumentList) Less func ( l DocumentList ) Less ( i , j int ) bool func (DocumentList) Sort func ( l DocumentList ) Sort () Sort the DocumentList func (DocumentList) Swap func ( l DocumentList ) Swap ( i , j int ) type Field type Field struct { Name string Type FieldType Sortable bool Options interface {} } Field represents a single field's Schema func NewNumericField func NewNumericField ( name string ) Field NewNumericField creates a new numeric field with the given name func NewNumericFieldOptions func NewNumericFieldOptions ( name string , options NumericFieldOptions ) Field NewNumericFieldOptions defines a numeric field with additional options func NewSortableNumericField func NewSortableNumericField ( name string ) Field NewSortableNumericField creates a new numeric field with the given name and a sortable flag func NewSortableTextField func NewSortableTextField ( name string , weight float32 ) Field NewSortableTextField creates a text field with the sortable flag set func NewTagField func NewTagField ( name string ) Field NewTagField creates a new text field with default options (separator: ,) func NewTagFieldOptions func NewTagFieldOptions ( name string , opts TagFieldOptions ) Field NewTagFieldOptions creates a new tag field with the given options func NewTextField func NewTextField ( name string ) Field NewTextField creates a new text field with the given weight func NewTextFieldOptions func NewTextFieldOptions ( name string , opts TextFieldOptions ) Field NewTextFieldOptions creates a new text field with given options (weight/sortable) type FieldType type FieldType int FieldType is an enumeration of field/property types const ( // TextField full-text field TextField FieldType = iota // NumericField numeric range field NumericField // GeoField geo-indexed point field GeoField // TagField is a field used for compact indexing of comma separated values TagField ) type Flag type Flag uint64 Flag is a type for query flags const ( // Treat the terms verbatim and do not perform expansion QueryVerbatim Flag = 0x1 // Do not load any content from the documents, return just IDs QueryNoContent Flag = 0x2 // Fetch document scores as well as IDs and fields QueryWithScores Flag = 0x4 // The query terms must appear in order in the document QueryInOrder Flag = 0x08 // Fetch document payloads as well as fields. See documentation for payloads on redisearch.io QueryWithPayloads Flag = 0x10 DefaultOffset = 0 DefaultNum = 10 ) Query Flags type HighlightOptions type HighlightOptions struct { Fields [] string Tags [ 2 ] string } HighlightOptions represents the options to higlight specific document fields. See http://redisearch.io/Highlight/ type IndexInfo type IndexInfo struct { Schema Schema Name string `redis: index_name ` DocCount uint64 `redis: num_docs ` RecordCount uint64 `redis: num_records ` TermCount uint64 `redis: num_terms ` MaxDocID uint64 `redis: max_doc_id ` InvertedIndexSizeMB float64 `redis: inverted_sz_mb ` OffsetVectorSizeMB float64 `redis: offset_vector_sz_mb ` DocTableSizeMB float64 `redis: doc_table_size_mb ` KeyTableSizeMB float64 `redis: key_table_size_mb ` RecordsPerDocAvg float64 `redis: records_per_doc_avg ` BytesPerRecordAvg float64 `redis: bytes_per_record_avg ` OffsetsPerTermAvg float64 `redis: offsets_per_term_avg ` OffsetBitsPerTermAvg float64 `redis: offset_bits_per_record_avg ` } IndexInfo - Structure showing information about an existing index type IndexingOptions type IndexingOptions struct { Language string NoSave bool Replace bool Partial bool } IndexingOptions represent the options for indexing a single document type MultiError type MultiError [] error MultiError Represents one or more errors func NewMultiError func NewMultiError ( len int ) MultiError NewMultiError initializes a multierror with the given len, and all sub-errors set to nil func (MultiError) Error func ( e MultiError ) Error () string Error returns a string representation of the error, in this case it just chains all the sub errors if they are not nil type MultiHostPool type MultiHostPool struct { sync . Mutex } func NewMultiHostPool func NewMultiHostPool ( hosts [] string ) * MultiHostPool func (*MultiHostPool) Get func ( p * MultiHostPool ) Get () redis . Conn type NumericFieldOptions type NumericFieldOptions struct { Sortable bool NoIndex bool } NumericFieldOptions Options for numeric fields type Operator type Operator string const ( Eq Operator = = Gt Operator = Gte Operator = = Lt Operator = Lte Operator = = Between Operator = BETWEEN BetweenInclusive Operator = BETWEEEN_EXCLUSIVE ) type Options type Options struct { // If set, we will not save the documents contents, just index them, for fetching ids only NoSave bool NoFieldFlags bool NoFrequencies bool NoOffsetVectors bool Stopwords [] string } Options are flags passed to the the abstract Index call, which receives them as interface{}, allowing for implementation specific options type Paging type Paging struct { Offset int Num int } Paging represents the offset paging of a search result type Predicate type Predicate struct { Property string Operator Operator Value [] interface {} } func Equals func Equals ( property string , value interface {}) Predicate func GreaterThan func GreaterThan ( property string , value interface {}) Predicate func GreaterThanEquals func GreaterThanEquals ( property string , value interface {}) Predicate func InRange func InRange ( property string , min , max interface {}, inclusive bool ) Predicate func LessThan func LessThan ( property string , value interface {}) Predicate func LessThanEquals func LessThanEquals ( property string , value interface {}) Predicate func NewPredicate func NewPredicate ( property string , operator Operator , values ... interface {}) Predicate type Query type Query struct { Raw string Paging Paging Flags Flag Slop int Filters [] Predicate InKeys [] string ReturnFields [] string Language string Expander string Scorer string Payload [] byte SortBy * SortingKey HighlightOpts * HighlightOptions SummarizeOpts * SummaryOptions } Query is a single search query and all its parameters and predicates func NewQuery func NewQuery ( raw string ) * Query NewQuery creates a new query for a given index with the given search term. For currently the index parameter is ignored func (*Query) Highlight func ( q * Query ) Highlight ( fields [] string , openTag , closeTag string ) * Query Highlight sets highighting on given fields. Highlighting marks all the query terms with the given open and close tags (i.e. and for HTML) func (*Query) Limit func ( q * Query ) Limit ( offset , num int ) * Query Limit sets the paging offset and limit for the query func (*Query) SetExpander func ( q * Query ) SetExpander ( exp string ) * Query SetExpander sets a custom user query expander to be used func (*Query) SetFlags func ( q * Query ) SetFlags ( flags Flag ) * Query SetFlags sets the query's optional flags func (*Query) SetInKeys func ( q * Query ) SetInKeys ( keys ... string ) * Query SetInKeys sets the INKEYS argument of the query - limiting the search to a given set of IDs func (*Query) SetLanguage func ( q * Query ) SetLanguage ( lang string ) * Query SetLanguage sets the query language, used by the stemmer to expand the query func (*Query) SetPayload func ( q * Query ) SetPayload ( payload [] byte ) * Query SetPayload sets a binary payload to the query, that can be used by custom scoring functions func (*Query) SetReturnFields func ( q * Query ) SetReturnFields ( fields ... string ) * Query SetReturnFields sets the fields that should be returned from each result. By default we return everything func (*Query) SetScorer func ( q * Query ) SetScorer ( scorer string ) * Query SetScorer sets an alternative scoring function to be used. The only pre-compiled supported one at the moment is DISMAX func (*Query) SetSortBy func ( q * Query ) SetSortBy ( field string , ascending bool ) * Query SetSortBy sets the sorting key for the query func (*Query) Summarize func ( q * Query ) Summarize ( fields ... string ) * Query Summarize sets summarization on the given list of fields. It will instruct the engine to extract the most relevant snippets from the fields and return them as the field content. This function works with the default values of the engine, and only sets the fields. There is a function that accepts all options - SummarizeOptions func (*Query) SummarizeOptions func ( q * Query ) SummarizeOptions ( opts SummaryOptions ) * Query SummarizeOptions sets summarization on the given list of fields. It will instruct the engine to extract the most relevant snippets from the fields and return them as the field content. This function accepts advanced settings for snippet length, separators and number of snippets type Schema type Schema struct { Fields [] Field Options Options } Schema represents an index schema Schema, or how the index would treat documents sent to it. func NewSchema func NewSchema ( opts Options ) * Schema NewSchema creates a new Schema object func (*Schema) AddField func ( m * Schema ) AddField ( f Field ) * Schema AddField adds a field to the Schema object type SingleHostPool type SingleHostPool struct { * redis . Pool } func NewSingleHostPool func NewSingleHostPool ( host string ) * SingleHostPool type SortingKey type SortingKey struct { Field string Ascending bool } SortingKey represents the sorting option if the query needs to be sorted based on a sortable fields and not a ranking function. See http://redisearch.io/Sorting/ type Suggestion type Suggestion struct { Term string Score float64 Payload string } Suggestion is a single suggestion being added or received from the Autocompleter type SuggestionList type SuggestionList [] Suggestion SuggestionList is a sortable list of suggestions returned from an engine func (SuggestionList) Len func ( l SuggestionList ) Len () int func (SuggestionList) Less func ( l SuggestionList ) Less ( i , j int ) bool func (SuggestionList) Sort func ( l SuggestionList ) Sort () Sort the SuggestionList func (SuggestionList) Swap func ( l SuggestionList ) Swap ( i , j int ) type SummaryOptions type SummaryOptions struct { Fields [] string FragmentLen int // default 20 NumFragments int // default 3 Separator string // default ... } SummaryOptions represents the configuration used to create field summaries. See http://redisearch.io/Highlight/ type TagFieldOptions type TagFieldOptions struct { // Separator is the custom separator between tags. defaults to comma (,) Separator byte NoIndex bool } TagFieldOptions options for indexing tag fields type TextFieldOptions type TextFieldOptions struct { Weight float32 Sortable bool NoStem bool NoIndex bool } TextFieldOptions Options for text fields - weight and stemming enabled/disabled.","title":"Go API"},{"location":"go_client/#redisearch","text":"-- import \"github.com/RedisLabs/redisearch-go/redisearch\" Package redisearch provides a Go client for the RediSearch search engine. For the full documentation of RediSearch, see https://oss.redislabs.com/redisearch","title":"redisearch"},{"location":"go_client/#example_usage","text":"import ( github.com/RedisLabs/redisearch-go/redisearch log fmt ) func ExampleClient () { // Create a client. By default a client is schemaless // unless a schema is provided when creating the index c := createClient ( myIndex ) // Create a schema sc := redisearch . NewSchema ( redisearch . DefaultOptions ). AddField ( redisearch . NewTextField ( body )). AddField ( redisearch . NewTextFieldOptions ( title , redisearch . TextFieldOptions { Weight : 5.0 , Sortable : true })). AddField ( redisearch . NewNumericField ( date )) // Drop an existing index. If the index does not exist an error is returned c . Drop () // Create the index with the given schema if err := c . CreateIndex ( sc ); err != nil { log . Fatal ( err ) } // Create a document with an id and given score doc := redisearch . NewDocument ( doc1 , 1.0 ) doc . Set ( title , Hello world ). Set ( body , foo bar ). Set ( date , time . Now (). Unix ()) // Index the document. The API accepts multiple documents at a time if err := c . IndexOptions ( redisearch . DefaultIndexingOptions , doc ); err != nil { log . Fatal ( err ) } // Searching with limit and sorting docs , total , err := c . Search ( redisearch . NewQuery ( hello world ). Limit ( 0 , 2 ). SetReturnFields ( title )) fmt . Println ( docs [ 0 ]. Id , docs [ 0 ]. Properties [ title ], total , err ) // Output: doc1 Hello world 1 nil }","title":"Example Usage"},{"location":"go_client/#usage","text":"var DefaultIndexingOptions = IndexingOptions { Language : , NoSave : false , Replace : false , Partial : false , } DefaultIndexingOptions are the default options for document indexing var DefaultOptions = Options { NoSave : false , NoFieldFlags : false , NoFrequencies : false , NoOffsetVectors : false , Stopwords : nil , } DefaultOptions represents the default options","title":"Usage"},{"location":"go_client/#type_autocompleter","text":"type Autocompleter struct { } Autocompleter implements a redisearch auto-completer API","title":"type Autocompleter"},{"location":"go_client/#func_newautocompleter","text":"func NewAutocompleter ( addr , name string ) * Autocompleter NewAutocompleter creates a new Autocompleter with the given host and key name","title":"func  NewAutocompleter"},{"location":"go_client/#func_autocompleter_addterms","text":"func ( a * Autocompleter ) AddTerms ( terms ... Suggestion ) error AddTerms pushes new term suggestions to the index","title":"func (*Autocompleter) AddTerms"},{"location":"go_client/#func_autocompleter_delete","text":"func ( a * Autocompleter ) Delete () error Delete deletes the Autocompleter key for this AC","title":"func (*Autocompleter) Delete"},{"location":"go_client/#func_autocompleter_suggest","text":"func ( a * Autocompleter ) Suggest ( prefix string , num int , fuzzy bool ) ([] Suggestion , error ) Suggest gets completion suggestions from the Autocompleter dictionary to the given prefix. If fuzzy is set, we also complete for prefixes that are in 1 Levenshten distance from the given prefix","title":"func (*Autocompleter) Suggest"},{"location":"go_client/#type_client","text":"type Client struct { } Client is an interface to redisearch's redis commands","title":"type Client"},{"location":"go_client/#func_newclient","text":"func NewClient ( addr , name string ) * Client NewClient creates a new client connecting to the redis host, and using the given name as key prefix. Addr can be a single host:port pair, or a comma separated list of host:port,host:port... In the case of multiple hosts we create a multi-pool and select connections at random","title":"func  NewClient"},{"location":"go_client/#func_client_createindex","text":"func ( i * Client ) CreateIndex ( s * Schema ) error CreateIndex configues the index and creates it on redis","title":"func (*Client) CreateIndex"},{"location":"go_client/#func_client_drop","text":"func ( i * Client ) Drop () error Drop the Currentl just flushes the DB - note that this will delete EVERYTHING on the redis instance","title":"func (*Client) Drop"},{"location":"go_client/#func_client_explain","text":"func ( i * Client ) Explain ( q * Query ) ( string , error ) Explain Return a textual string explaining the query","title":"func (*Client) Explain"},{"location":"go_client/#func_client_index","text":"func ( i * Client ) Index ( docs ... Document ) error Index indexes a list of documents with the default options","title":"func (*Client) Index"},{"location":"go_client/#func_client_indexoptions","text":"func ( i * Client ) IndexOptions ( opts IndexingOptions , docs ... Document ) error IndexOptions indexes multiple documents on the index, with optional Options passed to options","title":"func (*Client) IndexOptions"},{"location":"go_client/#func_client_info","text":"func ( i * Client ) Info () ( * IndexInfo , error ) Info - Get information about the index. This can also be used to check if the index exists","title":"func (*Client) Info"},{"location":"go_client/#func_client_search","text":"func ( i * Client ) Search ( q * Query ) ( docs [] Document , total int , err error ) Search searches the index for the given query, and returns documents, the total number of results, or an error if something went wrong","title":"func (*Client) Search"},{"location":"go_client/#type_connpool","text":"type ConnPool interface { Get () redis . Conn }","title":"type ConnPool"},{"location":"go_client/#type_document","text":"type Document struct { Id string Score float32 Payload [] byte Properties map [ string ] interface {} } Document represents a single document to be indexed or returned from a query. Besides a score and id, the Properties are completely arbitrary","title":"type Document"},{"location":"go_client/#func_newdocument","text":"func NewDocument ( id string , score float32 ) Document NewDocument creates a document with the specific id and score","title":"func  NewDocument"},{"location":"go_client/#func_document_estimatesize","text":"func ( d * Document ) EstimateSize () ( sz int )","title":"func (*Document) EstimateSize"},{"location":"go_client/#func_document_set","text":"func ( d Document ) Set ( name string , value interface {}) Document Set sets a property and its value in the document","title":"func (Document) Set"},{"location":"go_client/#func_document_setpayload","text":"func ( d * Document ) SetPayload ( payload [] byte ) SetPayload Sets the document payload","title":"func (*Document) SetPayload"},{"location":"go_client/#type_documentlist","text":"type DocumentList [] Document DocumentList is used to sort documents by descending score","title":"type DocumentList"},{"location":"go_client/#func_documentlist_len","text":"func ( l DocumentList ) Len () int","title":"func (DocumentList) Len"},{"location":"go_client/#func_documentlist_less","text":"func ( l DocumentList ) Less ( i , j int ) bool","title":"func (DocumentList) Less"},{"location":"go_client/#func_documentlist_sort","text":"func ( l DocumentList ) Sort () Sort the DocumentList","title":"func (DocumentList) Sort"},{"location":"go_client/#func_documentlist_swap","text":"func ( l DocumentList ) Swap ( i , j int )","title":"func (DocumentList) Swap"},{"location":"go_client/#type_field","text":"type Field struct { Name string Type FieldType Sortable bool Options interface {} } Field represents a single field's Schema","title":"type Field"},{"location":"go_client/#func_newnumericfield","text":"func NewNumericField ( name string ) Field NewNumericField creates a new numeric field with the given name","title":"func  NewNumericField"},{"location":"go_client/#func_newnumericfieldoptions","text":"func NewNumericFieldOptions ( name string , options NumericFieldOptions ) Field NewNumericFieldOptions defines a numeric field with additional options","title":"func  NewNumericFieldOptions"},{"location":"go_client/#func_newsortablenumericfield","text":"func NewSortableNumericField ( name string ) Field NewSortableNumericField creates a new numeric field with the given name and a sortable flag","title":"func  NewSortableNumericField"},{"location":"go_client/#func_newsortabletextfield","text":"func NewSortableTextField ( name string , weight float32 ) Field NewSortableTextField creates a text field with the sortable flag set","title":"func  NewSortableTextField"},{"location":"go_client/#func_newtagfield","text":"func NewTagField ( name string ) Field NewTagField creates a new text field with default options (separator: ,)","title":"func  NewTagField"},{"location":"go_client/#func_newtagfieldoptions","text":"func NewTagFieldOptions ( name string , opts TagFieldOptions ) Field NewTagFieldOptions creates a new tag field with the given options","title":"func  NewTagFieldOptions"},{"location":"go_client/#func_newtextfield","text":"func NewTextField ( name string ) Field NewTextField creates a new text field with the given weight","title":"func  NewTextField"},{"location":"go_client/#func_newtextfieldoptions","text":"func NewTextFieldOptions ( name string , opts TextFieldOptions ) Field NewTextFieldOptions creates a new text field with given options (weight/sortable)","title":"func  NewTextFieldOptions"},{"location":"go_client/#type_fieldtype","text":"type FieldType int FieldType is an enumeration of field/property types const ( // TextField full-text field TextField FieldType = iota // NumericField numeric range field NumericField // GeoField geo-indexed point field GeoField // TagField is a field used for compact indexing of comma separated values TagField )","title":"type FieldType"},{"location":"go_client/#type_flag","text":"type Flag uint64 Flag is a type for query flags const ( // Treat the terms verbatim and do not perform expansion QueryVerbatim Flag = 0x1 // Do not load any content from the documents, return just IDs QueryNoContent Flag = 0x2 // Fetch document scores as well as IDs and fields QueryWithScores Flag = 0x4 // The query terms must appear in order in the document QueryInOrder Flag = 0x08 // Fetch document payloads as well as fields. See documentation for payloads on redisearch.io QueryWithPayloads Flag = 0x10 DefaultOffset = 0 DefaultNum = 10 ) Query Flags","title":"type Flag"},{"location":"go_client/#type_highlightoptions","text":"type HighlightOptions struct { Fields [] string Tags [ 2 ] string } HighlightOptions represents the options to higlight specific document fields. See http://redisearch.io/Highlight/","title":"type HighlightOptions"},{"location":"go_client/#type_indexinfo","text":"type IndexInfo struct { Schema Schema Name string `redis: index_name ` DocCount uint64 `redis: num_docs ` RecordCount uint64 `redis: num_records ` TermCount uint64 `redis: num_terms ` MaxDocID uint64 `redis: max_doc_id ` InvertedIndexSizeMB float64 `redis: inverted_sz_mb ` OffsetVectorSizeMB float64 `redis: offset_vector_sz_mb ` DocTableSizeMB float64 `redis: doc_table_size_mb ` KeyTableSizeMB float64 `redis: key_table_size_mb ` RecordsPerDocAvg float64 `redis: records_per_doc_avg ` BytesPerRecordAvg float64 `redis: bytes_per_record_avg ` OffsetsPerTermAvg float64 `redis: offsets_per_term_avg ` OffsetBitsPerTermAvg float64 `redis: offset_bits_per_record_avg ` } IndexInfo - Structure showing information about an existing index","title":"type IndexInfo"},{"location":"go_client/#type_indexingoptions","text":"type IndexingOptions struct { Language string NoSave bool Replace bool Partial bool } IndexingOptions represent the options for indexing a single document","title":"type IndexingOptions"},{"location":"go_client/#type_multierror","text":"type MultiError [] error MultiError Represents one or more errors","title":"type MultiError"},{"location":"go_client/#func_newmultierror","text":"func NewMultiError ( len int ) MultiError NewMultiError initializes a multierror with the given len, and all sub-errors set to nil","title":"func  NewMultiError"},{"location":"go_client/#func_multierror_error","text":"func ( e MultiError ) Error () string Error returns a string representation of the error, in this case it just chains all the sub errors if they are not nil","title":"func (MultiError) Error"},{"location":"go_client/#type_multihostpool","text":"type MultiHostPool struct { sync . Mutex }","title":"type MultiHostPool"},{"location":"go_client/#func_newmultihostpool","text":"func NewMultiHostPool ( hosts [] string ) * MultiHostPool","title":"func  NewMultiHostPool"},{"location":"go_client/#func_multihostpool_get","text":"func ( p * MultiHostPool ) Get () redis . Conn","title":"func (*MultiHostPool) Get"},{"location":"go_client/#type_numericfieldoptions","text":"type NumericFieldOptions struct { Sortable bool NoIndex bool } NumericFieldOptions Options for numeric fields","title":"type NumericFieldOptions"},{"location":"go_client/#type_operator","text":"type Operator string const ( Eq Operator = = Gt Operator = Gte Operator = = Lt Operator = Lte Operator = = Between Operator = BETWEEN BetweenInclusive Operator = BETWEEEN_EXCLUSIVE )","title":"type Operator"},{"location":"go_client/#type_options","text":"type Options struct { // If set, we will not save the documents contents, just index them, for fetching ids only NoSave bool NoFieldFlags bool NoFrequencies bool NoOffsetVectors bool Stopwords [] string } Options are flags passed to the the abstract Index call, which receives them as interface{}, allowing for implementation specific options","title":"type Options"},{"location":"go_client/#type_paging","text":"type Paging struct { Offset int Num int } Paging represents the offset paging of a search result","title":"type Paging"},{"location":"go_client/#type_predicate","text":"type Predicate struct { Property string Operator Operator Value [] interface {} }","title":"type Predicate"},{"location":"go_client/#func_equals","text":"func Equals ( property string , value interface {}) Predicate","title":"func  Equals"},{"location":"go_client/#func_greaterthan","text":"func GreaterThan ( property string , value interface {}) Predicate","title":"func  GreaterThan"},{"location":"go_client/#func_greaterthanequals","text":"func GreaterThanEquals ( property string , value interface {}) Predicate","title":"func  GreaterThanEquals"},{"location":"go_client/#func_inrange","text":"func InRange ( property string , min , max interface {}, inclusive bool ) Predicate","title":"func  InRange"},{"location":"go_client/#func_lessthan","text":"func LessThan ( property string , value interface {}) Predicate","title":"func  LessThan"},{"location":"go_client/#func_lessthanequals","text":"func LessThanEquals ( property string , value interface {}) Predicate","title":"func  LessThanEquals"},{"location":"go_client/#func_newpredicate","text":"func NewPredicate ( property string , operator Operator , values ... interface {}) Predicate","title":"func  NewPredicate"},{"location":"go_client/#type_query","text":"type Query struct { Raw string Paging Paging Flags Flag Slop int Filters [] Predicate InKeys [] string ReturnFields [] string Language string Expander string Scorer string Payload [] byte SortBy * SortingKey HighlightOpts * HighlightOptions SummarizeOpts * SummaryOptions } Query is a single search query and all its parameters and predicates","title":"type Query"},{"location":"go_client/#func_newquery","text":"func NewQuery ( raw string ) * Query NewQuery creates a new query for a given index with the given search term. For currently the index parameter is ignored","title":"func  NewQuery"},{"location":"go_client/#func_query_highlight","text":"func ( q * Query ) Highlight ( fields [] string , openTag , closeTag string ) * Query Highlight sets highighting on given fields. Highlighting marks all the query terms with the given open and close tags (i.e. and for HTML)","title":"func (*Query) Highlight"},{"location":"go_client/#func_query_limit","text":"func ( q * Query ) Limit ( offset , num int ) * Query Limit sets the paging offset and limit for the query","title":"func (*Query) Limit"},{"location":"go_client/#func_query_setexpander","text":"func ( q * Query ) SetExpander ( exp string ) * Query SetExpander sets a custom user query expander to be used","title":"func (*Query) SetExpander"},{"location":"go_client/#func_query_setflags","text":"func ( q * Query ) SetFlags ( flags Flag ) * Query SetFlags sets the query's optional flags","title":"func (*Query) SetFlags"},{"location":"go_client/#func_query_setinkeys","text":"func ( q * Query ) SetInKeys ( keys ... string ) * Query SetInKeys sets the INKEYS argument of the query - limiting the search to a given set of IDs","title":"func (*Query) SetInKeys"},{"location":"go_client/#func_query_setlanguage","text":"func ( q * Query ) SetLanguage ( lang string ) * Query SetLanguage sets the query language, used by the stemmer to expand the query","title":"func (*Query) SetLanguage"},{"location":"go_client/#func_query_setpayload","text":"func ( q * Query ) SetPayload ( payload [] byte ) * Query SetPayload sets a binary payload to the query, that can be used by custom scoring functions","title":"func (*Query) SetPayload"},{"location":"go_client/#func_query_setreturnfields","text":"func ( q * Query ) SetReturnFields ( fields ... string ) * Query SetReturnFields sets the fields that should be returned from each result. By default we return everything","title":"func (*Query) SetReturnFields"},{"location":"go_client/#func_query_setscorer","text":"func ( q * Query ) SetScorer ( scorer string ) * Query SetScorer sets an alternative scoring function to be used. The only pre-compiled supported one at the moment is DISMAX","title":"func (*Query) SetScorer"},{"location":"go_client/#func_query_setsortby","text":"func ( q * Query ) SetSortBy ( field string , ascending bool ) * Query SetSortBy sets the sorting key for the query","title":"func (*Query) SetSortBy"},{"location":"go_client/#func_query_summarize","text":"func ( q * Query ) Summarize ( fields ... string ) * Query Summarize sets summarization on the given list of fields. It will instruct the engine to extract the most relevant snippets from the fields and return them as the field content. This function works with the default values of the engine, and only sets the fields. There is a function that accepts all options - SummarizeOptions","title":"func (*Query) Summarize"},{"location":"go_client/#func_query_summarizeoptions","text":"func ( q * Query ) SummarizeOptions ( opts SummaryOptions ) * Query SummarizeOptions sets summarization on the given list of fields. It will instruct the engine to extract the most relevant snippets from the fields and return them as the field content. This function accepts advanced settings for snippet length, separators and number of snippets","title":"func (*Query) SummarizeOptions"},{"location":"go_client/#type_schema","text":"type Schema struct { Fields [] Field Options Options } Schema represents an index schema Schema, or how the index would treat documents sent to it.","title":"type Schema"},{"location":"go_client/#func_newschema","text":"func NewSchema ( opts Options ) * Schema NewSchema creates a new Schema object","title":"func  NewSchema"},{"location":"go_client/#func_schema_addfield","text":"func ( m * Schema ) AddField ( f Field ) * Schema AddField adds a field to the Schema object","title":"func (*Schema) AddField"},{"location":"go_client/#type_singlehostpool","text":"type SingleHostPool struct { * redis . Pool }","title":"type SingleHostPool"},{"location":"go_client/#func_newsinglehostpool","text":"func NewSingleHostPool ( host string ) * SingleHostPool","title":"func  NewSingleHostPool"},{"location":"go_client/#type_sortingkey","text":"type SortingKey struct { Field string Ascending bool } SortingKey represents the sorting option if the query needs to be sorted based on a sortable fields and not a ranking function. See http://redisearch.io/Sorting/","title":"type SortingKey"},{"location":"go_client/#type_suggestion","text":"type Suggestion struct { Term string Score float64 Payload string } Suggestion is a single suggestion being added or received from the Autocompleter","title":"type Suggestion"},{"location":"go_client/#type_suggestionlist","text":"type SuggestionList [] Suggestion SuggestionList is a sortable list of suggestions returned from an engine","title":"type SuggestionList"},{"location":"go_client/#func_suggestionlist_len","text":"func ( l SuggestionList ) Len () int","title":"func (SuggestionList) Len"},{"location":"go_client/#func_suggestionlist_less","text":"func ( l SuggestionList ) Less ( i , j int ) bool","title":"func (SuggestionList) Less"},{"location":"go_client/#func_suggestionlist_sort","text":"func ( l SuggestionList ) Sort () Sort the SuggestionList","title":"func (SuggestionList) Sort"},{"location":"go_client/#func_suggestionlist_swap","text":"func ( l SuggestionList ) Swap ( i , j int )","title":"func (SuggestionList) Swap"},{"location":"go_client/#type_summaryoptions","text":"type SummaryOptions struct { Fields [] string FragmentLen int // default 20 NumFragments int // default 3 Separator string // default ... } SummaryOptions represents the configuration used to create field summaries. See http://redisearch.io/Highlight/","title":"type SummaryOptions"},{"location":"go_client/#type_tagfieldoptions","text":"type TagFieldOptions struct { // Separator is the custom separator between tags. defaults to comma (,) Separator byte NoIndex bool } TagFieldOptions options for indexing tag fields","title":"type TagFieldOptions"},{"location":"go_client/#type_textfieldoptions","text":"type TextFieldOptions struct { Weight float32 Sortable bool NoStem bool NoIndex bool } TextFieldOptions Options for text fields - weight and stemming enabled/disabled.","title":"type TextFieldOptions"},{"location":"java_client/","text":"JRediSearch - RediSearch Java Client https://github.com/RedisLabs/JRediSearch Overview JRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis. See full documentation at https://github.com/RedisLabs/JRediSearch . Usage example Initializing the client: import io.redisearch.client.Client ; import io.redisearch.Document ; import io.redisearch.SearchResult ; import io.redisearch.Query ; import io.redisearch.Schema ; ... Client client = new Client ( testung , localhost , 6379 ); Defining a schema for an index and creating it: Schema sc = new Schema () . addTextField ( title , 5.0 ) . addTextField ( body , 1.0 ) . addNumericField ( price ); client . createIndex ( sc , Client . IndexOptions . Default ()); Adding documents to the index: Map String , Object fields = new HashMap (); fields . put ( title , hello world ); fields . put ( body , lorem ipsum ); fields . put ( price , 1337 ); client . addDocument ( doc1 , fields ); Searching the index: // Creating a complex query Query q = new Query ( hello world ) . addFilter ( new Query . NumericFilter ( price , 0 , 1000 )) . limit ( 0 , 5 ); // actual search SearchResult res = client . search ( q );","title":"Java API"},{"location":"java_client/#jredisearch_-_redisearch_java_client","text":"https://github.com/RedisLabs/JRediSearch","title":"JRediSearch - RediSearch Java Client"},{"location":"java_client/#overview","text":"JRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis. See full documentation at https://github.com/RedisLabs/JRediSearch .","title":"Overview"},{"location":"java_client/#usage_example","text":"Initializing the client: import io.redisearch.client.Client ; import io.redisearch.Document ; import io.redisearch.SearchResult ; import io.redisearch.Query ; import io.redisearch.Schema ; ... Client client = new Client ( testung , localhost , 6379 ); Defining a schema for an index and creating it: Schema sc = new Schema () . addTextField ( title , 5.0 ) . addTextField ( body , 1.0 ) . addNumericField ( price ); client . createIndex ( sc , Client . IndexOptions . Default ()); Adding documents to the index: Map String , Object fields = new HashMap (); fields . put ( title , hello world ); fields . put ( body , lorem ipsum ); fields . put ( price , 1337 ); client . addDocument ( doc1 , fields ); Searching the index: // Creating a complex query Query q = new Query ( hello world ) . addFilter ( new Query . NumericFilter ( price , 0 , 1000 )) . limit ( 0 , 5 ); // actual search SearchResult res = client . search ( q );","title":"Usage example"},{"location":"payloads/","text":"Document Payloads Usually, RediSearch stores documents as hash keys. But if you want to access some data for aggregation or scoring functions, we might want to store that data as an inline payload. This will allow us to evaluate properties of a document for scoring purposes at very low cost. Since the scoring functions already have access to the DocumentMetaData, which contains document flags and score, We can add custom payloads that can be evaluated in run-time. Payloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose of evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, if you are interested in fast evaluation, some sort of binary encoded data which is fast to decode. Adding payloads for documents When inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload. This is done with the PAYLOAD keyword: FT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}... Evaluating payloads in query time When implementing a scoring function, the signature of the function exposed is: double ( * ScoringFunction )( DocumentMetadata * dmd , IndexResult * h ); Note Currently, scoring functions cannot be dynamically added, and forking the engine and replacing them is required. DocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with arbitrary length: typedef struct { char * data , uint32_t len ; } DocumentPayload ; If no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it. It is recommended to encode some metadata about the payload inside it, like a leading version number, etc. Retrieving payloads from documents When searching, it is possible to request the document payloads from the engine. This is done by adding the keyword WITHPAYLOADS to FT.SEARCH . If WITHPAYLOADS is set, the payloads follow the document id in the returned result. If WITHSCORES is set as well, the payloads follow the scores. e.g.: 127.0.0.1:6379 FT.CREATE foo SCHEMA bar TEXT OK 127.0.0.1:6379 FT.ADD foo doc2 1.0 PAYLOAD hi there! FIELDS bar hello OK 127.0.0.1:6379 FT.SEARCH foo hello WITHPAYLOADS WITHSCORES 1) (integer) 1 2) doc2 # id 3) 1 # score 4) hi there! # payload 5) 1) bar # fields 2) hello","title":"Document Payloads"},{"location":"payloads/#document_payloads","text":"Usually, RediSearch stores documents as hash keys. But if you want to access some data for aggregation or scoring functions, we might want to store that data as an inline payload. This will allow us to evaluate properties of a document for scoring purposes at very low cost. Since the scoring functions already have access to the DocumentMetaData, which contains document flags and score, We can add custom payloads that can be evaluated in run-time. Payloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose of evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, if you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.","title":"Document Payloads"},{"location":"payloads/#adding_payloads_for_documents","text":"When inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload. This is done with the PAYLOAD keyword: FT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...","title":"Adding payloads for documents"},{"location":"payloads/#evaluating_payloads_in_query_time","text":"When implementing a scoring function, the signature of the function exposed is: double ( * ScoringFunction )( DocumentMetadata * dmd , IndexResult * h ); Note Currently, scoring functions cannot be dynamically added, and forking the engine and replacing them is required. DocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with arbitrary length: typedef struct { char * data , uint32_t len ; } DocumentPayload ; If no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it. It is recommended to encode some metadata about the payload inside it, like a leading version number, etc.","title":"Evaluating payloads in query time"},{"location":"payloads/#retrieving_payloads_from_documents","text":"When searching, it is possible to request the document payloads from the engine. This is done by adding the keyword WITHPAYLOADS to FT.SEARCH . If WITHPAYLOADS is set, the payloads follow the document id in the returned result. If WITHSCORES is set as well, the payloads follow the scores. e.g.: 127.0.0.1:6379 FT.CREATE foo SCHEMA bar TEXT OK 127.0.0.1:6379 FT.ADD foo doc2 1.0 PAYLOAD hi there! FIELDS bar hello OK 127.0.0.1:6379 FT.SEARCH foo hello WITHPAYLOADS WITHSCORES 1) (integer) 1 2) doc2 # id 3) 1 # score 4) hi there! # payload 5) 1) bar # fields 2) hello","title":"Retrieving payloads from documents"},{"location":"python_client/","text":"Package redisearch Documentation Overview redisearch-py is a python search engine library that utilizes the RediSearch Redis Module API. It is the \"official\" client of RediSearch, and should be regarded as its canonical client implementation. The source code can be found at http://github.com/RedisLabs/redisearch-py Example: Using the Python Client from redisearch import Client , TextField , NumericField , Query # Creating a client with a given index name client = Client ( myIndex ) # Creating the index definition and schema client . create_index ([ TextField ( title , weight = 5.0 ), TextField ( body )]) # Indexing a document client . add_document ( doc1 , title = RediSearch , body = Redisearch implements a search engine on top of redis ) # Simple search res = client . search ( search engine ) # the result has the total number of results, and a list of documents print res . total # 1 print res . docs [ 0 ] . title # Searching with snippets res = client . search ( search engine , snippet_sizes = { body : 50 }) # Searching with complex parameters: q = Query ( search engine ) . verbatim () . no_content () . paging ( 0 , 5 ) res = client . search ( q ) Example: Using the Auto Completer Client: # Using the auto-completer ac = AutoCompleter ( ac ) # Adding some terms ac . add_suggestions ( Suggestion ( foo , 5.0 ), Suggestion ( bar , 1.0 )) # Getting suggestions suggs = ac . get_suggestions ( goo ) # returns nothing suggs = ac . get_suggestions ( goo , fuzzy = True ) # returns [ foo ] Installing Install Redis 4.0 or above Install RediSearch Install the python client $ pip install redisearch Class AutoCompleter A client to RediSearch's AutoCompleter API It provides prefix searches with optionally fuzzy matching of prefixes __init__ def __init__ ( self , key , host = localhost , port = 6379 , conn = None ) Create a new AutoCompleter client for the given key, and optional host and port If conn is not None, we employ an already existing redis connection add_suggestions def add_suggestions ( self , * suggestions , ** kwargs ) Add suggestion terms to the AutoCompleter engine. Each suggestion has a score and string. If kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores delete def delete ( self , string ) Delete a string from the AutoCompleter index. Returns 1 if the string was found and deleted, 0 otherwise get_suggestions def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) Get a list of suggestions from the AutoCompleter, for a given prefix Parameters: prefix : the prefix we are searching. Must be valid ascii or utf-8 fuzzy : If set to true, the prefix search is done in fuzzy mode. NOTE : Running fuzzy searches on short ( 3 letters) prefixes can be very slow, and even scan the entire index. with_scores : if set to true, we also return the (refactored) score of each suggestion. This is normally not needed, and is NOT the original score inserted into the index with_payloads : Return suggestion payloads num : The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions. Returns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1. len def len ( self ) Return the number of entries in the AutoCompleter index Class Client A client for the RediSearch module. It abstracts the API of the module and lets you just use the engine __init__ def __init__ ( self , index_name , host = localhost , port = 6379 , conn = None ) Create a new Client for the given index_name, and optional host and port If conn is not None, we employ an already existing redis connection add_document def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , ** fields ) Add a single document to the index. Parameters doc_id : the id of the saved document. nosave : if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids. score : the document ranking, between 0.0 and 1.0 payload : optional inner-index payload we can save for fast access in scoring functions replace : if True, and the document already is in the index, we perform an update and reindex the document partial : if True, the fields specified will be added to the existing document. This has the added benefit that any fields specified with no_index will not be reindexed again. Implies replace fields kwargs dictionary of the document fields to be saved and/or indexed. NOTE: Geo points should be encoded as strings of \"lon,lat\" batch_indexer def batch_indexer ( self , chunk_size = 100 ) Create a new batch indexer from the client with a given chunk size create_index def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) Create the search index. Creating an existing index just updates its properties Parameters: fields : a list of TextField or NumericField objects no_term_offsets : If true, we will not save term offsets in the index no_field_flags : If true, we will not save field flags that allow searching in specific fields stopwords : If not None, we create the index with this custom stopword list. The list can be empty delete_document def delete_document ( self , doc_id , conn = None ) Delete a document from index Returns 1 if the document was deleted, 0 if not drop_index def drop_index ( self ) Drop the index if it exists explain def explain ( self , query ) info def info ( self ) Get info an stats about the the current index, including the number of documents, memory consumption, etc load_document def load_document ( self , id ) Load a single document by id search def search ( self , query ) Search the index for a given query, and return a result of documents Parameters query : the search query. Either a text for simple queries with default parameters, or a Query object for complex queries. See RediSearch's documentation on query format snippet_sizes : A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500} Class BatchIndexer A batch indexer allows you to automatically batch document indexing in pipelines, flushing it every N documents. __init__ def __init__ ( self , client , chunk_size = 1000 ) add_document def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , ** fields ) Add a document to the batch query commit def commit ( self ) Manually commit and flush the batch indexing query Class Document Represents a single document in a result set __init__ def __init__ ( self , id , payload = None , ** fields ) Class GeoField GeoField is used to define a geo-indexing field in a schema definition __init__ def __init__ ( self , name ) redis_args def redis_args ( self ) Class GeoFilter None __init__ def __init__ ( self , field , lon , lat , radius , unit = km ) Class NumericField NumericField is used to define a numeric field in a schema definition __init__ def __init__ ( self , name , sortable = False , no_index = False ) redis_args def redis_args ( self ) Class NumericFilter None __init__ def __init__ ( self , field , minval , maxval , minExclusive = False , maxExclusive = False ) Class Query Query is used to build complex queries that have more parameters than just the query string. The query string is set in the constructor, and other options have setter functions. The setter functions return the query object, so they can be chained, i.e. Query(\"foo\").verbatim().filter(...) etc. __init__ def __init__ ( self , query_string ) Create a new query object. The query string is set in the constructor, and other options have setter functions. add_filter def add_filter ( self , flt ) Add a numeric or geo filter to the query. Currently only one of each filter is supported by the engine flt : A NumericFilter or GeoFilter object, used on a corresponding field get_args def get_args ( self ) Format the redis arguments for this query and return them highlight def highlight ( self , fields = None , tags = None ) Apply specified markup to matched term(s) within the returned field(s) fields If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted tags A list of two strings to surround the match. in_order def in_order ( self ) Match only documents where the query terms appear in the same order in the document. i.e. for the query 'hello world', we do not match 'world hello' limit_fields def limit_fields ( self , * fields ) Limit the search to specific TEXT fields only fields : A list of strings, case sensitive field names from the defined schema limit_ids def limit_ids ( self , * ids ) Limit the results to a specific set of pre-known document ids of any length no_content def no_content ( self ) Set the query to only return ids and not the document content no_stopwords def no_stopwords ( self ) Prevent the query from being filtered for stopwords. Only useful in very big queries that you are certain contain no stopwords. paging def paging ( self , offset , num ) Set the paging for the query (defaults to 0..10). offset : Paging offset for the results. Defaults to 0 num : How many results do we want query_string def query_string ( self ) Return the query string of this query only return_fields def return_fields ( self , * fields ) Only return values from these fields slop def slop ( self , slop ) Allow a maximum of N intervening non matched terms between phrase terms (0 means exact phrase) sort_by def sort_by ( self , field , asc = True ) Add a sortby field to the query field - the name of the field to sort by asc - when True , sorting will be done in ascending order summarize def summarize ( self , fields = None , context_len = None , num_frags = None , sep = None ) Return an abridged format of the field, containing only the segments of the field which contain the matching term(s). If fields is specified, then only the mentioned fields are summarized; otherwise all results are summarized. Server side defaults are used for each option (except fields ) if not specified fields List of fields to summarize. All fields are summarized if not specified context_len Amount of context to include with each fragment num_frags Number of fragments per document sep Separator string to separate fragments verbatim def verbatim ( self ) Set the query to be verbatim, i.e. use no query expansion or stemming with_payloads def with_payloads ( self ) Ask the engine to return document payloads Class Result Represents the result of a search query, and has an array of Document objects __init__ def __init__ ( self , res , hascontent , duration = 0 , has_payload = False ) snippets : An optional dictionary of the form {field: snippet_size} for snippet formatting Class SortbyField None __init__ def __init__ ( self , field , asc = True ) Class Suggestion Represents a single suggestion being sent or returned from the auto complete server __init__ def __init__ ( self , string , score = 1.0 , payload = None ) Class TagField TagField is a tag-indexing field with simpler compression and tokenization. See https://oss.redislabs.com/redisearch/Tags/ __init__ def __init__ ( self , name , separator = , , no_index = False ) redis_args def redis_args ( self ) Class TextField TextField is used to define a text field in a schema definition __init__ def __init__ ( self , name , weight = 1.0 , sortable = False , no_stem = False , no_index = False ) redis_args def redis_args ( self )","title":"Python API"},{"location":"python_client/#package_redisearch_documentation","text":"","title":"Package redisearch Documentation"},{"location":"python_client/#overview","text":"redisearch-py is a python search engine library that utilizes the RediSearch Redis Module API. It is the \"official\" client of RediSearch, and should be regarded as its canonical client implementation. The source code can be found at http://github.com/RedisLabs/redisearch-py","title":"Overview"},{"location":"python_client/#example_using_the_python_client","text":"from redisearch import Client , TextField , NumericField , Query # Creating a client with a given index name client = Client ( myIndex ) # Creating the index definition and schema client . create_index ([ TextField ( title , weight = 5.0 ), TextField ( body )]) # Indexing a document client . add_document ( doc1 , title = RediSearch , body = Redisearch implements a search engine on top of redis ) # Simple search res = client . search ( search engine ) # the result has the total number of results, and a list of documents print res . total # 1 print res . docs [ 0 ] . title # Searching with snippets res = client . search ( search engine , snippet_sizes = { body : 50 }) # Searching with complex parameters: q = Query ( search engine ) . verbatim () . no_content () . paging ( 0 , 5 ) res = client . search ( q )","title":"Example: Using the Python Client"},{"location":"python_client/#example_using_the_auto_completer_client","text":"# Using the auto-completer ac = AutoCompleter ( ac ) # Adding some terms ac . add_suggestions ( Suggestion ( foo , 5.0 ), Suggestion ( bar , 1.0 )) # Getting suggestions suggs = ac . get_suggestions ( goo ) # returns nothing suggs = ac . get_suggestions ( goo , fuzzy = True ) # returns [ foo ]","title":"Example: Using the Auto Completer Client:"},{"location":"python_client/#installing","text":"Install Redis 4.0 or above Install RediSearch Install the python client $ pip install redisearch","title":"Installing"},{"location":"python_client/#class_autocompleter","text":"A client to RediSearch's AutoCompleter API It provides prefix searches with optionally fuzzy matching of prefixes","title":"Class AutoCompleter"},{"location":"python_client/#9595init9595","text":"def __init__ ( self , key , host = localhost , port = 6379 , conn = None ) Create a new AutoCompleter client for the given key, and optional host and port If conn is not None, we employ an already existing redis connection","title":"__init__"},{"location":"python_client/#add95suggestions","text":"def add_suggestions ( self , * suggestions , ** kwargs ) Add suggestion terms to the AutoCompleter engine. Each suggestion has a score and string. If kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores","title":"add_suggestions"},{"location":"python_client/#delete","text":"def delete ( self , string ) Delete a string from the AutoCompleter index. Returns 1 if the string was found and deleted, 0 otherwise","title":"delete"},{"location":"python_client/#get95suggestions","text":"def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) Get a list of suggestions from the AutoCompleter, for a given prefix","title":"get_suggestions"},{"location":"python_client/#parameters","text":"prefix : the prefix we are searching. Must be valid ascii or utf-8 fuzzy : If set to true, the prefix search is done in fuzzy mode. NOTE : Running fuzzy searches on short ( 3 letters) prefixes can be very slow, and even scan the entire index. with_scores : if set to true, we also return the (refactored) score of each suggestion. This is normally not needed, and is NOT the original score inserted into the index with_payloads : Return suggestion payloads num : The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions. Returns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.","title":"Parameters:"},{"location":"python_client/#len","text":"def len ( self ) Return the number of entries in the AutoCompleter index","title":"len"},{"location":"python_client/#class_client","text":"A client for the RediSearch module. It abstracts the API of the module and lets you just use the engine","title":"Class Client"},{"location":"python_client/#9595init9595_1","text":"def __init__ ( self , index_name , host = localhost , port = 6379 , conn = None ) Create a new Client for the given index_name, and optional host and port If conn is not None, we employ an already existing redis connection","title":"__init__"},{"location":"python_client/#add95document","text":"def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , ** fields ) Add a single document to the index.","title":"add_document"},{"location":"python_client/#parameters_1","text":"doc_id : the id of the saved document. nosave : if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids. score : the document ranking, between 0.0 and 1.0 payload : optional inner-index payload we can save for fast access in scoring functions replace : if True, and the document already is in the index, we perform an update and reindex the document partial : if True, the fields specified will be added to the existing document. This has the added benefit that any fields specified with no_index will not be reindexed again. Implies replace fields kwargs dictionary of the document fields to be saved and/or indexed. NOTE: Geo points should be encoded as strings of \"lon,lat\"","title":"Parameters"},{"location":"python_client/#batch95indexer","text":"def batch_indexer ( self , chunk_size = 100 ) Create a new batch indexer from the client with a given chunk size","title":"batch_indexer"},{"location":"python_client/#create95index","text":"def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) Create the search index. Creating an existing index just updates its properties","title":"create_index"},{"location":"python_client/#parameters_2","text":"fields : a list of TextField or NumericField objects no_term_offsets : If true, we will not save term offsets in the index no_field_flags : If true, we will not save field flags that allow searching in specific fields stopwords : If not None, we create the index with this custom stopword list. The list can be empty","title":"Parameters:"},{"location":"python_client/#delete95document","text":"def delete_document ( self , doc_id , conn = None ) Delete a document from index Returns 1 if the document was deleted, 0 if not","title":"delete_document"},{"location":"python_client/#drop95index","text":"def drop_index ( self ) Drop the index if it exists","title":"drop_index"},{"location":"python_client/#explain","text":"def explain ( self , query )","title":"explain"},{"location":"python_client/#info","text":"def info ( self ) Get info an stats about the the current index, including the number of documents, memory consumption, etc","title":"info"},{"location":"python_client/#load95document","text":"def load_document ( self , id ) Load a single document by id","title":"load_document"},{"location":"python_client/#search","text":"def search ( self , query ) Search the index for a given query, and return a result of documents","title":"search"},{"location":"python_client/#parameters_3","text":"query : the search query. Either a text for simple queries with default parameters, or a Query object for complex queries. See RediSearch's documentation on query format snippet_sizes : A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}","title":"Parameters"},{"location":"python_client/#class_batchindexer","text":"A batch indexer allows you to automatically batch document indexing in pipelines, flushing it every N documents.","title":"Class BatchIndexer"},{"location":"python_client/#9595init9595_2","text":"def __init__ ( self , client , chunk_size = 1000 )","title":"__init__"},{"location":"python_client/#add95document_1","text":"def add_document ( self , doc_id , nosave = False , score = 1.0 , payload = None , replace = False , partial = False , ** fields ) Add a document to the batch query","title":"add_document"},{"location":"python_client/#commit","text":"def commit ( self ) Manually commit and flush the batch indexing query","title":"commit"},{"location":"python_client/#class_document","text":"Represents a single document in a result set","title":"Class Document"},{"location":"python_client/#9595init9595_3","text":"def __init__ ( self , id , payload = None , ** fields )","title":"__init__"},{"location":"python_client/#class_geofield","text":"GeoField is used to define a geo-indexing field in a schema definition","title":"Class GeoField"},{"location":"python_client/#9595init9595_4","text":"def __init__ ( self , name )","title":"__init__"},{"location":"python_client/#redis95args","text":"def redis_args ( self )","title":"redis_args"},{"location":"python_client/#class_geofilter","text":"None","title":"Class GeoFilter"},{"location":"python_client/#9595init9595_5","text":"def __init__ ( self , field , lon , lat , radius , unit = km )","title":"__init__"},{"location":"python_client/#class_numericfield","text":"NumericField is used to define a numeric field in a schema definition","title":"Class NumericField"},{"location":"python_client/#9595init9595_6","text":"def __init__ ( self , name , sortable = False , no_index = False )","title":"__init__"},{"location":"python_client/#redis95args_1","text":"def redis_args ( self )","title":"redis_args"},{"location":"python_client/#class_numericfilter","text":"None","title":"Class NumericFilter"},{"location":"python_client/#9595init9595_7","text":"def __init__ ( self , field , minval , maxval , minExclusive = False , maxExclusive = False )","title":"__init__"},{"location":"python_client/#class_query","text":"Query is used to build complex queries that have more parameters than just the query string. The query string is set in the constructor, and other options have setter functions. The setter functions return the query object, so they can be chained, i.e. Query(\"foo\").verbatim().filter(...) etc.","title":"Class Query"},{"location":"python_client/#9595init9595_8","text":"def __init__ ( self , query_string ) Create a new query object. The query string is set in the constructor, and other options have setter functions.","title":"__init__"},{"location":"python_client/#add95filter","text":"def add_filter ( self , flt ) Add a numeric or geo filter to the query. Currently only one of each filter is supported by the engine flt : A NumericFilter or GeoFilter object, used on a corresponding field","title":"add_filter"},{"location":"python_client/#get95args","text":"def get_args ( self ) Format the redis arguments for this query and return them","title":"get_args"},{"location":"python_client/#highlight","text":"def highlight ( self , fields = None , tags = None ) Apply specified markup to matched term(s) within the returned field(s) fields If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted tags A list of two strings to surround the match.","title":"highlight"},{"location":"python_client/#in95order","text":"def in_order ( self ) Match only documents where the query terms appear in the same order in the document. i.e. for the query 'hello world', we do not match 'world hello'","title":"in_order"},{"location":"python_client/#limit95fields","text":"def limit_fields ( self , * fields ) Limit the search to specific TEXT fields only fields : A list of strings, case sensitive field names from the defined schema","title":"limit_fields"},{"location":"python_client/#limit95ids","text":"def limit_ids ( self , * ids ) Limit the results to a specific set of pre-known document ids of any length","title":"limit_ids"},{"location":"python_client/#no95content","text":"def no_content ( self ) Set the query to only return ids and not the document content","title":"no_content"},{"location":"python_client/#no95stopwords","text":"def no_stopwords ( self ) Prevent the query from being filtered for stopwords. Only useful in very big queries that you are certain contain no stopwords.","title":"no_stopwords"},{"location":"python_client/#paging","text":"def paging ( self , offset , num ) Set the paging for the query (defaults to 0..10). offset : Paging offset for the results. Defaults to 0 num : How many results do we want","title":"paging"},{"location":"python_client/#query95string","text":"def query_string ( self ) Return the query string of this query only","title":"query_string"},{"location":"python_client/#return95fields","text":"def return_fields ( self , * fields ) Only return values from these fields","title":"return_fields"},{"location":"python_client/#slop","text":"def slop ( self , slop ) Allow a maximum of N intervening non matched terms between phrase terms (0 means exact phrase)","title":"slop"},{"location":"python_client/#sort95by","text":"def sort_by ( self , field , asc = True ) Add a sortby field to the query field - the name of the field to sort by asc - when True , sorting will be done in ascending order","title":"sort_by"},{"location":"python_client/#summarize","text":"def summarize ( self , fields = None , context_len = None , num_frags = None , sep = None ) Return an abridged format of the field, containing only the segments of the field which contain the matching term(s). If fields is specified, then only the mentioned fields are summarized; otherwise all results are summarized. Server side defaults are used for each option (except fields ) if not specified fields List of fields to summarize. All fields are summarized if not specified context_len Amount of context to include with each fragment num_frags Number of fragments per document sep Separator string to separate fragments","title":"summarize"},{"location":"python_client/#verbatim","text":"def verbatim ( self ) Set the query to be verbatim, i.e. use no query expansion or stemming","title":"verbatim"},{"location":"python_client/#with95payloads","text":"def with_payloads ( self ) Ask the engine to return document payloads","title":"with_payloads"},{"location":"python_client/#class_result","text":"Represents the result of a search query, and has an array of Document objects","title":"Class Result"},{"location":"python_client/#9595init9595_9","text":"def __init__ ( self , res , hascontent , duration = 0 , has_payload = False ) snippets : An optional dictionary of the form {field: snippet_size} for snippet formatting","title":"__init__"},{"location":"python_client/#class_sortbyfield","text":"None","title":"Class SortbyField"},{"location":"python_client/#9595init9595_10","text":"def __init__ ( self , field , asc = True )","title":"__init__"},{"location":"python_client/#class_suggestion","text":"Represents a single suggestion being sent or returned from the auto complete server","title":"Class Suggestion"},{"location":"python_client/#9595init9595_11","text":"def __init__ ( self , string , score = 1.0 , payload = None )","title":"__init__"},{"location":"python_client/#class_tagfield","text":"TagField is a tag-indexing field with simpler compression and tokenization. See https://oss.redislabs.com/redisearch/Tags/","title":"Class TagField"},{"location":"python_client/#9595init9595_12","text":"def __init__ ( self , name , separator = , , no_index = False )","title":"__init__"},{"location":"python_client/#redis95args_2","text":"def redis_args ( self )","title":"redis_args"},{"location":"python_client/#class_textfield","text":"TextField is used to define a text field in a schema definition","title":"Class TextField"},{"location":"python_client/#9595init9595_13","text":"def __init__ ( self , name , weight = 1.0 , sortable = False , no_stem = False , no_index = False )","title":"__init__"},{"location":"python_client/#redis95args_3","text":"def redis_args ( self )","title":"redis_args"},{"location":"design/gc/","text":"Garbage Collection in RediSearch 1. The Need For GC Deleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast. This means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion. Thus all inverted index entries belonging to this document id are just garbage. We do not want to go and explicitly delete them when deleting a document because it will make this operation very long and depending on the length of the document. On top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast. All of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory. Thus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect. 2. Garbage Collecting a Single Term Index A single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage. The algorithm is pretty simple: Create a reader and writer for each block Read each block's records one by one If no record is invalid, do nothing Once we found a garbage record, we advance the reader but not the writer. Once we found at least one garbage record, we encode the next records to the writer, recalculating the deltas. Pseudo code: foreach index_block as block: reader = new_reader(block) writer = new_write(block) garbage = 0 while not reader.end(): record = reader.decode_next() if record.is_valid(): if garbage != 0: # Write the record at the writer s tip with a newly calculated delta writer.write_record(record) else: writer.advance(record.length) else: garbage += record.length NOTE : Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do. 2.1 Garbage Collection on Numeric Indexes Numeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree. 3. GC And Concurrency Since RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us). It GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing. This means, however, that we need to consider a few things: From the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to. From the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep. To solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple: Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed. Before starting an index iterator, we copy the index's gc marker to the iterator's context. After waking up from sleep in the iterator, we check the gc markers in both objects. If they are the same we can simply trust the byte offset of the reader in the current block. * IF not, we seek the reader to the previously read docId, which is slower. To solve 2 is simpler: The GC will of course operate only while the GIL is locked. The GC will never yield execution while in the middle of a block. The GC will check whether the key has been deleted while it slept. The GC will get a new pointer to the next block on each read, assuring the pointer is safe. 4. Scheduling Garbage Collection While the GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. The problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively. So the GC will use sampling of random terms and collect them. This leaves two problems: 1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little). 2. How to make sure we hit terms that are more likely to contain garbage. Solving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation. Thus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited. Solving 1 can be done in the following way: We start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured. Then, we do the following: * Each time a document is deleted or updated we increase the frequency a bit. * Each time we find a key with garbage we increase the frequency a bit. * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage. The frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.","title":"Garbage Collection"},{"location":"design/gc/#garbage_collection_in_redisearch","text":"","title":"Garbage Collection in RediSearch"},{"location":"design/gc/#1_the_need_for_gc","text":"Deleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast. This means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion. Thus all inverted index entries belonging to this document id are just garbage. We do not want to go and explicitly delete them when deleting a document because it will make this operation very long and depending on the length of the document. On top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast. All of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory. Thus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.","title":"1. The Need For GC"},{"location":"design/gc/#2_garbage_collecting_a_single_term_index","text":"A single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage. The algorithm is pretty simple: Create a reader and writer for each block Read each block's records one by one If no record is invalid, do nothing Once we found a garbage record, we advance the reader but not the writer. Once we found at least one garbage record, we encode the next records to the writer, recalculating the deltas. Pseudo code: foreach index_block as block: reader = new_reader(block) writer = new_write(block) garbage = 0 while not reader.end(): record = reader.decode_next() if record.is_valid(): if garbage != 0: # Write the record at the writer s tip with a newly calculated delta writer.write_record(record) else: writer.advance(record.length) else: garbage += record.length NOTE : Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.","title":"2. Garbage Collecting a Single Term Index"},{"location":"design/gc/#21_garbage_collection_on_numeric_indexes","text":"Numeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.","title":"2.1 Garbage Collection on Numeric Indexes"},{"location":"design/gc/#3_gc_and_concurrency","text":"Since RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us). It GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing. This means, however, that we need to consider a few things: From the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to. From the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep. To solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple: Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed. Before starting an index iterator, we copy the index's gc marker to the iterator's context. After waking up from sleep in the iterator, we check the gc markers in both objects. If they are the same we can simply trust the byte offset of the reader in the current block. * IF not, we seek the reader to the previously read docId, which is slower. To solve 2 is simpler: The GC will of course operate only while the GIL is locked. The GC will never yield execution while in the middle of a block. The GC will check whether the key has been deleted while it slept. The GC will get a new pointer to the next block on each read, assuring the pointer is safe.","title":"3. GC And Concurrency"},{"location":"design/gc/#4_scheduling_garbage_collection","text":"While the GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. The problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively. So the GC will use sampling of random terms and collect them. This leaves two problems: 1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little). 2. How to make sure we hit terms that are more likely to contain garbage. Solving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation. Thus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited. Solving 1 can be done in the following way: We start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured. Then, we do the following: * Each time a document is deleted or updated we increase the frequency a bit. * Each time we find a key with garbage we increase the frequency a bit. * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage. The frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.","title":"4. Scheduling Garbage Collection"},{"location":"design/indexing/","text":"Document Indexing This document describes how documents are added to the index. Components Document - this contains the actual document and its fields. RSAddDocumentCtx - this is the per-document state that is used while it is being indexed. The state is discarded once complete ForwardIndex - contains terms found in the document. The forward index is used to write the InvertedIndex (later on) InvertedIndex - an index mapping a term to occurrences within applicable documents. Architecture The indexing process begins by creating a new RSAddDocumentCtx and adding a document to it. Internally this is divided into several steps. Submission. A DocumentContext is created, and is associated with a document (as received) from input. The submission process will also perform some preliminary caching. Preprocessing Once a document has been submitted, it is preprocessed. Preprocessing performs stateless processing on all document input fields. For text fields, this means tokenizing the document and creating a forward index. The preprocesors will store this information in per-field variables within the AddDocumentCtx . This computed result is then written to the (persistent) index later on during the indexing phase. If the document is sufficiently large, the preprocessing is done in a separate thread, which allows concurrent preprocessing and also avoids blocking other threads. If the document is smaller, the preprocessing is done within the main thread, avoiding the overhead of additional context switching. The SELF_EXC_THRESHOLD (macro) contains the threshold for 'sufficiently large'. Once the document is preprocessed, it is submitted to be indexed. Indexing Indexing proper consists of writing down the precomputed results of the preprocessing phase above. It is done in a single thread, and is in the form of a queue. Because documents must be written to the index in the exact order of their document ID assignment, and because we must also yield to other potential indexing processes, we may end up in a situation where document IDs are written to the index out-of-order. In order to solve that, the order in which documents are actually written must be well-defined. If there is only one thread writing documents, then this thread will not need to worry about out-of-order IDs while writing. Having a single background thread also helps optimize in several areas, as will be seen later on. The basic idea is that when there are a lot of documents queued for the indexing thread, the indexing thread may treat them as batch commands, greatly reducing the number of locks/unlocks of the GIL and the number of times term keys need to be opened and closed. Skipping already indexed documents The phases below may operate on more than one document at a time. When a document is fully indexed, it is marked as done. When the thread iterates over the queue it will only perform processing/indexing on items not yet marked as done. Term Merging Term merging, or forward index merging, is done when there is more than a single document in the queue. The forward index of each document in the queue is scanned, and a larger, 'master' forward index is constructed in its place. Each entry in the forward index contains a reference to the origin document as well as the normal offset/score/frequency information. Creating a 'master' forward index avoids opening common term keys once per document. If there is only one document within the queue, a 'master' forward index is not created. Note that the internal type of the master forward index is not actually ForwardIndex . Document ID assignment At this point, the GIL is locked and every document in the queue is assigned a document ID. The assignment is done immediately before writing to the index so as to reduce the number of times the GIL is locked; thus, the GIL is locked only once - right before the index is written. Writing to Indexes With the GIL being locked, any pending index data is written to the indexes. This usually involves opening one or more Redis keys, and writing/copying computed data into those keys. Once this is done, the reply for the given document is sent, and the AddDocumentCtx freed.","title":"Document Indexing"},{"location":"design/indexing/#document_indexing","text":"This document describes how documents are added to the index.","title":"Document Indexing"},{"location":"design/indexing/#components","text":"Document - this contains the actual document and its fields. RSAddDocumentCtx - this is the per-document state that is used while it is being indexed. The state is discarded once complete ForwardIndex - contains terms found in the document. The forward index is used to write the InvertedIndex (later on) InvertedIndex - an index mapping a term to occurrences within applicable documents.","title":"Components"},{"location":"design/indexing/#architecture","text":"The indexing process begins by creating a new RSAddDocumentCtx and adding a document to it. Internally this is divided into several steps. Submission. A DocumentContext is created, and is associated with a document (as received) from input. The submission process will also perform some preliminary caching. Preprocessing Once a document has been submitted, it is preprocessed. Preprocessing performs stateless processing on all document input fields. For text fields, this means tokenizing the document and creating a forward index. The preprocesors will store this information in per-field variables within the AddDocumentCtx . This computed result is then written to the (persistent) index later on during the indexing phase. If the document is sufficiently large, the preprocessing is done in a separate thread, which allows concurrent preprocessing and also avoids blocking other threads. If the document is smaller, the preprocessing is done within the main thread, avoiding the overhead of additional context switching. The SELF_EXC_THRESHOLD (macro) contains the threshold for 'sufficiently large'. Once the document is preprocessed, it is submitted to be indexed. Indexing Indexing proper consists of writing down the precomputed results of the preprocessing phase above. It is done in a single thread, and is in the form of a queue. Because documents must be written to the index in the exact order of their document ID assignment, and because we must also yield to other potential indexing processes, we may end up in a situation where document IDs are written to the index out-of-order. In order to solve that, the order in which documents are actually written must be well-defined. If there is only one thread writing documents, then this thread will not need to worry about out-of-order IDs while writing. Having a single background thread also helps optimize in several areas, as will be seen later on. The basic idea is that when there are a lot of documents queued for the indexing thread, the indexing thread may treat them as batch commands, greatly reducing the number of locks/unlocks of the GIL and the number of times term keys need to be opened and closed. Skipping already indexed documents The phases below may operate on more than one document at a time. When a document is fully indexed, it is marked as done. When the thread iterates over the queue it will only perform processing/indexing on items not yet marked as done. Term Merging Term merging, or forward index merging, is done when there is more than a single document in the queue. The forward index of each document in the queue is scanned, and a larger, 'master' forward index is constructed in its place. Each entry in the forward index contains a reference to the origin document as well as the normal offset/score/frequency information. Creating a 'master' forward index avoids opening common term keys once per document. If there is only one document within the queue, a 'master' forward index is not created. Note that the internal type of the master forward index is not actually ForwardIndex . Document ID assignment At this point, the GIL is locked and every document in the queue is assigned a document ID. The assignment is done immediately before writing to the index so as to reduce the number of times the GIL is locked; thus, the GIL is locked only once - right before the index is written. Writing to Indexes With the GIL being locked, any pending index data is written to the indexes. This usually involves opening one or more Redis keys, and writing/copying computed data into those keys. Once this is done, the reply for the given document is sent, and the AddDocumentCtx freed.","title":"Architecture"}]}