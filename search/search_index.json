{
    "docs": [
        {
            "location": "/", 
            "text": "RediSearch - Redis Powered Search Engine\n\n\nRediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by \nRedis Labs\n. \n\n\n\n\nQuick Links:\n\n\n\n\nSource Code at GitHub\n.\n\n\nLatest Release: 1.4.0\n\n\nDocker Image: redislabs/redisearch\n\n\nQuick Start Guide\n\n\nMailing list / Forum\n\n\n\n\n\n\n\n\nSupported Platforms\n\n\nRediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.\n\n\ni386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently. \n\n\n\n\nOverview\n\n\nRedisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.\n\n\nThis also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional Redis search approaches.\n\n\nClient Libraries\n\n\nOfficial and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee \nClients Page\n\n\nCluster Support and Commercial Version\n\n\nRediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial support for RediSearch. See the \nRedis Labs Website\n for more info and contact information. \n\n\nPrimary Features\n\n\n\n\nFull-Text indexing of multiple fields in documents.\n\n\nIncremental indexing without performance loss.\n\n\nDocument ranking (provided manually by the user at index time).\n\n\nComplex boolean queries with AND, OR, NOT operators between sub-queries.\n\n\nOptional query clauses.\n\n\nPrefix based searches.\n\n\nField weights.\n\n\nAuto-complete suggestions (with fuzzy prefix suggestions).\n\n\nExact Phrase Search, Slop based search.\n\n\nStemming based query expansion in \nmany languages\n (using \nSnowball\n).\n\n\nSupport for custom functions for query expansion and scoring (see \nExtensions\n).\n\n\nLimiting searches to specific document fields.\n\n\nNumeric filters and ranges.\n\n\nGeo filtering using Redis' own Geo-commands. \n\n\nUnicode support (UTF-8 input required).\n\n\nRetrieve full document content or just ids\n\n\nDocument deletion and updating with index garbage collection.\n\n\nPartial and conditional document updates.", 
            "title": "Home"
        }, 
        {
            "location": "/#redisearch\"_\"-\"_\"redis\"_\"powered\"_\"search\"_\"engine", 
            "text": "RediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by  Redis Labs .    Quick Links:   Source Code at GitHub .  Latest Release: 1.4.0  Docker Image: redislabs/redisearch  Quick Start Guide  Mailing list / Forum     Supported Platforms  RediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.  i386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently.", 
            "title": "RediSearch - Redis Powered Search Engine"
        }, 
        {
            "location": "/#overview", 
            "text": "Redisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.  This also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional Redis search approaches.", 
            "title": "Overview"
        }, 
        {
            "location": "/#client\"_\"libraries", 
            "text": "Official and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee  Clients Page", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/#cluster\"_\"support\"_\"and\"_\"commercial\"_\"version", 
            "text": "RediSearch has a distributed cluster version that can scale to billions of documents and hundreds of servers. However, it is only available as part of Redis Labs Enterprise. We also offer official commercial support for RediSearch. See the  Redis Labs Website  for more info and contact information.", 
            "title": "Cluster Support and Commercial Version"
        }, 
        {
            "location": "/#primary\"_\"features", 
            "text": "Full-Text indexing of multiple fields in documents.  Incremental indexing without performance loss.  Document ranking (provided manually by the user at index time).  Complex boolean queries with AND, OR, NOT operators between sub-queries.  Optional query clauses.  Prefix based searches.  Field weights.  Auto-complete suggestions (with fuzzy prefix suggestions).  Exact Phrase Search, Slop based search.  Stemming based query expansion in  many languages  (using  Snowball ).  Support for custom functions for query expansion and scoring (see  Extensions ).  Limiting searches to specific document fields.  Numeric filters and ranges.  Geo filtering using Redis' own Geo-commands.   Unicode support (UTF-8 input required).  Retrieve full document content or just ids  Document deletion and updating with index garbage collection.  Partial and conditional document updates.", 
            "title": "Primary Features"
        }, 
        {
            "location": "/Quick_Start/", 
            "text": "Quick Start Guide for RediSearch\n\n\nRunning with Docker\n\n\ndocker run -p \n6379\n:6379 redislabs/redisearch:latest\n\n\n\n\n\nBuilding and running from source\n\n\nRediSearch uses \nCMake\n as its build system. CMake is\navailable for almost every available platform. You can obtain cmake through\nyour operating system's package manager. RediSearch requires CMake version\n3 or greater. If your package repository does not contain CMake3, you can\ndownload a precompiled binary from \nCMake downloads\n.\n\n\nTo build using CMake:\n\n\ngit clone https://github.com/RedisLabsModules/RediSearch.git\n\ncd\n RediSearch\nmkdir build\n\ncd\n build\ncmake .. -DCMAKE_BUILD_TYPE\n=\nRelWithDebInfo\nmake\n\nredis-server --loadmodule ./redisearch.so\n\n\n\n\n\nThe resulting module will be in the current directory.\n\n\nYou can also simply type \nmake\n from the top level directory, this will\ntake care of running \ncmake\n with the appropriate arguments, and provide you\nwith a \nredisearch.so\n file in the \nsrc\n directory:\n\n\ngit clone https://github.com/RedisLabsModules/RediSearch.git\n\ncd\n RediSearch\nmake\nredis-server --loadmodule ./src/redisearch.so\n\n\n\n\n\nCreating an index with fields and weights (default weight is 1.0)\n\n\n127.0.0.1:6379\n FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK \n\n\n\n\n\nAdding documents to the index\n\n\n127.0.0.1:6379\n FT.ADD myIdx doc1 1.0 FIELDS title \nhello world\n body \nlorem ipsum\n url \nhttp://redis.io\n \nOK\n\n\n\n\n\nSearching the index\n\n\n127.0.0.1:6379\n FT.SEARCH myIdx \nhello world\n LIMIT 0 10\n1) (integer) 1\n2) \ndoc1\n\n3) 1) \ntitle\n\n   2) \nhello world\n\n   3) \nbody\n\n   4) \nlorem ipsum\n\n   5) \nurl\n\n   6) \nhttp://redis.io\n\n\n\n\n\n\n\n\nNote\n\n\nInput is expected to be valid utf-8 or ASCII. The engine cannot handle wide character unicode at the moment. \n\n\n\n\nDropping the index\n\n\n127.0.0.1:6379\n FT.DROP myIdx\nOK\n\n\n\n\n\nAdding and getting Auto-complete suggestions\n\n\n127.0.0.1:6379\n FT.SUGADD autocomplete \nhello world\n 100\nOK\n\n127.0.0.1:6379\n FT.SUGGET autocomplete \nhe\n\n1) \nhello world", 
            "title": "Quick Start"
        }, 
        {
            "location": "/Quick_Start/#quick\"_\"start\"_\"guide\"_\"for\"_\"redisearch", 
            "text": "", 
            "title": "Quick Start Guide for RediSearch"
        }, 
        {
            "location": "/Quick_Start/#running\"_\"with\"_\"docker", 
            "text": "docker run -p  6379 :6379 redislabs/redisearch:latest", 
            "title": "Running with Docker"
        }, 
        {
            "location": "/Quick_Start/#building\"_\"and\"_\"running\"_\"from\"_\"source", 
            "text": "RediSearch uses  CMake  as its build system. CMake is\navailable for almost every available platform. You can obtain cmake through\nyour operating system's package manager. RediSearch requires CMake version\n3 or greater. If your package repository does not contain CMake3, you can\ndownload a precompiled binary from  CMake downloads .  To build using CMake:  git clone https://github.com/RedisLabsModules/RediSearch.git cd  RediSearch\nmkdir build cd  build\ncmake .. -DCMAKE_BUILD_TYPE = RelWithDebInfo\nmake\n\nredis-server --loadmodule ./redisearch.so  The resulting module will be in the current directory.  You can also simply type  make  from the top level directory, this will\ntake care of running  cmake  with the appropriate arguments, and provide you\nwith a  redisearch.so  file in the  src  directory:  git clone https://github.com/RedisLabsModules/RediSearch.git cd  RediSearch\nmake\nredis-server --loadmodule ./src/redisearch.so", 
            "title": "Building and running from source"
        }, 
        {
            "location": "/Quick_Start/#creating\"_\"an\"_\"index\"_\"with\"_\"fields\"_\"and\"_\"weights\"_\"default\"_\"weight\"_\"is\"_\"10", 
            "text": "127.0.0.1:6379  FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK", 
            "title": "Creating an index with fields and weights (default weight is 1.0)"
        }, 
        {
            "location": "/Quick_Start/#adding\"_\"documents\"_\"to\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.ADD myIdx doc1 1.0 FIELDS title  hello world  body  lorem ipsum  url  http://redis.io  \nOK", 
            "title": "Adding documents to the index"
        }, 
        {
            "location": "/Quick_Start/#searching\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.SEARCH myIdx  hello world  LIMIT 0 10\n1) (integer) 1\n2)  doc1 \n3) 1)  title \n   2)  hello world \n   3)  body \n   4)  lorem ipsum \n   5)  url \n   6)  http://redis.io    Note  Input is expected to be valid utf-8 or ASCII. The engine cannot handle wide character unicode at the moment.", 
            "title": "Searching the index"
        }, 
        {
            "location": "/Quick_Start/#dropping\"_\"the\"_\"index", 
            "text": "127.0.0.1:6379  FT.DROP myIdx\nOK", 
            "title": "Dropping the index"
        }, 
        {
            "location": "/Quick_Start/#adding\"_\"and\"_\"getting\"_\"auto-complete\"_\"suggestions", 
            "text": "127.0.0.1:6379  FT.SUGADD autocomplete  hello world  100\nOK\n\n127.0.0.1:6379  FT.SUGGET autocomplete  he \n1)  hello world", 
            "title": "Adding and getting Auto-complete suggestions"
        }, 
        {
            "location": "/Commands/", 
            "text": "RediSearch Full Command Documentation\n\n\nFT.CREATE\n\n\nFormat\n\n\n  FT.CREATE {index} \n    [MAXTEXTFIELDS] [NOOFFSETS] [NOHL] [NOFIELDS] [NOFREQS]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] [PHONETIC {matcher}] | NUMERIC | GEO | TAG [SEPARATOR {sep}] ] [SORTABLE][NOINDEX] ...\n\n\n\n\n\nDescription\n\n\nCreates an index with the given spec. The index name will be used in all the key names so keep it short!\n\n\n\n\nNote on field number limits\nRediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields.\n\n\nOn 32 bit builds, at most 64 fields can be TEXT fields.\n\n\nNote that the more fields you have, the larger your index will be, as each additional 8 fields require one extra byte per index record to encode.\n\n\nYou can always use the \nNOFIELDS\n option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields.\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\nindex\n: the index name to create. If it exists the old spec will be overwritten\n\n\n\n\n\n\nMAXTEXTFIELDS\n: For efficiency, RediSearch encodes indexes differently if they are\n  created with less than 32 text fields. This option forces RediSearch to encode indexes as if\n  there were more than 32 text fields, which allows you to add additional fields (beyond 32)\n  using \nFT.ALTER\n.\n\n\n\n\n\n\nNOOFFSETS\n: If set, we do not store term offsets for documents (saves memory, does not\n  allow exact searches or highlighting). Implies \nNOHL\n.\n\n\n\n\n\n\nNOHL\n: Conserves storage space and memory by disabling highlighting support. If set, we do\n  not store corresponding byte offsets for term positions. \nNOHL\n is also implied by \nNOOFFSETS\n.\n\n\n\n\n\n\nNOFIELDS\n: If set, we do not store field bits for each term. Saves memory, does not allow\n  filtering by specific fields.\n\n\n\n\n\n\nNOFREQS\n: If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.\n\n\n\n\n\n\nSTOPWORDS\n: If set, we set the index with a custom stopword list, to be ignored during\n  indexing and search time. {num} is the number of stopwords, followed by a list of stopword\n  arguments exactly the length of {num}. \n\n\nIf not set, we take the default list of stopwords. \n\n\nIf \n{num}\n is set to 0, the index will not have stopwords.\n\n\n\n\n\n\nSCHEMA {field} {options...}\n: After the SCHEMA keyword we define the index fields. They\n  can be numeric, textual or geographical. For textual fields we optionally specify a weight.\n  The default weight is 1.0.\n\n\nField Options\n\n\n\n\n\n\nSORTABLE\n\n\nNumeric, tag or text field can have the optional SORTABLE argument that allows the user to later \nsort the results by the value of this field\n (this adds memory overhead so do not declare it on large text fields).\n\n\n\n\n\n\nNOSTEM\n\n\nText fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names.\n\n\n\n\n\n\nNOINDEX\n\n\nFields can have the \nNOINDEX\n option, which means they will not be indexed. \nThis is useful in conjunction with \nSORTABLE\n, to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index.\n\n\n\n\n\n\nPHONETIC {matcher}\n\n\nDeclaring a text field as \nPHONETIC\n will perform phonetic matching on it in searches by default. The obligatory {matcher} argument specifies the phonetic algorithm and language used. The following matchers are supported:\n\n\n\n\ndm:en\n - Double Metaphone for English\n\n\ndm:fr\n - Double Metaphone for French\n\n\ndm:pt\n - Double Metaphone for Portuguese\n\n\ndm:es\n - Double Metaphone for Spanish\n\n\n\n\nFor more details see \nPhonetic Matching\n.\n\n\n\n\n\n\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nOK or an error\n\n\n\n\nFT.ADD\n\n\nFormat\n\n\nFT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE [PARTIAL]]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  [IF {condition}]\n  FIELDS {field} {value} [{field} {value}...]\n\n\n\n\n\nDescription\n\n\nAdds a document to the index.\n\n\nParameters\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id that will be returned from searches. \n\n\n\n\n\n\n\n\nNotes on docId\nThe same docId cannot be added twice to the same index.\n\n\nThe same docId can be added to multiple indices, but a single document with that docId is saved in the database.\n\n\n\n\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nNOSAVE\n: If set to true, we will not save the actual document in the database and only index it.\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the\n  document if it exists. \n\n\n\n\n\n\nPARTIAL\n (only applicable with REPLACE): If set, you do not have to specify all fields for\n  reindexing. Fields not given to the command will be loaded from the current version of the\n  document. Also, if only non-indexable fields, score or payload are set - we do not do a full\n  re-indexing of the document, and this will be a lot faster.\n\n\n\n\n\n\nFIELDS\n: Following the FIELDS specifier, we are looking for pairs of  \n{field} {value}\n to be\n  indexed. Each field will be scored based on the index spec given in \nFT.CREATE\n. \n  Passing fields that are not in the index spec will make them be stored as part of the document,\n  or ignored if NOSAVE is set \n\n\n\n\n\n\nPAYLOAD {payload}\n: Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.\n\n\n\n\n\n\nIF {condition}\n: (Applicable only in conjunction with \nREPLACE\n and optionally \nPARTIAL\n). \n  Update the document only if a boolean expression applies to the document \nbefore the update\n, \n  e.g. \nFT.ADD idx doc 1 REPLACE IF \"@timestamp \n 23323234234\"\n. \n\n\n\n\n\n\nThe expression is evaluated atomically before the update, ensuring that the update will happen only if it is true.\n\n\nSee \nAggregations\n for more details on the expression language. \n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied language during indexing. Default\n  to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\"chinese\"\n\n\n\n\n\n\n\n\nIf indexing a Chinese language document, you must set the language to \nchinese\n\n  in order for Chinese characters to be tokenized properly.\n\n\nAdding Chinese Documents\n\n\nWhen adding Chinese-language documents, \nLANGUAGE chinese\n should be set in\norder for the indexer to properly tokenize the terms. If the default language\nis used then search terms will be extracted based on punctuation characters and\nwhitespace. The Chinese language tokenizer makes use of a segmentation algorithm\n(via \nFriso\n) which segments texts and\nchecks it against a predefined dictionary. See \nStemming\n for more\ninformation.\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\nA special status \nNOADD\n is returned if an \nIF\n condition evaluated to false.\n\n\n\n\nFT.ADD with REPLACE and PARTIAL\nBy default, FT.ADD does not allow updating the document, and will fail if it already exists in the index.\n\n\nHowever, updating the document is possible with the REPLACE and REPLACE PARTIAL options.\n\n\nREPLACE\n: On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document.\n\n\nREPLACE PARTIAL\n: When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage.\n\n\n\n\n\n\n\n\nFT.ADDHASH\n\n\nFormat\n\n\n \nFT\n.\nADDHASH\n \n{\nindex\n}\n \n{\ndocId\n}\n \n{\nscore\n}\n \n[\nLANGUAGE\n \nlanguage\n]\n \n[\nREPLACE\n]\n\n\n\n\n\n\nDescription\n\n\nAdds a document to the index from an existing HASH key in Redis.\n\n\nParameters\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id. This has to be an existing HASH key in Redis that will hold the fields \n    the index needs.\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.\n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied language during indexing. Defaults \n  to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\n\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\n\n\nFT.ALTER\n\n\nFormat\n\n\nFT.ALTER {index} SCHEMA ADD {field} {options} ...\n\n\n\n\n\nDescription\n\n\nAlters an existing index. Currently, adding fields to the index is the only supported\nalteration.\n\n\nAdding a field to the index will cause any future document updates to use the new field when\nindexing. Existing documents will not be reindexed.\n\n\n\n\nNote\n\n\nDepending on how the index was created, you may be limited by the amount of additional text\nfields which can be added to an existing index. If the current index contains less than 32\ntext fields, then \nSCHEMA ADD\n will only be able to add up to 32 fields (meaning that the\nindex will only ever be able to contain 32 total text fields). If you wish for the index to\ncontain more than 32 fields, create it with the \nMAXTEXTFIELDS\n option.\n\n\n\n\nParameters\n\n\n\n\nindex\n: the index name.\n\n\nfield\n: the field name.\n\n\noptions\n: the field options - refer to \nFT.CREATE\n for more information.\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nOK or an error.\n\n\n\n\nFT.INFO\n\n\nFormat\n\n\nFT.INFO {index} \n\n\n\n\n\nDescription\n\n\nReturns information and statistics on the index. Returned values include:\n\n\n\n\nNumber of documents.\n\n\nNumber of distinct terms.\n\n\nAverage bytes per record.\n\n\nSize and capacity of the index buffers.\n\n\n\n\nExample:\n\n\n127.0.0.1:6379\n ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n 5) num_docs\n 6) \n502694\n\n 7) num_terms\n 8) \n439158\n\n 9) num_records\n10) \n8098583\n\n11) inverted_sz_mb\n12) \n45.58\n13) inverted_cap_mb\n14) \n56.61\n15) inverted_cap_ovh\n16) \n0.19\n17) offset_vectors_sz_mb\n18) \n9.27\n19) skip_index_size_mb\n20) \n7.35\n21) score_index_size_mb\n22) \n30.8\n23) records_per_doc_avg\n24) \n16.1\n25) bytes_per_record_avg\n26) \n5.90\n27) offsets_per_term_avg\n28) \n1.20\n29) offset_bits_per_record_avg\n30) \n8.00\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nArray Response. A nested array of keys and values.\n\n\n\n\nFT.SEARCH\n\n\nFormat\n\n\nFT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]]\n  [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]\n\n\n\n\n\nDescription\n\n\nSearches the index with a textual query, returning either documents or just ids.\n\n\nParameters\n\n\n\n\nindex\n: The index name. The index must be first created with \nFT.CREATE\n.\n\n\n\n\nquery\n: the text query to search. If it's more than a single word, put it in quotes.\n  Refer to \nquery syntax\n for more details. \n\n\n\n\n\n\nNOCONTENT\n: If it appears after the query, we only return the document ids and not \n  the content. This is useful if RediSearch is only an index on an external document collection\n\n\n\n\nVERBATIM\n: if set, we do not try to use stemming for query expansion but search the query terms \n  verbatim.\n\n\nNOSTOPWORDS\n: If set, we do not filter stopwords from the query.\n\n\nWITHSCORES\n: If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances\n\n\nWITHPAYLOADS\n: If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if \nWITHSCORES\n was set, follow the scores.\n\n\n\n\nWITHSORTKEYS\n: Only relevant in conjunction with \nSORTBY\n. Returns the value of the sorting key,\n  right after the id and score and /or payload if requested. This is usually not needed by users, and \n  exists for distributed search coordination purposes.\n\n\n\n\n\n\nFILTER numeric_field min max\n: If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be \n-inf\n, \n+inf\n and use \n(\n for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.\n\n\n\n\nGEOFILTER {geo_field} {lon} {lat} {radius} m|km|mi|ft\n: If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See \nGEORADIUS\n \n  for more details.\n\n\nINKEYS {num} {field} ...\n: If set, we limit the result to a given set of keys specified in the \n  list. \n  the first argument must be the length of the list, and greater than zero.\n  Non-existent keys are ignored - unless all the keys are non-existent.\n\n\n\n\nINFIELDS {num} {field} ...\n: If set, filter the results to ones appearing only in specific\n  fields of the document, like title or URL. num is the number of specified field arguments  \n\n\n\n\n\n\nRETURN {num} {field} ...\n: Use this keyword to limit which fields from the document are returned.\n  \nnum\n is the number of fields following the keyword. If \nnum\n is 0, it acts like \nNOCONTENT\n.  \n\n\n\n\nSUMMARIZE ...\n: Use this option to return only the sections of the field which contain the \n  matched text.\n  See \nHighlighting\n for more details\n\n\nHIGHLIGHT ...\n: Use this option to format occurrences of matched text. See \nHighligting\n for more\n  details\n\n\nSLOP {slop}\n: If set, we allow a maximum of N intervening number of unmatched offsets between \n  phrase terms. (i.e the slop for exact phrases is 0)\n\n\nINORDER\n: If set, and usually used in conjunction with SLOP, we make sure the query terms appear \n  in the same order in the document as in the query, regardless of the offsets between them. \n\n\n\n\nLANGUAGE {language}\n: If set, we use a stemmer for the supplied language during search for query \n  expansion.\n  If querying documents in Chinese, this should be set to \nchinese\n in order to\n  properly tokenize the query terms. \n  Defaults to English. If an unsupported language is sent, the command returns an error.\n  See FT.ADD for the list of languages.\n\n\n\n\n\n\nEXPANDER {expander}\n: If set, we will use a custom query expander instead of the stemmer. \nSee Extensions\n.\n\n\n\n\nSCORER {scorer}\n: If set, we will use a custom scoring function defined by the user. \nSee Extensions\n.\n\n\n\n\nPAYLOAD {payload}\n: Add an arbitrary, binary safe payload that will be exposed to custom scoring \n  functions. \nSee Extensions\n.\n\n\n\n\n\n\nSORTBY {field} [ASC|DESC]\n: If specified, and field is a \nsortable field\n, the results \n  are ordered by the value of this field. This applies to both text and numeric fields.\n\n\n\n\nLIMIT first num\n: If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10\n\n\n\n\nComplexity\n\n\nO(n) for single word queries (though for popular words we save a cache of the top 50 results).\n\n\nComplexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.\n\n\nReturns\n\n\nArray reply,\n where the first element is the total number of results, and then pairs of document id, and a nested array of field/value. \n\n\nIf \nNOCONTENT\n was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.\n\n\n\n\nFT.AGGREGATE\n\n\nFormat\n\n\nFT.AGGREGATE  {index_name}\n  {query_string}\n  [WITHSCHEMA] [VERBATIM]\n  [LOAD {nargs} {property} ...]\n  [GROUPBY {nargs} {property} ...\n    REDUCE {func} {nargs} {arg} ... [AS {name:string}]\n    ...\n  ] ...\n  [SORTBY {nargs} {property} [ASC|DESC] ... [MAX {num}]]\n  [APPLY {expr} AS {alias}] ...\n  [LIMIT {offset} {num}] ...\n  [FILTER {expr}] ...\n\n\n\n\n\nDescription\n\n\nRuns a search query on an index, and performs aggregate transformations on the results, extracting statistics etc from them. See \nthe full documentation on aggregations\n for further details.\n\n\nParameters\n\n\n\n\n\n\nindex_name\n: The index the query is executed against.\n\n\n\n\n\n\nquery_string\n: The base filtering query that retrieves the documents. It follows\n  \nthe exact same syntax\n as the search query, including filters, unions, not, optional, etc.\n\n\n\n\n\n\nLOAD {nargs} {property} \u2026\n: Load document fields from the document HASH objects. This should be \n  avoided as a general rule of thumb. Fields needed for aggregations should be stored as \nSORTABLE\n, \n  where they are available to the aggregation pipeline with very load latency. LOAD hurts the \n  performance of aggregate queries considerably, since every processed record needs to execute the \n  equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to very \n  high processing times. \n\n\n\n\n\n\nGROUPBY {nargs} {property}\n: Group the results in the pipeline based on one or more properties. \n  Each group should have at least one reducer (See below), a function that handles the group entries, \n  either counting them, or performing multiple aggregate operations (see below).\n\n\n\n\n\n\nREDUCE {func} {nargs} {arg} \u2026 [AS {name}]\n: Reduce the matching results in each group into a single record, using a reduction function. For example COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers. \n\n\nThe reducers can have their own property names using the \nAS {name}\n optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property \n@foo\n, the resulting name will be \ncount_distinct(@foo)\n. \n\n\n\n\n\n\n\n\n\n\nSORTBY {nargs} {property} {ASC|DESC} [MAX {num}]\n: Sort the pipeline up until the point of SORTBY,\n  using a list of properties. By default, sorting is ascending, but \nASC\n or \nDESC\n can be added for \n  each property. \nnargs\n is the number of sorting parameters, including ASC and DESC. for example: \n  \nSORTBY 4 @foo ASC @bar DESC\n. \n\n\nMAX\n is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to \nLIMIT\n, you usually need just \nSORTBY \u2026 MAX\n for common queries. \n\n\n\n\n\n\nAPPLY {expr} AS {name}\n: Apply a 1-to-1 transformation on one or more properties, and either \n  store the result as a new property down the pipeline, or replace any property using this \n  transformation. \nexpr\n is an expression that can be used to perform arithmetic operations on numeric \n  properties, or functions that can be applied on properties depending on their types (see below), or \n  any combination thereof. For example: \nAPPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz\n will evaluate this \n  expression dynamically for each record in the pipeline and store the result as a new property called \n  baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the \n  pipeline. \n\n\n\n\n\n\nLIMIT {offset} {num}\n. Limit the number of results to return just \nnum\n results starting at index \n  \noffset\n (zero-based). AS mentioned above, it is much more efficient to use \nSORTBY \u2026 MAX\n if you \n  are interested in just limiting the output of a sort operation.\n\n\nHowever, limit can be used to limit results without sorting, or for paging the n-largest results as determined by \nSORTBY MAX\n. For example, getting results 50-100 of the top 100 results is most efficiently expressed as \nSORTBY 1 @foo MAX 100 LIMIT 50 50\n. Removing the MAX from SORTBY will result in the pipeline sorting \nall\n the records and then paging over results 50-100. \n\n\n\n\n\n\nFILTER {expr}\n. Filter the results using predicate expressions relating to values in each result. \n  They are is applied post-query and relate to the current state of the pipeline. \n\n\n\n\n\n\nComplexity\n\n\nNon-deterministic. Depends on the query and aggregations performed, but it is usually linear to the number of results returned. \n\n\nReturns\n\n\nArray Response. Each row is an array and represents a single aggregate result.\n\n\nExample output\n\n\nHere we are counting GitHub events by user (actor), to produce the most active users:\n\n\n127.0.0.1:6379\n FT.AGGREGATE gh \n*\n GROUPBY 1 @actor REDUCE COUNT 0 AS num SORTBY 2 @num DESC MAX 10\n 1) (integer) 284784\n 2) 1) \nactor\n\n    2) \nlombiqbot\n\n    3) \nnum\n\n    4) \n22197\n\n 3) 1) \nactor\n\n    2) \ncodepipeline-test\n\n    3) \nnum\n\n    4) \n17746\n\n 4) 1) \nactor\n\n    2) \ndirewolf-github\n\n    3) \nnum\n\n    4) \n10683\n\n 5) 1) \nactor\n\n    2) \nogate\n\n    3) \nnum\n\n    4) \n6449\n\n 6) 1) \nactor\n\n    2) \nopenlocalizationtest\n\n    3) \nnum\n\n    4) \n4759\n\n 7) 1) \nactor\n\n    2) \ndigimatic\n\n    3) \nnum\n\n    4) \n3809\n\n 8) 1) \nactor\n\n    2) \ngugod\n\n    3) \nnum\n\n    4) \n3512\n\n 9) 1) \nactor\n\n    2) \nxdzou\n\n    3) \nnum\n\n    4) \n3216\n\n10) 1) \nactor\n\n    2) \nopstest\n\n    3) \nnum\n\n    4) \n2863\n\n11) 1) \nactor\n\n    2) \njikker\n\n    3) \nnum\n\n    4) \n2794\n\n(0.59s)\n\n\n\n\n\n\n\nFT.EXPLAIN\n\n\nFormat\n\n\nFT.EXPLAIN {index} {query}\n\n\n\n\n\nDescription\n\n\nReturns the execution plan for a complex query.\n\n\nIn the returned response, a \n+\n on a term is an indication of stemming. \n\n\nExample:\n\n\n$ redis-cli --raw\n\n\n127\n.0.0.1:6379\n FT.EXPLAIN rd \n(foo bar)|(hello world) @date:[100 200]|@date:[500 +inf]\n\nINTERSECT \n{\n\n  UNION \n{\n\n    INTERSECT \n{\n\n      foo\n      bar\n    \n}\n\n    INTERSECT \n{\n\n      hello\n      world\n    \n}\n\n  \n}\n\n  UNION \n{\n\n    NUMERIC \n{\n100\n.000000 \n=\n x \n=\n \n200\n.000000\n}\n\n    NUMERIC \n{\n500\n.000000 \n=\n x \n=\n inf\n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The index name. The index must be first created with FT.CREATE\n\n\nquery\n: The query string, as if sent to FT.SEARCH\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nString Response. A string representing the execution plan (see above example). \n\n\nNote\n: You should use \nredis-cli --raw\n to properly read line-breaks in the returned response.\n\n\n\n\nFT.DEL\n\n\nFormat\n\n\nFT.DEL {index} {doc_id} [DD]\n\n\n\n\n\nDescription\n\n\nDeletes a document from the index. Returns 1 if the document was in the index, or 0 if not. \n\n\nAfter deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.\n\n\n\n\nFT.DEL does not delete the actual document By default!\nSince RediSearch regards documents as separate entities to the index and allows things like adding existing documents or indexing without saving the document - by default FT.DEL only deletes the reference to the document from the index, not the actual Redis HASH key where the document is stored. \n\n\nSpecifying \nDD\n (Delete Document) after the document ID, will make RediSearch also delete the actual document \nif it is in the index\n.\n\n\nAlternatively, you can just send an extra \nDEL {doc_id}\n to redis and delete the document directly. You can run both of them in a MULTI transaction.\n\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The index name. The index must be first created with FT.CREATE\n\n\ndoc_id\n: the id of the document to be deleted. It does not actually delete the HASH key in which \n  the document is stored. Use DEL to do that manually if needed.\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nInteger Reply: 1 if the document was deleted, 0 if not.\n\n\n\n\nFT.GET\n\n\nFormat\n\n\nFT.GET {index} {doc id}\n\n\n\n\n\nDescription\n\n\nReturns the full contents of a document.\n\n\nCurrently it is equivalent to HGETALL, but this is future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode.\n\n\nIf the document does not exist or is not a HASH object, we return a NULL reply\n\n\nParameters\n\n\n\n\nindex\n: The index name. The index must be first created with FT.CREATE\n\n\ndocumentId\n: The id of the document as inserted to the index\n\n\n\n\nReturns\n\n\nArray Reply: Key-value pairs of field names and values of the document\n\n\n\n\nFT.MGET\n\n\nFormat\n\n\nFT.MGET {index} {docId} ...\n\n\n\n\n\nDescription\n\n\nReturns the full contents of multiple documents. \n\n\nCurrently it is equivalent to calling multiple HGETALL commands, although faster. \nThis command is also future-proof and will allow us to change the internal representation of documents inside Redis in the future. \nIn addition, it allows simpler implementation of fetching documents in clustered mode.\n\n\nWe return an array with exactly the same number of elements as the number of keys sent to the command. \n\n\nEach element, in turn, is an array of key-value pairs representing the document. \n\n\nIf a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\ndocumentIds\n: The ids of the requested documents as inserted to the index\n\n\n\n\nReturns\n\n\nArray Reply: An array with exactly the same number of elements as the number of keys sent to the command.  Each element in it is either an array representing the document or Null if it was not found.\n\n\n\n\nFT.DROP\n\n\nFormat\n\n\nFT.DROP {index} [KEEPDOCS]\n\n\n\n\n\nDescription\n\n\nDeletes all the keys associated with the index. \n\n\nBy default, DROP deletes the document hashes as well, but adding the KEEPDOCS option keeps the documents in place, ready for re-indexing.\n\n\nIf no other data is on the Redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nKEEPDOCS\n: If set, the drop operation will not delete the actual document hashes.\n\n\n\n\nReturns\n\n\nStatus Reply: OK on success.\n\n\n\n\nFT.TAGVALS\n\n\nFormat\n\n\nFT.TAGVALS {index} {field_name}\n\n\n\n\n\nDescription\n\n\nReturns the distinct tags indexed in a \nTag field\n. \n\n\nThis is useful if your tag field indexes things like cities, categories, etc.\n\n\n\n\nLimitations\n\n\nThere is no paging or sorting, the tags are not alphabetically sorted. \n\n\nThis command only operates on \nTag fields\n.  \n\n\nThe strings return lower-cased and stripped of whitespaces, but otherwise unchanged.\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nfiled_name\n: The name of a Tag file defined in the schema.\n\n\n\n\nReturns\n\n\nArray Reply: All the distinct tags in the tag index.\n\n\nComplexity\n\n\nO(n), n being the cardinality of the tag field.\n\n\n\n\nFT.SUGADD\n\n\nFormat\n\n\nFT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]\n\n\n\n\n\nDescription\n\n\nAdds a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestions dictionaries to the user.\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the suggestion string we index\n\n\nscore\n: a floating point number of the suggestion string's weight\n\n\nINCR\n: if set, we increment the existing entry of the suggestion by the given score, instead of \n  replacing the score. This is useful for updating the dictionary based on user queries in real time\n\n\nPAYLOAD {payload}\n: If set, we save an extra payload with the suggestion, that can be fetched by \n  adding the \nWITHPAYLOADS\n argument to \nFT.SUGGET\n.\n\n\n\n\nReturns\n\n\nInteger Reply: the current size of the suggestion dictionary.\n\n\n\n\nFT.SUGGET\n\n\nFormat\n\n\nFT\n.\nSUGGET\n \n{\nkey\n}\n \n{\nprefix\n}\n \n[\nFUZZY\n]\n \n[\nWITHSCORES\n]\n \n[\nWITHPAYLOADS\n]\n \n[\nMAX\n \nnum\n]\n\n\n\n\n\n\nDescription\n\n\nGets completion suggestions for a prefix.\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nprefix\n: the prefix to complete on\n\n\nFUZZY\n: if set, we do a fuzzy prefix search, including prefixes at Levenshtein distance of 1 from \n  the prefix sent\n\n\nMAX num\n: If set, we limit the results to a maximum of \nnum\n (default: 5).\n\n\nWITHSCORES\n: If set, we also return the score of each suggestion. this can be used to merge \n  results from multiple instances\n\n\nWITHPAYLOADS\n: If set, we return optional payloads saved along with the suggestions. If no \n  payload is present for an entry, we return a Null Reply.\n\n\n\n\nReturns\n\n\nArray Reply: a list of the top suggestions matching the prefix, optionally with score after each entry\n\n\n\n\nFT.SUGDEL\n\n\nFormat\n\n\nFT.SUGDEL {key} {string}\n\n\n\n\n\nDescription\n\n\nDeletes a string from a suggestion index. \n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the string to delete\n\n\n\n\nReturns\n\n\nInteger Reply: 1 if the string was found and deleted, 0 otherwise.\n\n\n\n\nFT.SUGLEN\n\n\nFormat\n\n\nFT.SUGLEN {key}\n\n\n\n\n\nDescription\n\n\nGets the size of an auto-complete suggestion dictionary\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\n\n\nReturns\n\n\nInteger Reply: the current size of the suggestion dictionary.\n\n\n\n\nFT.OPTIMIZE\n\n\n\n\nThis command is deprecated\n\n\nIndex optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command and remove it if they haven't already.\n\n\n\n\nFormat\n\n\nFT.OPTIMIZE {index}\n\n\n\n\n\nDescription\n\n\nThis command is deprecated. \n\n\n\n\nFT.SYNADD\n\n\nFormat\n\n\nFT.SYNADD \nindex name\n \nterm1\n \nterm2\n ...\n\n\n\n\n\nDescription\n\n\nAdds a synonym group.\n\n\nThe command is used to create a new synonyms group. The command returns the synonym group id which can later be used to add additional terms to that synonym group. Only documents which was indexed after the adding operation will be effected.\n\n\n\n\nFT.SYNUPDATE\n\n\nFormat\n\n\nFT.SYNUPDATE \nindex name\n \nsynonym group id\n \nterm1\n \nterm2\n ...\n\n\n\n\n\nDescription\n\n\nUpdates a synonym group.\n\n\nThe command is used to update an existing synonym group with additional terms. Only documents which was indexed after the update will be effected.\n\n\n\n\nFT.SYNDUMP\n\n\nFormat\n\n\nFT.SYNDUMP \nindex name\n\n\n\n\n\n\nDescription\n\n\nDumps the contents of a synonym group.\n\n\nThe command is used to dump the synonyms data structure. Returns a list of synonym terms and their synonym group ids.\n\n\n\n\nFT.SPELLCHECK\n\n\nFormat\n\n\n  \nFT\n.\nSPELLCHECK\n \n{\nindex\n}\n \n{\nquery\n}\n\n    \n[\nDISTANCE\n \ndist\n]\n\n    \n[\nTERMS\n \n{\nINCLUDE\n \n|\n \nEXCLUDE\n}\n \n{\ndict\n}\n \n[\nTERMS\n \n...]]\n\n\n\n\n\n\nDescription\n\n\nPerforms spelling correction on a query, returning suggestions for misspelled terms.\n\n\nSee \nQuery Spelling Correction\n for more details.\n\n\nParameters\n\n\n\n\n\n\nindex\n: the index with the indexed terms.\n\n\n\n\n\n\nquery\n: the search query.\n\n\n\n\n\n\nTERMS\n: specifies an inclusion (\nINCLUDE\n) or exclusion (\nEXCLUDE\n) custom dictionary named \n{dict}\n. Refer to \nFT.DICTADD\n, \nFT.DICTDEL\n and \nFT.DICTDUMP\n for managing custom dictionaries.\n\n\n\n\n\n\nDISTANCE\n: the maximal Levenshtein distance for spelling suggestions (default: 1, max: 4).\n\n\n\n\n\n\nReturns\n\n\nAn array, in which each element represents a misspelled term from the query. The misspelled terms are ordered by their order of appearance in the query.\n\n\nEach misspelled term, in turn, is a 3-element array consisting of the constant string \"TERM\", the term itself and an array of suggestions for spelling corrections.\n\n\nEach element in the spelling corrections array consists of the score of the suggestion and the suggestion itself. The suggestions array, per misspelled term, is ordered in descending order by score.\n\n\nExample output\n\n\n1)  1) \nTERM\n\n    2) \n{term1}\n\n    3)  1)  1)  \n{score1}\n\n            2)  \n{suggestion1}\n\n        2)  1)  \n{score2}\n\n            2)  \n{suggestion2}\n\n        .\n        .\n        .\n2)  1) \nTERM\n\n    2) \n{term2}\n\n    3)  1)  1)  \n{score1}\n\n            2)  \n{suggestion1}\n\n        2)  1)  \n{score2}\n\n            2)  \n{suggestion2}\n\n        .\n        .\n        .\n.\n.\n.\n\n\n\n\n\n\n\nFT.DICTADD\n\n\nFormat\n\n\n  FT.DICTADD {dict} {term} [{term} ...]\n\n\n\n\n\nDescription\n\n\nAdds terms to a dictionary.\n\n\nParameters\n\n\n\n\n\n\ndict\n: the dictionary name.\n\n\n\n\n\n\nterm\n: the term to add to the dictionary.\n\n\n\n\n\n\nReturns\n\n\nReturns int, specifically the number of new terms that were added.\n\n\n\n\nFT.DICTDEL\n\n\nFormat\n\n\n  FT.DICTDEL {dict} {term} [{term} ...]\n\n\n\n\n\nDescription\n\n\nDeletes terms from a dictionary.\n\n\nParameters\n\n\n\n\n\n\ndict\n: the dictionary name.\n\n\n\n\n\n\nterm\n: the term to delete from the dictionary.\n\n\n\n\n\n\nReturns\n\n\nReturns int, specifically the number of terms that were deleted.\n\n\n\n\nFT.DICTDUMP\n\n\nFormat\n\n\n  FT.DICTDUMP {dict}\n\n\n\n\n\nDescription\n\n\nDumps all terms in the given dictionary.\n\n\nParameters\n\n\n\n\ndict\n: the dictionary name.\n\n\n\n\nReturns\n\n\nReturns an array, where each element is term (string).\n\n\n\n\nFT.CONFIG\n\n\nFormat\n\n\n  FT.CONFIG \nGET|HELP\n {option}\n  FT.CONFIG SET {option} {value}\n\n\n\n\n\nDescription\n\n\nRetrieves, describes and sets runtime configuration options.\n\n\nParameters\n\n\n\n\noption\n: the name of the configuration option, or '*' for all.\n\n\nvalue\n: a value for the configuration option.\n\n\n\n\nFor details about the configuration options refer to \nConfiguring\n.\n\n\nSetting values in runtime is supported for these configuration options:\n\n\n\n\nNOGC\n\n\nMINPREFIX\n\n\nMAXEXPANSIONS\n\n\nTIMEOUT\n\n\nON_TIMEOUT\n\n\nMIN_PHONETIC_TERM_LEN\n\n\n\n\nReturns\n\n\nWhen provided with a valid option name, the \nGET\n subcommand returns a string with the current option's value. An array containing an array for each configuration option, consisting of the option's name and current value, is returned when '*' is provided.\n\n\nThe \nSET\n subcommand returns 'OK' for valid runtime-settable option names and values.", 
            "title": "Command Reference"
        }, 
        {
            "location": "/Commands/#redisearch\"_\"full\"_\"command\"_\"documentation", 
            "text": "", 
            "title": "RediSearch Full Command Documentation"
        }, 
        {
            "location": "/Commands/#ftcreate", 
            "text": "", 
            "title": "FT.CREATE"
        }, 
        {
            "location": "/Commands/#format", 
            "text": "FT.CREATE {index} \n    [MAXTEXTFIELDS] [NOOFFSETS] [NOHL] [NOFIELDS] [NOFREQS]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] [PHONETIC {matcher}] | NUMERIC | GEO | TAG [SEPARATOR {sep}] ] [SORTABLE][NOINDEX] ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description", 
            "text": "Creates an index with the given spec. The index name will be used in all the key names so keep it short!   Note on field number limits RediSearch supports up to 1024 fields per schema, out of which at most 128 can be TEXT fields.  On 32 bit builds, at most 64 fields can be TEXT fields.  Note that the more fields you have, the larger your index will be, as each additional 8 fields require one extra byte per index record to encode.  You can always use the  NOFIELDS  option and not encode field information into the index, for saving space, if you do not need filtering by text fields. This will still allow filtering by numeric and geo fields.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters", 
            "text": "index : the index name to create. If it exists the old spec will be overwritten    MAXTEXTFIELDS : For efficiency, RediSearch encodes indexes differently if they are\n  created with less than 32 text fields. This option forces RediSearch to encode indexes as if\n  there were more than 32 text fields, which allows you to add additional fields (beyond 32)\n  using  FT.ALTER .    NOOFFSETS : If set, we do not store term offsets for documents (saves memory, does not\n  allow exact searches or highlighting). Implies  NOHL .    NOHL : Conserves storage space and memory by disabling highlighting support. If set, we do\n  not store corresponding byte offsets for term positions.  NOHL  is also implied by  NOOFFSETS .    NOFIELDS : If set, we do not store field bits for each term. Saves memory, does not allow\n  filtering by specific fields.    NOFREQS : If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.    STOPWORDS : If set, we set the index with a custom stopword list, to be ignored during\n  indexing and search time. {num} is the number of stopwords, followed by a list of stopword\n  arguments exactly the length of {num}.   If not set, we take the default list of stopwords.   If  {num}  is set to 0, the index will not have stopwords.    SCHEMA {field} {options...} : After the SCHEMA keyword we define the index fields. They\n  can be numeric, textual or geographical. For textual fields we optionally specify a weight.\n  The default weight is 1.0.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#field\"_\"options", 
            "text": "SORTABLE  Numeric, tag or text field can have the optional SORTABLE argument that allows the user to later  sort the results by the value of this field  (this adds memory overhead so do not declare it on large text fields).    NOSTEM  Text fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names.    NOINDEX  Fields can have the  NOINDEX  option, which means they will not be indexed. \nThis is useful in conjunction with  SORTABLE , to create fields whose update using PARTIAL will not cause full reindexing of the document. If a field has NOINDEX and doesn't have SORTABLE, it will just be ignored by the index.    PHONETIC {matcher}  Declaring a text field as  PHONETIC  will perform phonetic matching on it in searches by default. The obligatory {matcher} argument specifies the phonetic algorithm and language used. The following matchers are supported:   dm:en  - Double Metaphone for English  dm:fr  - Double Metaphone for French  dm:pt  - Double Metaphone for Portuguese  dm:es  - Double Metaphone for Spanish   For more details see  Phonetic Matching .", 
            "title": "Field Options"
        }, 
        {
            "location": "/Commands/#complexity", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns", 
            "text": "OK or an error", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftadd", 
            "text": "", 
            "title": "FT.ADD"
        }, 
        {
            "location": "/Commands/#format_1", 
            "text": "FT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE [PARTIAL]]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  [IF {condition}]\n  FIELDS {field} {value} [{field} {value}...]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_1", 
            "text": "Adds a document to the index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_1", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id that will be returned from searches.      Notes on docId The same docId cannot be added twice to the same index.  The same docId can be added to multiple indices, but a single document with that docId is saved in the database.      score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    NOSAVE : If set to true, we will not save the actual document in the database and only index it.    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the\n  document if it exists.     PARTIAL  (only applicable with REPLACE): If set, you do not have to specify all fields for\n  reindexing. Fields not given to the command will be loaded from the current version of the\n  document. Also, if only non-indexable fields, score or payload are set - we do not do a full\n  re-indexing of the document, and this will be a lot faster.    FIELDS : Following the FIELDS specifier, we are looking for pairs of   {field} {value}  to be\n  indexed. Each field will be scored based on the index spec given in  FT.CREATE . \n  Passing fields that are not in the index spec will make them be stored as part of the document,\n  or ignored if NOSAVE is set     PAYLOAD {payload} : Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.    IF {condition} : (Applicable only in conjunction with  REPLACE  and optionally  PARTIAL ). \n  Update the document only if a boolean expression applies to the document  before the update , \n  e.g.  FT.ADD idx doc 1 REPLACE IF \"@timestamp   23323234234\" .     The expression is evaluated atomically before the update, ensuring that the update will happen only if it is true.  See  Aggregations  for more details on the expression language.     LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Default\n  to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:   \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\"chinese\"     If indexing a Chinese language document, you must set the language to  chinese \n  in order for Chinese characters to be tokenized properly.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#adding\"_\"chinese\"_\"documents", 
            "text": "When adding Chinese-language documents,  LANGUAGE chinese  should be set in\norder for the indexer to properly tokenize the terms. If the default language\nis used then search terms will be extracted based on punctuation characters and\nwhitespace. The Chinese language tokenizer makes use of a segmentation algorithm\n(via  Friso ) which segments texts and\nchecks it against a predefined dictionary. See  Stemming  for more\ninformation.", 
            "title": "Adding Chinese Documents"
        }, 
        {
            "location": "/Commands/#complexity_1", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_1", 
            "text": "OK on success, or an error if something went wrong.  A special status  NOADD  is returned if an  IF  condition evaluated to false.   FT.ADD with REPLACE and PARTIAL By default, FT.ADD does not allow updating the document, and will fail if it already exists in the index.  However, updating the document is possible with the REPLACE and REPLACE PARTIAL options.  REPLACE : On its own, sets the document to the new values, and reindexes it. Any fields not given will not be loaded from the current version of the document.  REPLACE PARTIAL : When both arguments are used, we can update just part of the document fields, and the rest will be loaded before reindexing. Not only that, but if only the score, payload and non-indexed fields (using NOINDEX) are updated, we will not actually reindex the document, just update its metadata internally, which is a lot faster and does not create index garbage.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftaddhash", 
            "text": "", 
            "title": "FT.ADDHASH"
        }, 
        {
            "location": "/Commands/#format_2", 
            "text": "FT . ADDHASH   { index }   { docId }   { score }   [ LANGUAGE   language ]   [ REPLACE ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_2", 
            "text": "Adds a document to the index from an existing HASH key in Redis.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_2", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id. This has to be an existing HASH key in Redis that will hold the fields \n    the index needs.    score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.    LANGUAGE language : If set, we use a stemmer for the supplied language during indexing. Defaults \n  to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:     \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_2", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_2", 
            "text": "OK on success, or an error if something went wrong.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftalter", 
            "text": "", 
            "title": "FT.ALTER"
        }, 
        {
            "location": "/Commands/#format_3", 
            "text": "FT.ALTER {index} SCHEMA ADD {field} {options} ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_3", 
            "text": "Alters an existing index. Currently, adding fields to the index is the only supported\nalteration.  Adding a field to the index will cause any future document updates to use the new field when\nindexing. Existing documents will not be reindexed.   Note  Depending on how the index was created, you may be limited by the amount of additional text\nfields which can be added to an existing index. If the current index contains less than 32\ntext fields, then  SCHEMA ADD  will only be able to add up to 32 fields (meaning that the\nindex will only ever be able to contain 32 total text fields). If you wish for the index to\ncontain more than 32 fields, create it with the  MAXTEXTFIELDS  option.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_3", 
            "text": "index : the index name.  field : the field name.  options : the field options - refer to  FT.CREATE  for more information.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_3", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_3", 
            "text": "OK or an error.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftinfo", 
            "text": "", 
            "title": "FT.INFO"
        }, 
        {
            "location": "/Commands/#format_4", 
            "text": "FT.INFO {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_4", 
            "text": "Returns information and statistics on the index. Returned values include:   Number of documents.  Number of distinct terms.  Average bytes per record.  Size and capacity of the index buffers.   Example:  127.0.0.1:6379  ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n 5) num_docs\n 6)  502694 \n 7) num_terms\n 8)  439158 \n 9) num_records\n10)  8098583 \n11) inverted_sz_mb\n12)  45.58\n13) inverted_cap_mb\n14)  56.61\n15) inverted_cap_ovh\n16)  0.19\n17) offset_vectors_sz_mb\n18)  9.27\n19) skip_index_size_mb\n20)  7.35\n21) score_index_size_mb\n22)  30.8\n23) records_per_doc_avg\n24)  16.1\n25) bytes_per_record_avg\n26)  5.90\n27) offsets_per_term_avg\n28)  1.20\n29) offset_bits_per_record_avg\n30)  8.00", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_4", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_4", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_4", 
            "text": "Array Response. A nested array of keys and values.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsearch", 
            "text": "", 
            "title": "FT.SEARCH"
        }, 
        {
            "location": "/Commands/#format_5", 
            "text": "FT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SUMMARIZE [FIELDS {num} {field} ... ] [FRAGS {num}] [LEN {fragsize}] [SEPARATOR {separator}]]\n  [HIGHLIGHT [FIELDS {num} {field} ... ] [TAGS {open} {close}]]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_5", 
            "text": "Searches the index with a textual query, returning either documents or just ids.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_5", 
            "text": "index : The index name. The index must be first created with  FT.CREATE .   query : the text query to search. If it's more than a single word, put it in quotes.\n  Refer to  query syntax  for more details.     NOCONTENT : If it appears after the query, we only return the document ids and not \n  the content. This is useful if RediSearch is only an index on an external document collection   VERBATIM : if set, we do not try to use stemming for query expansion but search the query terms \n  verbatim.  NOSTOPWORDS : If set, we do not filter stopwords from the query.  WITHSCORES : If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances  WITHPAYLOADS : If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if  WITHSCORES  was set, follow the scores.   WITHSORTKEYS : Only relevant in conjunction with  SORTBY . Returns the value of the sorting key,\n  right after the id and score and /or payload if requested. This is usually not needed by users, and \n  exists for distributed search coordination purposes.    FILTER numeric_field min max : If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be  -inf ,  +inf  and use  (  for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.   GEOFILTER {geo_field} {lon} {lat} {radius} m|km|mi|ft : If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See  GEORADIUS  \n  for more details.  INKEYS {num} {field} ... : If set, we limit the result to a given set of keys specified in the \n  list. \n  the first argument must be the length of the list, and greater than zero.\n  Non-existent keys are ignored - unless all the keys are non-existent.   INFIELDS {num} {field} ... : If set, filter the results to ones appearing only in specific\n  fields of the document, like title or URL. num is the number of specified field arguments      RETURN {num} {field} ... : Use this keyword to limit which fields from the document are returned.\n   num  is the number of fields following the keyword. If  num  is 0, it acts like  NOCONTENT .     SUMMARIZE ... : Use this option to return only the sections of the field which contain the \n  matched text.\n  See  Highlighting  for more details  HIGHLIGHT ... : Use this option to format occurrences of matched text. See  Highligting  for more\n  details  SLOP {slop} : If set, we allow a maximum of N intervening number of unmatched offsets between \n  phrase terms. (i.e the slop for exact phrases is 0)  INORDER : If set, and usually used in conjunction with SLOP, we make sure the query terms appear \n  in the same order in the document as in the query, regardless of the offsets between them.    LANGUAGE {language} : If set, we use a stemmer for the supplied language during search for query \n  expansion.\n  If querying documents in Chinese, this should be set to  chinese  in order to\n  properly tokenize the query terms. \n  Defaults to English. If an unsupported language is sent, the command returns an error.\n  See FT.ADD for the list of languages.    EXPANDER {expander} : If set, we will use a custom query expander instead of the stemmer.  See Extensions .   SCORER {scorer} : If set, we will use a custom scoring function defined by the user.  See Extensions .   PAYLOAD {payload} : Add an arbitrary, binary safe payload that will be exposed to custom scoring \n  functions.  See Extensions .    SORTBY {field} [ASC|DESC] : If specified, and field is a  sortable field , the results \n  are ordered by the value of this field. This applies to both text and numeric fields.   LIMIT first num : If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_5", 
            "text": "O(n) for single word queries (though for popular words we save a cache of the top 50 results).  Complexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_5", 
            "text": "Array reply,  where the first element is the total number of results, and then pairs of document id, and a nested array of field/value.   If  NOCONTENT  was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftaggregate", 
            "text": "", 
            "title": "FT.AGGREGATE"
        }, 
        {
            "location": "/Commands/#format_6", 
            "text": "FT.AGGREGATE  {index_name}\n  {query_string}\n  [WITHSCHEMA] [VERBATIM]\n  [LOAD {nargs} {property} ...]\n  [GROUPBY {nargs} {property} ...\n    REDUCE {func} {nargs} {arg} ... [AS {name:string}]\n    ...\n  ] ...\n  [SORTBY {nargs} {property} [ASC|DESC] ... [MAX {num}]]\n  [APPLY {expr} AS {alias}] ...\n  [LIMIT {offset} {num}] ...\n  [FILTER {expr}] ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_6", 
            "text": "Runs a search query on an index, and performs aggregate transformations on the results, extracting statistics etc from them. See  the full documentation on aggregations  for further details.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_6", 
            "text": "index_name : The index the query is executed against.    query_string : The base filtering query that retrieves the documents. It follows\n   the exact same syntax  as the search query, including filters, unions, not, optional, etc.    LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be \n  avoided as a general rule of thumb. Fields needed for aggregations should be stored as  SORTABLE , \n  where they are available to the aggregation pipeline with very load latency. LOAD hurts the \n  performance of aggregate queries considerably, since every processed record needs to execute the \n  equivalent of HMGET against a Redis key, which when executed over millions of keys, amounts to very \n  high processing times.     GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. \n  Each group should have at least one reducer (See below), a function that handles the group entries, \n  either counting them, or performing multiple aggregate operations (see below).    REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers.   The reducers can have their own property names using the  AS {name}  optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property  @foo , the resulting name will be  count_distinct(@foo) .       SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY,\n  using a list of properties. By default, sorting is ascending, but  ASC  or  DESC  can be added for \n  each property.  nargs  is the number of sorting parameters, including ASC and DESC. for example: \n   SORTBY 4 @foo ASC @bar DESC .   MAX  is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to  LIMIT , you usually need just  SORTBY \u2026 MAX  for common queries.     APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either \n  store the result as a new property down the pipeline, or replace any property using this \n  transformation.  expr  is an expression that can be used to perform arithmetic operations on numeric \n  properties, or functions that can be applied on properties depending on their types (see below), or \n  any combination thereof. For example:  APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz  will evaluate this \n  expression dynamically for each record in the pipeline and store the result as a new property called \n  baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the \n  pipeline.     LIMIT {offset} {num} . Limit the number of results to return just  num  results starting at index \n   offset  (zero-based). AS mentioned above, it is much more efficient to use  SORTBY \u2026 MAX  if you \n  are interested in just limiting the output of a sort operation.  However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by  SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as  SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting  all  the records and then paging over results 50-100.     FILTER {expr} . Filter the results using predicate expressions relating to values in each result. \n  They are is applied post-query and relate to the current state of the pipeline.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_6", 
            "text": "Non-deterministic. Depends on the query and aggregations performed, but it is usually linear to the number of results returned.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_6", 
            "text": "Array Response. Each row is an array and represents a single aggregate result.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#example\"_\"output", 
            "text": "Here we are counting GitHub events by user (actor), to produce the most active users:  127.0.0.1:6379  FT.AGGREGATE gh  *  GROUPBY 1 @actor REDUCE COUNT 0 AS num SORTBY 2 @num DESC MAX 10\n 1) (integer) 284784\n 2) 1)  actor \n    2)  lombiqbot \n    3)  num \n    4)  22197 \n 3) 1)  actor \n    2)  codepipeline-test \n    3)  num \n    4)  17746 \n 4) 1)  actor \n    2)  direwolf-github \n    3)  num \n    4)  10683 \n 5) 1)  actor \n    2)  ogate \n    3)  num \n    4)  6449 \n 6) 1)  actor \n    2)  openlocalizationtest \n    3)  num \n    4)  4759 \n 7) 1)  actor \n    2)  digimatic \n    3)  num \n    4)  3809 \n 8) 1)  actor \n    2)  gugod \n    3)  num \n    4)  3512 \n 9) 1)  actor \n    2)  xdzou \n    3)  num \n    4)  3216 \n10) 1)  actor \n    2)  opstest \n    3)  num \n    4)  2863 \n11) 1)  actor \n    2)  jikker \n    3)  num \n    4)  2794 \n(0.59s)", 
            "title": "Example output"
        }, 
        {
            "location": "/Commands/#ftexplain", 
            "text": "", 
            "title": "FT.EXPLAIN"
        }, 
        {
            "location": "/Commands/#format_7", 
            "text": "FT.EXPLAIN {index} {query}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_7", 
            "text": "Returns the execution plan for a complex query.  In the returned response, a  +  on a term is an indication of stemming.   Example:  $ redis-cli --raw 127 .0.0.1:6379  FT.EXPLAIN rd  (foo bar)|(hello world) @date:[100 200]|@date:[500 +inf] \nINTERSECT  { \n  UNION  { \n    INTERSECT  { \n      foo\n      bar\n     } \n    INTERSECT  { \n      hello\n      world\n     } \n   } \n  UNION  { \n    NUMERIC  { 100 .000000  =  x  =   200 .000000 } \n    NUMERIC  { 500 .000000  =  x  =  inf } \n   }  }", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_7", 
            "text": "index : The index name. The index must be first created with FT.CREATE  query : The query string, as if sent to FT.SEARCH", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_7", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_7", 
            "text": "String Response. A string representing the execution plan (see above example).   Note : You should use  redis-cli --raw  to properly read line-breaks in the returned response.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdel", 
            "text": "", 
            "title": "FT.DEL"
        }, 
        {
            "location": "/Commands/#format_8", 
            "text": "FT.DEL {index} {doc_id} [DD]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_8", 
            "text": "Deletes a document from the index. Returns 1 if the document was in the index, or 0 if not.   After deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.   FT.DEL does not delete the actual document By default! Since RediSearch regards documents as separate entities to the index and allows things like adding existing documents or indexing without saving the document - by default FT.DEL only deletes the reference to the document from the index, not the actual Redis HASH key where the document is stored.   Specifying  DD  (Delete Document) after the document ID, will make RediSearch also delete the actual document  if it is in the index .  Alternatively, you can just send an extra  DEL {doc_id}  to redis and delete the document directly. You can run both of them in a MULTI transaction.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_8", 
            "text": "index : The index name. The index must be first created with FT.CREATE  doc_id : the id of the document to be deleted. It does not actually delete the HASH key in which \n  the document is stored. Use DEL to do that manually if needed.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_8", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_8", 
            "text": "Integer Reply: 1 if the document was deleted, 0 if not.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftget", 
            "text": "", 
            "title": "FT.GET"
        }, 
        {
            "location": "/Commands/#format_9", 
            "text": "FT.GET {index} {doc id}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_9", 
            "text": "Returns the full contents of a document.  Currently it is equivalent to HGETALL, but this is future-proof and will allow us to change the internal representation of documents inside Redis in the future. In addition, it allows simpler implementation of fetching documents in clustered mode.  If the document does not exist or is not a HASH object, we return a NULL reply", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_9", 
            "text": "index : The index name. The index must be first created with FT.CREATE  documentId : The id of the document as inserted to the index", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_9", 
            "text": "Array Reply: Key-value pairs of field names and values of the document", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftmget", 
            "text": "", 
            "title": "FT.MGET"
        }, 
        {
            "location": "/Commands/#format_10", 
            "text": "FT.MGET {index} {docId} ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_10", 
            "text": "Returns the full contents of multiple documents.   Currently it is equivalent to calling multiple HGETALL commands, although faster. \nThis command is also future-proof and will allow us to change the internal representation of documents inside Redis in the future. \nIn addition, it allows simpler implementation of fetching documents in clustered mode.  We return an array with exactly the same number of elements as the number of keys sent to the command.   Each element, in turn, is an array of key-value pairs representing the document.   If a document is not found or is not a valid HASH object, its place in the parent array is filled with a Null reply object.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_10", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  documentIds : The ids of the requested documents as inserted to the index", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_10", 
            "text": "Array Reply: An array with exactly the same number of elements as the number of keys sent to the command.  Each element in it is either an array representing the document or Null if it was not found.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdrop", 
            "text": "", 
            "title": "FT.DROP"
        }, 
        {
            "location": "/Commands/#format_11", 
            "text": "FT.DROP {index} [KEEPDOCS]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_11", 
            "text": "Deletes all the keys associated with the index.   By default, DROP deletes the document hashes as well, but adding the KEEPDOCS option keeps the documents in place, ready for re-indexing.  If no other data is on the Redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_11", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  KEEPDOCS : If set, the drop operation will not delete the actual document hashes.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_11", 
            "text": "Status Reply: OK on success.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#fttagvals", 
            "text": "", 
            "title": "FT.TAGVALS"
        }, 
        {
            "location": "/Commands/#format_12", 
            "text": "FT.TAGVALS {index} {field_name}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_12", 
            "text": "Returns the distinct tags indexed in a  Tag field .   This is useful if your tag field indexes things like cities, categories, etc.   Limitations  There is no paging or sorting, the tags are not alphabetically sorted.   This command only operates on  Tag fields .    The strings return lower-cased and stripped of whitespaces, but otherwise unchanged.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_12", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  filed_name : The name of a Tag file defined in the schema.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_12", 
            "text": "Array Reply: All the distinct tags in the tag index.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#complexity_9", 
            "text": "O(n), n being the cardinality of the tag field.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#ftsugadd", 
            "text": "", 
            "title": "FT.SUGADD"
        }, 
        {
            "location": "/Commands/#format_13", 
            "text": "FT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_13", 
            "text": "Adds a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestions dictionaries to the user.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_13", 
            "text": "key : the suggestion dictionary key.  string : the suggestion string we index  score : a floating point number of the suggestion string's weight  INCR : if set, we increment the existing entry of the suggestion by the given score, instead of \n  replacing the score. This is useful for updating the dictionary based on user queries in real time  PAYLOAD {payload} : If set, we save an extra payload with the suggestion, that can be fetched by \n  adding the  WITHPAYLOADS  argument to  FT.SUGGET .", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_13", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsugget", 
            "text": "", 
            "title": "FT.SUGGET"
        }, 
        {
            "location": "/Commands/#format_14", 
            "text": "FT . SUGGET   { key }   { prefix }   [ FUZZY ]   [ WITHSCORES ]   [ WITHPAYLOADS ]   [ MAX   num ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_14", 
            "text": "Gets completion suggestions for a prefix.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_14", 
            "text": "key : the suggestion dictionary key.  prefix : the prefix to complete on  FUZZY : if set, we do a fuzzy prefix search, including prefixes at Levenshtein distance of 1 from \n  the prefix sent  MAX num : If set, we limit the results to a maximum of  num  (default: 5).  WITHSCORES : If set, we also return the score of each suggestion. this can be used to merge \n  results from multiple instances  WITHPAYLOADS : If set, we return optional payloads saved along with the suggestions. If no \n  payload is present for an entry, we return a Null Reply.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_14", 
            "text": "Array Reply: a list of the top suggestions matching the prefix, optionally with score after each entry", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsugdel", 
            "text": "", 
            "title": "FT.SUGDEL"
        }, 
        {
            "location": "/Commands/#format_15", 
            "text": "FT.SUGDEL {key} {string}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_15", 
            "text": "Deletes a string from a suggestion index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_15", 
            "text": "key : the suggestion dictionary key.  string : the string to delete", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_15", 
            "text": "Integer Reply: 1 if the string was found and deleted, 0 otherwise.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsuglen", 
            "text": "", 
            "title": "FT.SUGLEN"
        }, 
        {
            "location": "/Commands/#format_16", 
            "text": "FT.SUGLEN {key}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_16", 
            "text": "Gets the size of an auto-complete suggestion dictionary", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_16", 
            "text": "key : the suggestion dictionary key.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_16", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftoptimize", 
            "text": "This command is deprecated  Index optimizations are done by the internal garbage collector in the background. Client libraries should not implement this command and remove it if they haven't already.", 
            "title": "FT.OPTIMIZE"
        }, 
        {
            "location": "/Commands/#format_17", 
            "text": "FT.OPTIMIZE {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_17", 
            "text": "This command is deprecated.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#ftsynadd", 
            "text": "", 
            "title": "FT.SYNADD"
        }, 
        {
            "location": "/Commands/#format_18", 
            "text": "FT.SYNADD  index name   term1   term2  ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_18", 
            "text": "Adds a synonym group.  The command is used to create a new synonyms group. The command returns the synonym group id which can later be used to add additional terms to that synonym group. Only documents which was indexed after the adding operation will be effected.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#ftsynupdate", 
            "text": "", 
            "title": "FT.SYNUPDATE"
        }, 
        {
            "location": "/Commands/#format_19", 
            "text": "FT.SYNUPDATE  index name   synonym group id   term1   term2  ...", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_19", 
            "text": "Updates a synonym group.  The command is used to update an existing synonym group with additional terms. Only documents which was indexed after the update will be effected.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#ftsyndump", 
            "text": "", 
            "title": "FT.SYNDUMP"
        }, 
        {
            "location": "/Commands/#format_20", 
            "text": "FT.SYNDUMP  index name", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_20", 
            "text": "Dumps the contents of a synonym group.  The command is used to dump the synonyms data structure. Returns a list of synonym terms and their synonym group ids.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#ftspellcheck", 
            "text": "", 
            "title": "FT.SPELLCHECK"
        }, 
        {
            "location": "/Commands/#format_21", 
            "text": "FT . SPELLCHECK   { index }   { query } \n     [ DISTANCE   dist ] \n     [ TERMS   { INCLUDE   |   EXCLUDE }   { dict }   [ TERMS   ...]]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_21", 
            "text": "Performs spelling correction on a query, returning suggestions for misspelled terms.  See  Query Spelling Correction  for more details.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_17", 
            "text": "index : the index with the indexed terms.    query : the search query.    TERMS : specifies an inclusion ( INCLUDE ) or exclusion ( EXCLUDE ) custom dictionary named  {dict} . Refer to  FT.DICTADD ,  FT.DICTDEL  and  FT.DICTDUMP  for managing custom dictionaries.    DISTANCE : the maximal Levenshtein distance for spelling suggestions (default: 1, max: 4).", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_17", 
            "text": "An array, in which each element represents a misspelled term from the query. The misspelled terms are ordered by their order of appearance in the query.  Each misspelled term, in turn, is a 3-element array consisting of the constant string \"TERM\", the term itself and an array of suggestions for spelling corrections.  Each element in the spelling corrections array consists of the score of the suggestion and the suggestion itself. The suggestions array, per misspelled term, is ordered in descending order by score.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#example\"_\"output_1", 
            "text": "1)  1)  TERM \n    2)  {term1} \n    3)  1)  1)   {score1} \n            2)   {suggestion1} \n        2)  1)   {score2} \n            2)   {suggestion2} \n        .\n        .\n        .\n2)  1)  TERM \n    2)  {term2} \n    3)  1)  1)   {score1} \n            2)   {suggestion1} \n        2)  1)   {score2} \n            2)   {suggestion2} \n        .\n        .\n        .\n.\n.\n.", 
            "title": "Example output"
        }, 
        {
            "location": "/Commands/#ftdictadd", 
            "text": "", 
            "title": "FT.DICTADD"
        }, 
        {
            "location": "/Commands/#format_22", 
            "text": "FT.DICTADD {dict} {term} [{term} ...]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_22", 
            "text": "Adds terms to a dictionary.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_18", 
            "text": "dict : the dictionary name.    term : the term to add to the dictionary.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_18", 
            "text": "Returns int, specifically the number of new terms that were added.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdictdel", 
            "text": "", 
            "title": "FT.DICTDEL"
        }, 
        {
            "location": "/Commands/#format_23", 
            "text": "FT.DICTDEL {dict} {term} [{term} ...]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_23", 
            "text": "Deletes terms from a dictionary.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_19", 
            "text": "dict : the dictionary name.    term : the term to delete from the dictionary.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_19", 
            "text": "Returns int, specifically the number of terms that were deleted.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdictdump", 
            "text": "", 
            "title": "FT.DICTDUMP"
        }, 
        {
            "location": "/Commands/#format_24", 
            "text": "FT.DICTDUMP {dict}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_24", 
            "text": "Dumps all terms in the given dictionary.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_20", 
            "text": "dict : the dictionary name.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_20", 
            "text": "Returns an array, where each element is term (string).", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftconfig", 
            "text": "", 
            "title": "FT.CONFIG"
        }, 
        {
            "location": "/Commands/#format_25", 
            "text": "FT.CONFIG  GET|HELP  {option}\n  FT.CONFIG SET {option} {value}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_25", 
            "text": "Retrieves, describes and sets runtime configuration options.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_21", 
            "text": "option : the name of the configuration option, or '*' for all.  value : a value for the configuration option.   For details about the configuration options refer to  Configuring .  Setting values in runtime is supported for these configuration options:   NOGC  MINPREFIX  MAXEXPANSIONS  TIMEOUT  ON_TIMEOUT  MIN_PHONETIC_TERM_LEN", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_21", 
            "text": "When provided with a valid option name, the  GET  subcommand returns a string with the current option's value. An array containing an array for each configuration option, consisting of the option's name and current value, is returned when '*' is provided.  The  SET  subcommand returns 'OK' for valid runtime-settable option names and values.", 
            "title": "Returns"
        }, 
        {
            "location": "/Configuring/", 
            "text": "Run-time configuration\n\n\nRediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added. \n\n\nPassing Configuration Options During Loading\n\n\nIn general, passing configuration options is done by appending arguments after the \n--loadmodule\n argument in the command line, \nloadmodule\n configuration directive in a Redis config file, or the \nMODULE LOAD\n command. For example:\n\n\nIn redis.conf:\n\n\nloadmodule redisearch.so OPT1 OPT2\n\n\n\n\n\nFrom redis-cli:\n\n\n127.0.0.6379\n MODULE load redisearch.so OPT1 OPT2\n\n\n\n\n\nFrom command line:\n\n\n$ redis-server --loadmodule ./redisearch.so OPT1 OPT2\n\n\n\n\n\nSetting Configuration Options In Run-Time\n\n\nAs of v1.4.1, the \nFT.CONFIG\n allows setting some options during runtime. In addition, the command can be used to view the current run-time configuration options.\n\n\nRediSearch configuration options\n\n\nTIMEOUT\n\n\nThe maximum amount of time \nin milliseconds\n that a search query is allowed to run. If this time is exceeded we return the top results accumulated so far, or an error depending on the policy set with \nON_TIMEOUT\n. The timeout can be disabled by setting it to 0.\n\n\n\n\nNote\n\n\nThis works only in concurrent mode, so enabling \nSAFEMODE\n disables this option.\n\n\n\n\nDefault\n\n\n500\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so TIMEOUT \n100\n\n\n\n\n\n\n\n\nON_TIMEOUT {policy}\n\n\nThe response policy for queries that exceed the \nTIMEOUT\n setting.\n\n\nThe policy can be one of the following:\n\n\n\n\nRETURN\n: this policy will return the top results accumulated by the query until it timed out.\n\n\nFAIL\n: will return an error when the query exeeds the timeout value.\n\n\n\n\nDefault\n\n\nRETURN\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so ON_TIMEOUT fail\n\n\n\n\n\n\n\nSAFEMODE\n\n\nIf present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.\n\n\nThis is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily inconsistent results (i.e. documents that were valid during the invocation of the query are not returned because they were deleted during query processing).\n\n\nDefault\n\n\nOff (not present)\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so SAFEMODE\n\n\n\n\n\n\n\nEXTLOAD {file_name}\n\n\nIf present, we try to load a RediSearch extension dynamic library from the specified file path. See \nExtensions\n for details.\n\n\nDefault\n\n\nNone\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so\n\n\n\n\n\n\n\nNOGC\n\n\nIf set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.\n\n\nDefault\n\n\nNot set\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so NOGC\n\n\n\n\n\n\n\nMINPREFIX\n\n\nThe minimum number of characters we allow for prefix queries (e.g. \nhel*\n). Setting it to 1 can hurt performance.\n\n\nDefault\n\n\n2\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so MINPREFIX \n3\n\n\n\n\n\n\n\n\nMAXEXPANSIONS\n\n\nThe maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues.\n\n\nDefault\n\n\n200\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS \n1000\n\n\n\n\n\n\n\n\nMAXDOCTABLESIZE\n\n\nThe maximum size of the internal hash table used for storing the documents.\n\n\nDefault\n\n\n1000000\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so MAXDOCTABLESIZE \n3000000\n\n\n\n\n\n\n\n\nFRISOINI {file_name}\n\n\nIf present, we load the custom Chinese dictionary from the specified path. See \nUsing custom dictionaries\n for more details.\n\n\nDefault\n\n\nNot set\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so FRISOINI /opt/dict/friso.ini\n\n\n\n\n\n\n\nGC_SCANSIZE\n\n\nThe garbage collection bulk size of the internal gc used for cleaning up the indexes.\n\n\nDefault\n\n\n100\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so GC_SCANSIZE \n10\n\n\n\n\n\n\n\n\nGC_POLICY\n\n\nThe policy for the garbage collector. Supported policies are:\n\n\n\n\nDEFAULT\n: the default policy.\n\n\nFORK\n: uses a forked thread for garbage collection (v1.4.1 and above).\n\n\n\n\n\n\nThe \nFORK\n garbage collection policy is considered an experimental feature, and should be used responsibly.\n\n\n\n\nDefault\n\n\n\"default\"\n\n\nExample\n\n\n$ redis-server --loadmodule ./redisearch.so GC_POLICY DEFAULT", 
            "title": "Configuration"
        }, 
        {
            "location": "/Configuring/#run-time\"_\"configuration", 
            "text": "RediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added.", 
            "title": "Run-time configuration"
        }, 
        {
            "location": "/Configuring/#passing\"_\"configuration\"_\"options\"_\"during\"_\"loading", 
            "text": "In general, passing configuration options is done by appending arguments after the  --loadmodule  argument in the command line,  loadmodule  configuration directive in a Redis config file, or the  MODULE LOAD  command. For example:  In redis.conf:  loadmodule redisearch.so OPT1 OPT2  From redis-cli:  127.0.0.6379  MODULE load redisearch.so OPT1 OPT2  From command line:  $ redis-server --loadmodule ./redisearch.so OPT1 OPT2", 
            "title": "Passing Configuration Options During Loading"
        }, 
        {
            "location": "/Configuring/#setting\"_\"configuration\"_\"options\"_\"in\"_\"run-time", 
            "text": "As of v1.4.1, the  FT.CONFIG  allows setting some options during runtime. In addition, the command can be used to view the current run-time configuration options.", 
            "title": "Setting Configuration Options In Run-Time"
        }, 
        {
            "location": "/Configuring/#redisearch\"_\"configuration\"_\"options", 
            "text": "", 
            "title": "RediSearch configuration options"
        }, 
        {
            "location": "/Configuring/#timeout", 
            "text": "The maximum amount of time  in milliseconds  that a search query is allowed to run. If this time is exceeded we return the top results accumulated so far, or an error depending on the policy set with  ON_TIMEOUT . The timeout can be disabled by setting it to 0.   Note  This works only in concurrent mode, so enabling  SAFEMODE  disables this option.", 
            "title": "TIMEOUT"
        }, 
        {
            "location": "/Configuring/#default", 
            "text": "500", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example", 
            "text": "$ redis-server --loadmodule ./redisearch.so TIMEOUT  100", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#on\"_\"timeout\"_\"policy", 
            "text": "The response policy for queries that exceed the  TIMEOUT  setting.  The policy can be one of the following:   RETURN : this policy will return the top results accumulated by the query until it timed out.  FAIL : will return an error when the query exeeds the timeout value.", 
            "title": "ON_TIMEOUT {policy}"
        }, 
        {
            "location": "/Configuring/#default_1", 
            "text": "RETURN", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_1", 
            "text": "$ redis-server --loadmodule ./redisearch.so ON_TIMEOUT fail", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#safemode", 
            "text": "If present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.  This is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily inconsistent results (i.e. documents that were valid during the invocation of the query are not returned because they were deleted during query processing).", 
            "title": "SAFEMODE"
        }, 
        {
            "location": "/Configuring/#default_2", 
            "text": "Off (not present)", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_2", 
            "text": "$ redis-server --loadmodule ./redisearch.so SAFEMODE", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#extload\"_\"file\"_\"name", 
            "text": "If present, we try to load a RediSearch extension dynamic library from the specified file path. See  Extensions  for details.", 
            "title": "EXTLOAD {file_name}"
        }, 
        {
            "location": "/Configuring/#default_3", 
            "text": "None", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_3", 
            "text": "$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#nogc", 
            "text": "If set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.", 
            "title": "NOGC"
        }, 
        {
            "location": "/Configuring/#default_4", 
            "text": "Not set", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_4", 
            "text": "$ redis-server --loadmodule ./redisearch.so NOGC", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#minprefix", 
            "text": "The minimum number of characters we allow for prefix queries (e.g.  hel* ). Setting it to 1 can hurt performance.", 
            "title": "MINPREFIX"
        }, 
        {
            "location": "/Configuring/#default_5", 
            "text": "2", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_5", 
            "text": "$ redis-server --loadmodule ./redisearch.so MINPREFIX  3", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#maxexpansions", 
            "text": "The maximum number of expansions we allow for query prefixes. Setting it too high can cause performance issues.", 
            "title": "MAXEXPANSIONS"
        }, 
        {
            "location": "/Configuring/#default_6", 
            "text": "200", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_6", 
            "text": "$ redis-server --loadmodule ./redisearch.so MAXEXPANSIONS  1000", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#maxdoctablesize", 
            "text": "The maximum size of the internal hash table used for storing the documents.", 
            "title": "MAXDOCTABLESIZE"
        }, 
        {
            "location": "/Configuring/#default_7", 
            "text": "1000000", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_7", 
            "text": "$ redis-server --loadmodule ./redisearch.so MAXDOCTABLESIZE  3000000", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#frisoini\"_\"file\"_\"name", 
            "text": "If present, we load the custom Chinese dictionary from the specified path. See  Using custom dictionaries  for more details.", 
            "title": "FRISOINI {file_name}"
        }, 
        {
            "location": "/Configuring/#default_8", 
            "text": "Not set", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_8", 
            "text": "$ redis-server --loadmodule ./redisearch.so FRISOINI /opt/dict/friso.ini", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#gc\"_\"scansize", 
            "text": "The garbage collection bulk size of the internal gc used for cleaning up the indexes.", 
            "title": "GC_SCANSIZE"
        }, 
        {
            "location": "/Configuring/#default_9", 
            "text": "100", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_9", 
            "text": "$ redis-server --loadmodule ./redisearch.so GC_SCANSIZE  10", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#gc\"_\"policy", 
            "text": "The policy for the garbage collector. Supported policies are:   DEFAULT : the default policy.  FORK : uses a forked thread for garbage collection (v1.4.1 and above).    The  FORK  garbage collection policy is considered an experimental feature, and should be used responsibly.", 
            "title": "GC_POLICY"
        }, 
        {
            "location": "/Configuring/#default_10", 
            "text": "\"default\"", 
            "title": "Default"
        }, 
        {
            "location": "/Configuring/#example_10", 
            "text": "$ redis-server --loadmodule ./redisearch.so GC_POLICY DEFAULT", 
            "title": "Example"
        }, 
        {
            "location": "/Administration/", 
            "text": "RediSearch Administration Guide\n\n\nRediSearch doesn't require any configuration to work, but there are a few things worth noting when running RediSearch on top of Redis.\n\n\nPersistence\n\n\nRediSearch supports both RDB and AOF based persistence. For a pure RDB set-up, nothing special is needed beyond the standard Redis RDB configuration.\n\n\nAOF Persistence\n\n\nWhile RediSearch supports working with AOF based persistence, as of version 1.1.0 it \ndoes not support\n \"classic AOF\" mode, which uses AOF rewriting. Instead, it only supports AOF with RDB preamble mode. In this mode, rewriting the AOF log just creates an RDB file, which is appended to. \n\n\nTo enable AOF persistence with RediSearch, add the two following lines to your redis.conf:\n\n\nappendonly yes\naof-use-rdb-preamble yes\n\n\n\n\n\nMaster/Slave Replication\n\n\nRediSearch supports replication inherently, and using a master/slave set-up, you can use slaves for high availability. On top of that, slaves can be used for searching, to load-balance read traffic. \n\n\nCluster Support\n\n\nRediSearch will not work correctly on a cluster. The enterprise version of RediSearch, which is commercially available from Redis Labs, does support a cluster set up and scales to hundreds of nodes, billions of documents and terabytes of data. See the \nRedis Labs Website\n for more details.", 
            "title": "Administration"
        }, 
        {
            "location": "/Administration/#redisearch\"_\"administration\"_\"guide", 
            "text": "RediSearch doesn't require any configuration to work, but there are a few things worth noting when running RediSearch on top of Redis.", 
            "title": "RediSearch Administration Guide"
        }, 
        {
            "location": "/Administration/#persistence", 
            "text": "RediSearch supports both RDB and AOF based persistence. For a pure RDB set-up, nothing special is needed beyond the standard Redis RDB configuration.", 
            "title": "Persistence"
        }, 
        {
            "location": "/Administration/#aof\"_\"persistence", 
            "text": "While RediSearch supports working with AOF based persistence, as of version 1.1.0 it  does not support  \"classic AOF\" mode, which uses AOF rewriting. Instead, it only supports AOF with RDB preamble mode. In this mode, rewriting the AOF log just creates an RDB file, which is appended to.   To enable AOF persistence with RediSearch, add the two following lines to your redis.conf:  appendonly yes\naof-use-rdb-preamble yes", 
            "title": "AOF Persistence"
        }, 
        {
            "location": "/Administration/#masterslave\"_\"replication", 
            "text": "RediSearch supports replication inherently, and using a master/slave set-up, you can use slaves for high availability. On top of that, slaves can be used for searching, to load-balance read traffic.", 
            "title": "Master/Slave Replication"
        }, 
        {
            "location": "/Administration/#cluster\"_\"support", 
            "text": "RediSearch will not work correctly on a cluster. The enterprise version of RediSearch, which is commercially available from Redis Labs, does support a cluster set up and scales to hundreds of nodes, billions of documents and terabytes of data. See the  Redis Labs Website  for more details.", 
            "title": "Cluster Support"
        }, 
        {
            "location": "/Query_Syntax/", 
            "text": "Search Query Syntax\n\n\nWe support a simple syntax for complex queries with the following rules:\n\n\n\n\nMulti-word phrases simply a list of tokens, e.g. \nfoo bar baz\n, and imply intersection (AND) of the terms.\n\n\nExact phrases are wrapped in quotes, e.g \n\"hello world\"\n.\n\n\nOR Unions (i.e \nword1 OR word2\n), are expressed with a pipe (\n|\n), e.g. \nhello|hallo|shalom|hola\n.\n\n\nNOT negation (i.e. \nword1 NOT word2\n) of expressions or sub-queries. e.g. \nhello -world\n. As of version 0.19.3, purely negative queries (i.e. \n-foo\n or \n-@title:(foo|bar)\n) are supported.\n\n\nPrefix matches (all terms starting with a prefix) are expressed with a \n*\n. For performance reasons, a minimum prefix length is enforced (2 by default, but is configurable)\n\n\nA special \"wildcard query\" that returns all results in the index - \n*\n (cannot be combined with anything else).\n\n\nSelection of specific fields using the syntax \n@field:hello world\n.\n\n\nNumeric Range matches on numeric fields with the syntax \n@field:[{min} {max}]\n.\n\n\nGeo radius matches on geo fields with the syntax \n@field:[{lon} {lat} {radius} {m|km|mi|ft}]\n\n\nTag field filters with the syntax \n@field:{tag | tag | ...}\n. See the full documentation on [tag fields|/Tags].\n\n\nOptional terms or clauses: \nfoo ~bar\n means bar is optional but documents with bar in them will rank higher.\n\n\nFuzzy matching on terms (as of v1.2.0): \n%hello%\n means all terms with Levenshtein distance of 1 from it.\n\n\nAn expression in a query can be wrapped in parentheses to disambiguate, e.g. \n(hello|hella) (world|werld)\n.\n\n\nQuery attributes can be applied to individual clauses, e.g. \n(foo bar) =\n { $weight: 2.0; $slop: 1; $inorder: false; }\n\n\nCombinations of the above can be used together, e.g \nhello (world|foo) \"bar baz\" bbbb\n\n\n\n\nPure negative queries\n\n\nAs of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g. \n-hello\n or \n-(@title:foo|bar)\n. The results will be all the documents \nNOT\n containing the query terms.\n\n\n\n\nWarning\n\n\nAny complex expression can be negated this way, however, caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.\n\n\n\n\nField modifiers\n\n\nAs of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword.\n\n\nPer query expression or sub-expression, it is possible to specify which fields it matches, by prepending the expression with the \n@\n symbol, the field name and a \n:\n (colon) symbol.\n\n\nIf a field modifier precedes multiple words, they are considered to be a phrase with the same modifier.\n\n\nIf a field modifier precedes an expression in parentheses, it applies only to the expression inside the parentheses.\n\n\nMultiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:\n\n\nFT.SEARCH cars \n@country:korea @engine:(diesel|hybrid) @class:suv\n\n\n\n\n\n\nMultiple modifiers can be applied to the same term or grouped terms. e.g.:\n\n\nFT.SEARCH idx \n@title|body:(hello world) @url|image:mydomain\n\n\n\n\n\n\nThis will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.\n\n\nNumeric filters in query\n\n\nIf a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the query. The syntax is \n@field:[{min} {max}]\n - e.g. \n@price:[100 200]\n.\n\n\nA few notes on numeric predicates\n\n\n\n\n\n\nIt is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.\n\n\n\n\n\n\nIt is possible to intersect or union multiple numeric filters in the same query, be it for the same field or different ones.\n\n\n\n\n\n\n-inf\n, \ninf\n and \n+inf\n are acceptable numbers in a range. Thus greater-than 100 is expressed as \n[(100 inf]\n.\n\n\n\n\n\n\nNumeric filters are inclusive. Exclusive min or max are expressed with \n(\n prepended to the number, e.g. \n[(100 (200]\n.\n\n\n\n\n\n\nIt is possible to negate a numeric filter by prepending a \n-\n sign to the filter, e.g. returning a result where price differs from 100 is expressed as: \n@title:foo -@price:[100 100]\n.\n\n\n\n\n\n\nTag filters\n\n\nRediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax:\n\n\n@field:{ tag | tag | ...}\n\ne.g.\n\n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\nTags can have multiple words or include other punctuation marks other than the field's separator (\n,\n by default). Punctuation marks in tags should be escaped with a backslash (\n\\\n). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.\n\n\nNotice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing \nall\n tags, you should repeat the tag filter several times, e.g.:\n\n\n# This will return all documents containing all three cities as tags:\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n# This will return all documents containing either city:\n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\nTag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc.\n\n\nGeo filters in query\n\n\nAs of version 0.21, it is possible to add geo radius queries directly into the query language  with the syntax \n@field:[{lon} {lat} {radius} {m|km|mi|ft}]\n. This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own \nGEORADIUS\n command for more details as it is used internally for that).\n\n\nRadius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as: \nchinese restaurant @location:[-122.41 37.77 5 km]\n.\n\n\nPrefix matching\n\n\nOn index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending \n*\n to a prefix token. For example:\n\n\nhel* world\n\n\n\n\n\nWill be expanded to cover \n(hello|help|helm|...) world\n.\n\n\nA few notes on prefix searches\n\n\n\n\n\n\nAs prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffixes.\n\n\n\n\n\n\nAs a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:\n\n\n\n\n\n\nPrefixes are limited to 2 letters or more. You can change this number by using the \nMINPREFIX\n setting on the module command line.\n\n\n\n\n\n\nExpansion is limited to 200 terms or less. You can change this number by using the \nMAXEXPANSIONS\n setting on the module command line.\n\n\n\n\n\n\nPrefix matching fully supports Unicode and is case insensitive.\n\n\n\n\n\n\nCurrently, there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap.\n\n\n\n\n\n\nFuzzy matching\n\n\nAs of v1.2.0, the dictionary of all terms in the index can also be used to perform \nFuzzy Matching\n. Fuzzy matches are performed based on \nLevenshtein distance\n (LD). Fuzzy matching on a term is performed by surrounding the term with '%', for example:\n\n\n%hello\n%\n \nworld\n\n\n\n\n\n\nWill perform fuzzy matching on 'hello' for all terms where LD is 1.\n\n\nAs of v1.4.0, the LD of the fuzzy match can be set by the number of '%' surrounding it, so that \n%%hello%%\n will perform fuzzy matching on 'hello' for all terms where LD is 2.\n\n\nThe maximal LD for fuzzy matching is 3.\n\n\nWildcard queries\n\n\nAs of version 1.1.0, we provide a special query to retrieve all the documents in an index. This is meant mostly for the aggregation engine. You can call it by specifying only a single star sign as the query string - i.e. \nFT.SEARCH myIndex *\n.\n\n\nThis cannot be combined with any other filters, field modifiers or anything inside the query. It is technically possible to use the deprecated FILTER and GEOFILTER request parameters outside the query string in conjunction with a wildcard, but this makes the wildcard meaningless and only hurts performance.\n\n\nQuery attributes\n\n\nAs of version 1.2.0, it is possible to apply specific query modifying attributes to specific clauses of the query.\n\n\nThe syntax is \n(foo bar) =\n { $attribute: value; $attribute:value; ...}\n, e.g:\n\n\n(\nfoo\n \nbar\n)\n \n=\n \n{\n \n$\nweight\n:\n \n2.0\n;\n \n$\nslop\n:\n \n1\n;\n \n$\ninorder\n:\n \ntrue\n;\n \n}\n\n\n~(\nbar\n \nbaz\n)\n \n=\n \n{\n \n$\nweight\n:\n \n0.5\n;\n \n}\n\n\n\n\n\n\nThe supported attributes are:\n\n\n\n\n$weight\n: determines the weight of the sub-query or token in the overall ranking on the result (default: 1.0).\n\n\n$slop\n: determines the maximum allowed \"slop\" (space between terms) in the query clause (default: 0).\n\n\n$inorder\n: whether or not the terms in a query clause must appear in the same order as in the query, usually set alongside with \n$slop\n (default: false).\n\n\n$phonetic\n: whether or not to perform phonetic matching (default: true). Note: setting this attribute on for fields which were not creates as \nPHONETIC\n will produce an error.\n\n\n\n\nA few query examples\n\n\n\n\n\n\nSimple phrase query - hello AND world\n\n\nhello world\n\n\n\n\n\n\n\n\n\nExact phrase query - \nhello\n FOLLOWED BY \nworld\n\n\nhello world\n\n\n\n\n\n\n\n\n\n\nUnion: documents containing either \nhello\n OR \nworld\n\n\nhello|world\n\n\n\n\n\n\n\n\n\nNot: documents containing \nhello\n but not \nworld\n\n\nhello -world\n\n\n\n\n\n\n\n\n\nIntersection of unions\n\n\n(hello|halo) (world|werld)\n\n\n\n\n\n\n\n\n\nNegation of union\n\n\nhello -(world|werld)\n\n\n\n\n\n\n\n\n\nUnion inside phrase\n\n\n(barack|barrack) obama\n\n\n\n\n\n\n\n\n\nOptional terms with higher priority to ones containing more matches:\n\n\nobama ~barack ~michelle\n\n\n\n\n\n\n\n\n\nExact phrase in one field, one word in another field:\n\n\n@title:\nbarack obama\n @job:president\n\n\n\n\n\n\n\n\n\nCombined AND, OR with field specifiers:\n\n\n@title:hello world @body:(foo bar) @category:(articles|biographies)\n\n\n\n\n\n\n\n\n\nPrefix Queries:\n\n\nhello worl*\n\nhel* worl*\n\nhello -worl*\n\n\n\n\n\n\n\n\n\nNumeric Filtering - products named \"tv\" with a price range of 200-500:\n\n\n@name:tv @price:[200 500]\n\n\n\n\n\n\n\n\n\nNumeric Filtering - users with age greater than 18:\n\n\n@age:[(18 +inf]\n\n\n\n\n\n\n\n\n\nMapping common SQL predicates to RediSearch\n\n\n\n\n\n\n\n\nSQL Condition\n\n\nRediSearch Equivalent\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nWHERE x='foo' AND y='bar'\n\n\n@x:foo @y:bar\n\n\nfor less ambiguity use (@x:foo) (@y:bar)\n\n\n\n\n\n\nWHERE x='foo' AND y!='bar'\n\n\n@x:foo -@y:bar\n\n\n\n\n\n\n\n\nWHERE x='foo' OR y='bar'\n\n\n(@x:foo)|(@y:bar)\n\n\n\n\n\n\n\n\nWHERE x IN ('foo', 'bar','hello world')\n\n\n@x:(foo|bar|\"hello world\")\n\n\nquotes mean exact phrase\n\n\n\n\n\n\nWHERE y='foo' AND x NOT IN ('foo','bar')\n\n\n@y:foo (-@x:foo) (-@x:bar)\n\n\n\n\n\n\n\n\nWHERE x NOT IN ('foo','bar')\n\n\n-@x:(foo|bar)\n\n\n\n\n\n\n\n\nWHERE num BETWEEN 10 AND 20\n\n\n@num:[10 20]\n\n\n\n\n\n\n\n\nWHERE num \n= 10\n\n\n@num:[10 +inf]\n\n\n\n\n\n\n\n\nWHERE num \n 10\n\n\n@num:[(10 +inf]\n\n\n\n\n\n\n\n\nWHERE num \n 10\n\n\n@num:[-inf (10]\n\n\n\n\n\n\n\n\nWHERE num \n= 10\n\n\n@num:[-inf 10]\n\n\n\n\n\n\n\n\nWHERE num \n 10 OR num \n 20\n\n\n@num:[-inf (10] | @num:[(20 +inf]\n\n\n\n\n\n\n\n\nWHERE name LIKE 'john%'\n\n\n@name:john*\n\n\n\n\n\n\n\n\n\n\nTechnical note\n\n\nThe query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition \nat the git repo\n.", 
            "title": "Query Syntax"
        }, 
        {
            "location": "/Query_Syntax/#search\"_\"query\"_\"syntax", 
            "text": "We support a simple syntax for complex queries with the following rules:   Multi-word phrases simply a list of tokens, e.g.  foo bar baz , and imply intersection (AND) of the terms.  Exact phrases are wrapped in quotes, e.g  \"hello world\" .  OR Unions (i.e  word1 OR word2 ), are expressed with a pipe ( | ), e.g.  hello|hallo|shalom|hola .  NOT negation (i.e.  word1 NOT word2 ) of expressions or sub-queries. e.g.  hello -world . As of version 0.19.3, purely negative queries (i.e.  -foo  or  -@title:(foo|bar) ) are supported.  Prefix matches (all terms starting with a prefix) are expressed with a  * . For performance reasons, a minimum prefix length is enforced (2 by default, but is configurable)  A special \"wildcard query\" that returns all results in the index -  *  (cannot be combined with anything else).  Selection of specific fields using the syntax  @field:hello world .  Numeric Range matches on numeric fields with the syntax  @field:[{min} {max}] .  Geo radius matches on geo fields with the syntax  @field:[{lon} {lat} {radius} {m|km|mi|ft}]  Tag field filters with the syntax  @field:{tag | tag | ...} . See the full documentation on [tag fields|/Tags].  Optional terms or clauses:  foo ~bar  means bar is optional but documents with bar in them will rank higher.  Fuzzy matching on terms (as of v1.2.0):  %hello%  means all terms with Levenshtein distance of 1 from it.  An expression in a query can be wrapped in parentheses to disambiguate, e.g.  (hello|hella) (world|werld) .  Query attributes can be applied to individual clauses, e.g.  (foo bar) =  { $weight: 2.0; $slop: 1; $inorder: false; }  Combinations of the above can be used together, e.g  hello (world|foo) \"bar baz\" bbbb", 
            "title": "Search Query Syntax"
        }, 
        {
            "location": "/Query_Syntax/#pure\"_\"negative\"_\"queries", 
            "text": "As of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g.  -hello  or  -(@title:foo|bar) . The results will be all the documents  NOT  containing the query terms.   Warning  Any complex expression can be negated this way, however, caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.", 
            "title": "Pure negative queries"
        }, 
        {
            "location": "/Query_Syntax/#field\"_\"modifiers", 
            "text": "As of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword.  Per query expression or sub-expression, it is possible to specify which fields it matches, by prepending the expression with the  @  symbol, the field name and a  :  (colon) symbol.  If a field modifier precedes multiple words, they are considered to be a phrase with the same modifier.  If a field modifier precedes an expression in parentheses, it applies only to the expression inside the parentheses.  Multiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:  FT.SEARCH cars  @country:korea @engine:(diesel|hybrid) @class:suv   Multiple modifiers can be applied to the same term or grouped terms. e.g.:  FT.SEARCH idx  @title|body:(hello world) @url|image:mydomain   This will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.", 
            "title": "Field modifiers"
        }, 
        {
            "location": "/Query_Syntax/#numeric\"_\"filters\"_\"in\"_\"query", 
            "text": "If a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the Redis request or filter with it by specifying filtering rules in the query. The syntax is  @field:[{min} {max}]  - e.g.  @price:[100 200] .", 
            "title": "Numeric filters in query"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"notes\"_\"on\"_\"numeric\"_\"predicates", 
            "text": "It is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.    It is possible to intersect or union multiple numeric filters in the same query, be it for the same field or different ones.    -inf ,  inf  and  +inf  are acceptable numbers in a range. Thus greater-than 100 is expressed as  [(100 inf] .    Numeric filters are inclusive. Exclusive min or max are expressed with  (  prepended to the number, e.g.  [(100 (200] .    It is possible to negate a numeric filter by prepending a  -  sign to the filter, e.g. returning a result where price differs from 100 is expressed as:  @title:foo -@price:[100 100] .", 
            "title": "A few notes on numeric predicates"
        }, 
        {
            "location": "/Query_Syntax/#tag\"_\"filters", 
            "text": "RediSearch (starting with version 0.91) allows a special field type called \"tag field\", with simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search, and can be used only with a special syntax:  @field:{ tag | tag | ...}\n\ne.g.\n\n@cities:{ New York | Los Angeles | Barcelona }  Tags can have multiple words or include other punctuation marks other than the field's separator ( ,  by default). Punctuation marks in tags should be escaped with a backslash ( \\ ). It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.  Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing  all  tags, you should repeat the tag filter several times, e.g.:  # This will return all documents containing all three cities as tags:\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n# This will return all documents containing either city:\n@cities:{ New York | Los Angeles | Barcelona }  Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc.", 
            "title": "Tag filters"
        }, 
        {
            "location": "/Query_Syntax/#geo\"_\"filters\"_\"in\"_\"query", 
            "text": "As of version 0.21, it is possible to add geo radius queries directly into the query language  with the syntax  @field:[{lon} {lat} {radius} {m|km|mi|ft}] . This filters the result to a given radius from a lon,lat point, defined in meters, kilometers, miles or feet. See Redis' own  GEORADIUS  command for more details as it is used internally for that).  Radius filters can be added into the query just like numeric filters. For example, in a database of businesses, looking for Chinese restaurants near San Francisco (within a 5km radius) would be expressed as:  chinese restaurant @location:[-122.41 37.77 5 km] .", 
            "title": "Geo filters in query"
        }, 
        {
            "location": "/Query_Syntax/#prefix\"_\"matching", 
            "text": "On index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending  *  to a prefix token. For example:  hel* world  Will be expanded to cover  (hello|help|helm|...) world .", 
            "title": "Prefix matching"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"notes\"_\"on\"_\"prefix\"_\"searches", 
            "text": "As prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffixes.    As a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:    Prefixes are limited to 2 letters or more. You can change this number by using the  MINPREFIX  setting on the module command line.    Expansion is limited to 200 terms or less. You can change this number by using the  MAXEXPANSIONS  setting on the module command line.    Prefix matching fully supports Unicode and is case insensitive.    Currently, there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap.", 
            "title": "A few notes on prefix searches"
        }, 
        {
            "location": "/Query_Syntax/#fuzzy\"_\"matching", 
            "text": "As of v1.2.0, the dictionary of all terms in the index can also be used to perform  Fuzzy Matching . Fuzzy matches are performed based on  Levenshtein distance  (LD). Fuzzy matching on a term is performed by surrounding the term with '%', for example:  %hello %   world   Will perform fuzzy matching on 'hello' for all terms where LD is 1.  As of v1.4.0, the LD of the fuzzy match can be set by the number of '%' surrounding it, so that  %%hello%%  will perform fuzzy matching on 'hello' for all terms where LD is 2.  The maximal LD for fuzzy matching is 3.", 
            "title": "Fuzzy matching"
        }, 
        {
            "location": "/Query_Syntax/#wildcard\"_\"queries", 
            "text": "As of version 1.1.0, we provide a special query to retrieve all the documents in an index. This is meant mostly for the aggregation engine. You can call it by specifying only a single star sign as the query string - i.e.  FT.SEARCH myIndex * .  This cannot be combined with any other filters, field modifiers or anything inside the query. It is technically possible to use the deprecated FILTER and GEOFILTER request parameters outside the query string in conjunction with a wildcard, but this makes the wildcard meaningless and only hurts performance.", 
            "title": "Wildcard queries"
        }, 
        {
            "location": "/Query_Syntax/#query\"_\"attributes", 
            "text": "As of version 1.2.0, it is possible to apply specific query modifying attributes to specific clauses of the query.  The syntax is  (foo bar) =  { $attribute: value; $attribute:value; ...} , e.g:  ( foo   bar )   =   {   $ weight :   2.0 ;   $ slop :   1 ;   $ inorder :   true ;   }  ~( bar   baz )   =   {   $ weight :   0.5 ;   }   The supported attributes are:   $weight : determines the weight of the sub-query or token in the overall ranking on the result (default: 1.0).  $slop : determines the maximum allowed \"slop\" (space between terms) in the query clause (default: 0).  $inorder : whether or not the terms in a query clause must appear in the same order as in the query, usually set alongside with  $slop  (default: false).  $phonetic : whether or not to perform phonetic matching (default: true). Note: setting this attribute on for fields which were not creates as  PHONETIC  will produce an error.", 
            "title": "Query attributes"
        }, 
        {
            "location": "/Query_Syntax/#a\"_\"few\"_\"query\"_\"examples", 
            "text": "Simple phrase query - hello AND world  hello world    Exact phrase query -  hello  FOLLOWED BY  world  hello world     Union: documents containing either  hello  OR  world  hello|world    Not: documents containing  hello  but not  world  hello -world    Intersection of unions  (hello|halo) (world|werld)    Negation of union  hello -(world|werld)    Union inside phrase  (barack|barrack) obama    Optional terms with higher priority to ones containing more matches:  obama ~barack ~michelle    Exact phrase in one field, one word in another field:  @title: barack obama  @job:president    Combined AND, OR with field specifiers:  @title:hello world @body:(foo bar) @category:(articles|biographies)    Prefix Queries:  hello worl*\n\nhel* worl*\n\nhello -worl*    Numeric Filtering - products named \"tv\" with a price range of 200-500:  @name:tv @price:[200 500]    Numeric Filtering - users with age greater than 18:  @age:[(18 +inf]", 
            "title": "A few query examples"
        }, 
        {
            "location": "/Query_Syntax/#mapping\"_\"common\"_\"sql\"_\"predicates\"_\"to\"_\"redisearch", 
            "text": "SQL Condition  RediSearch Equivalent  Comments      WHERE x='foo' AND y='bar'  @x:foo @y:bar  for less ambiguity use (@x:foo) (@y:bar)    WHERE x='foo' AND y!='bar'  @x:foo -@y:bar     WHERE x='foo' OR y='bar'  (@x:foo)|(@y:bar)     WHERE x IN ('foo', 'bar','hello world')  @x:(foo|bar|\"hello world\")  quotes mean exact phrase    WHERE y='foo' AND x NOT IN ('foo','bar')  @y:foo (-@x:foo) (-@x:bar)     WHERE x NOT IN ('foo','bar')  -@x:(foo|bar)     WHERE num BETWEEN 10 AND 20  @num:[10 20]     WHERE num  = 10  @num:[10 +inf]     WHERE num   10  @num:[(10 +inf]     WHERE num   10  @num:[-inf (10]     WHERE num  = 10  @num:[-inf 10]     WHERE num   10 OR num   20  @num:[-inf (10] | @num:[(20 +inf]     WHERE name LIKE 'john%'  @name:john*", 
            "title": "Mapping common SQL predicates to RediSearch"
        }, 
        {
            "location": "/Query_Syntax/#technical\"_\"note", 
            "text": "The query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition  at the git repo .", 
            "title": "Technical note"
        }, 
        {
            "location": "/Stopwords/", 
            "text": "Stop-Words\n\n\nRediSearch has a pre-defined default list of \nstop-words\n. These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index. \n\n\nWhen indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query. \n\n\nAt the moment, the default stop-word list applies to all full-text indexes in all languages and can be overridden manually at index creation time. \n\n\nDefault stop-word list\n\n\nThe following words are treated as stop-words by default: \n\n\n a,    is,    the,   an,   and,  are, as,  at,   be,   but,  by,   for,\n if,   in,    into,  it,   no,   not, of,  on,   or,   such, that, their,\n then, there, these, they, this, to,  was, will, with\n\n\n\n\n\nOverriding the default stop-words\n\n\nStop-words for an index can be defined (or disabled completely) on index creation using the \nSTOPWORDS\n argument in the \nFT.CREATE\n command.\n\n\nThe format is \nSTOPWORDS {number} {stopword} ...\n where number is the number of stopwords given. The \nSTOPWORDS\n argument must come before the \nSCHEMA\n argument. For example:\n\n\nFT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT \n\n\n\n\n\nDisabling stop-words completely\n\n\nDisabling stopwords completely can be done by passing \nSTOPWORDS 0\n on \nFT.CREATE\n.\n\n\nAvoiding stop-word detection in search queries\n\n\nIn rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.", 
            "title": "Stop-Words"
        }, 
        {
            "location": "/Stopwords/#stop-words", 
            "text": "RediSearch has a pre-defined default list of  stop-words . These are words that are usually so common that they do not add much information to search, but take up a lot of space and CPU time in the index.   When indexing, stop-words are discarded and not indexed. When searching, they are also ignored and treated as if they were not sent to the query processor. This is done when parsing the query.   At the moment, the default stop-word list applies to all full-text indexes in all languages and can be overridden manually at index creation time.", 
            "title": "Stop-Words"
        }, 
        {
            "location": "/Stopwords/#default\"_\"stop-word\"_\"list", 
            "text": "The following words are treated as stop-words by default:    a,    is,    the,   an,   and,  are, as,  at,   be,   but,  by,   for,\n if,   in,    into,  it,   no,   not, of,  on,   or,   such, that, their,\n then, there, these, they, this, to,  was, will, with", 
            "title": "Default stop-word list"
        }, 
        {
            "location": "/Stopwords/#overriding\"_\"the\"_\"default\"_\"stop-words", 
            "text": "Stop-words for an index can be defined (or disabled completely) on index creation using the  STOPWORDS  argument in the  FT.CREATE  command.  The format is  STOPWORDS {number} {stopword} ...  where number is the number of stopwords given. The  STOPWORDS  argument must come before the  SCHEMA  argument. For example:  FT.CREATE myIndex STOPWORDS 3 foo bar baz SCHEMA title TEXT body TEXT", 
            "title": "Overriding the default stop-words"
        }, 
        {
            "location": "/Stopwords/#disabling\"_\"stop-words\"_\"completely", 
            "text": "Disabling stopwords completely can be done by passing  STOPWORDS 0  on  FT.CREATE .", 
            "title": "Disabling stop-words completely"
        }, 
        {
            "location": "/Stopwords/#avoiding\"_\"stop-word\"_\"detection\"_\"in\"_\"search\"_\"queries", 
            "text": "In rare use cases, where queries are very long and are guaranteed by the client application to not contain stopwords, it is possible to avoid checking for them when parsing the query. This saves some CPU time and is only worth it if the query has dozens or more terms in it. Using this without verifying that the query doesn't contain stop-words might result in empty queries.", 
            "title": "Avoiding stop-word detection in search queries"
        }, 
        {
            "location": "/Aggregations/", 
            "text": "RediSearch Aggregations\n\n\nAggregations are a way to process the results of a search query, group, sort and transform them - and extract analytic insights from them. Much like aggregation queries in other databases and search engines, they can be used to create analytics reports, or perform \nFaceted Search\n style queries. \n\n\nFor example, indexing a web-server's logs, we can create a report for unique users by hour, country or any other breakdown; or create different reports for errors, warnings, etc. \n\n\nCore concepts\n\n\nThe basic idea of an aggregate query is this:\n\n\n\n\nPerform a search query, filtering for records you wish to process.\n\n\nBuild a pipeline of operations that transform the results by zero or more steps of:\n\n\nGroup and Reduce\n: grouping by fields in the results, and applying reducer functions on each group.\n\n\nSort\n: sort the results based on one or more fields.\n\n\nApply Transformations\n: Apply mathematical and string functions on fields in the pipeline, optionally creating new fields or replacing existing ones\n\n\nLimit\n: Limit the result, regardless of sorting the result. \n\n\nFilter\n: Filter the results (post-query) based on predicates relating to its values. \n\n\n\n\nThe pipeline is dynamic and reentrant, and every operation can be repeated. For example, you can group by property X, sort the top 100 results by group size, then group by property Y and sort the results by some other property, then apply a transformation on the output. \n\n\nFigure 1: Aggregation Pipeline Example\n\n\n\nAggregate request format\n\n\nThe aggregate request's syntax is defined as follows:\n\n\nFT\n.\nAGGREGATE\n\n  \n{\nindex_name\n:\nstring\n}\n\n  \n{\nquery_string\n:\nstring\n}\n\n  \n[\nWITHSCHEMA\n]\n \n[\nVERBATIM\n]\n\n  \n[\nLOAD\n \n{\nnargs\n:\ninteger\n}\n \n{\nproperty\n:\nstring\n}\n \n...]\n\n  \n[\nGROUPBY\n\n    \n{\nnargs\n:\ninteger\n}\n \n{\nproperty\n:\nstring\n}\n \n...\n\n    \nREDUCE\n\n      \n{\nFUNC\n:\nstring\n}\n\n      \n{\nnargs\n:\ninteger\n}\n \n{\narg\n:\nstring\n}\n \n...\n\n      \n[\nAS\n \n{\nname\n:\nstring\n}\n]\n\n    \n...\n\n  \n]\n \n...\n\n  \n[\nSORTBY\n\n    \n{\nnargs\n:\ninteger\n}\n \n{\nstring\n}\n \n...\n\n    \n[\nMAX\n \n{\nnum\n:\ninteger\n}\n]\n \n...\n\n  \n]\n \n...\n\n  \n[\nAPPLY\n\n    \n{\nEXPR\n:\nstring\n}\n\n    \nAS\n \n{\nname\n:\nstring\n}\n\n  \n]\n \n...\n\n  \n[\nFILTER\n \n{\nEXPR\n:\nstring\n}\n]\n \n...\n\n  \n[\nLIMIT\n \n{\noffset\n:\ninteger\n}\n \n{\nnum\n:\ninteger\n}\n \n]\n \n...\n\n\n\n\n\n\nParameters in detail\n\n\nParameters which may take a variable number of arguments are expressed in the\nform of \nparam {nargs} {property_1... property_N}\n. The first argument to the\nparameter is the number of arguments following the parameter. This allows\nRediSearch to avoid a parsing ambiguity in case one of your arguments has the\nname of another parameter. For example, to sort by first name, last name, and\ncountry, one would specify \nSORTBY 6 firstName ASC lastName DESC country ASC\n.\n\n\n\n\n\n\nindex_name\n: The index the query is executed again.\n\n\n\n\n\n\nquery_string\n: The base filtering query that retrieves the documents. It follows \nthe exact same syntax\n as the search query, including filters, unions, not, optional, etc.\n\n\n\n\n\n\nLOAD {nargs} {property} \u2026\n: Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as \nSORTABLE\n, where they are available to the aggregation pipeline with very low latency. LOAD hurts the performance of aggregate queries considerably since every processed record needs to execute the equivalent of HMGET against a redis key, which when executed over millions of keys, amounts to very high processing times. \n\n\n\n\n\n\nGROUPBY {nargs} {property}\n: Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them or performing multiple aggregate operations (see below).\n\n\n\n\n\n\nREDUCE {func} {nargs} {arg} \u2026 [AS {name}]\n: Reduce the matching results in each group into a single record, using a reduction function. For example, COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers.\n\n\nThe reducers can have their own property names using the \nAS {name}\n optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property \n@foo\n, the resulting name will be \ncount_distinct(@foo)\n. \n\n\n\n\n\n\nSORTBY {nargs} {property} {ASC|DESC} [MAX {num}]\n: Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but \nASC\n or \nDESC\n can be added for each property. \nnargs\n is the number of sorting parameters, including ASC and DESC. for example: \nSORTBY 4 @foo ASC @bar DESC\n. \n\n\nMAX\n is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to \nLIMIT\n, you usually need just \nSORTBY \u2026 MAX\n for common queries. \n\n\n\n\n\n\nAPPLY {expr} AS {name}\n: Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation. \nexpr\n is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example: \nAPPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz\n will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline. \n\n\n\n\n\n\nLIMIT {offset} {num}\n. Limit the number of results to return just \nnum\n results starting at index \noffset\n (zero based). AS mentioned above, it is much more efficient to use \nSORTBY \u2026 MAX\n if you are interested in just limiting the optput of a sort operation.\n\n\nHowever, limit can be used to limit results without sorting, or for paging the n-largest results as determined by \nSORTBY MAX\n. For example, getting results 50-100 of the top 100 results is most efficiently expressed as \nSORTBY 1 @foo MAX 100 LIMIT 50 50\n. Removing the MAX from SORTBY will result in the pipeline sorting \nall\n the records and then paging over results 50-100. \n\n\n\n\n\n\nFILTER {expr}\n. Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline. See FILTER Expressions below for full details.\n\n\n\n\n\n\nQuick example\n\n\nLet's assume we have log of visits to our website, each record containing the following fields/properties:\n\n\n\n\nurl\n (text, sortable)\n\n\ntimestamp\n (numeric, sortable) - unix timestamp of visit entry. \n\n\ncountry\n (tag, sortable)\n\n\nuser_id\n (text, sortable, not indexed)\n\n\n\n\nExample 1: unique users by hour, ordered chronologically.\n\n\nFirst of all, we want \nall\n records in the index, because why not. The first step is to determine the index name and the filtering query. A filter query of \n*\n means \"get all records\":\n\n\nFT.AGGREGATE myIndex \n*\n\n\n\n\n\n\nNow we want to group the results by hour. Since we have the visit times as unix timestamps in second resolution, we need to extract the hour component of the timestamp. So we first add an APPLY step, that strips the sub-hour information from the timestamp and stores is as a new property, \nhour\n:\n\n\nFT.AGGREGATE myIndex \n*\n\n  APPLY \n@timestamp - (@timestamp % 3600)\n AS hour\n\n\n\n\n\nNow we want to group the results by hour, and count the distinct user ids in each hour. This is done by a GROUPBY/REDUCE  step:\n\n\nFT.AGGREGATE myIndex \n*\n\n  APPLY \n@timestamp - (@timestamp % 3600)\n AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users\n\n\n\n\n\nNow we'd like to sort the results by hour, ascending:\n\n\nFT.AGGREGATE myIndex \n*\n\n  APPLY \n@timestamp - (@timestamp % 3600)\n AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users\n\n  SORTBY 2 @hour ASC\n\n\n\n\n\nAnd as a final step, we can format the hour as a human readable timestamp. This is done by calling the transformation function \ntimefmt\n that formats unix timestamps. You can specify a format to be passed to the system's \nstrftime\n function (\nsee documentation\n), but not specifying one  is equivalent to specifying \n%FT%TZ\n to \nstrftime\n.\n\n\nFT.AGGREGATE myIndex \n*\n\n  APPLY \n@timestamp - (@timestamp % 3600)\n AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users\n\n  SORTBY 2 @hour ASC\n\n  APPLY timefmt(@hour) AS hour\n\n\n\n\n\nExample 2: Sort visits to a specific URL by day and country:\n\n\nIn this example we filter by the url, transform the timestamp to its day part, and group by the day and country, simply counting the number of visits per group. sorting by day ascending and country descending. \n\n\nFT.AGGREGATE myIndex \n@url:\\\nabout.html\\\n\n    APPLY \n@timestamp - (@timestamp % 86400)\n AS day\n    GROUPBY 2 @day @country\n        REDUCE count 0 AS num_visits \n    SORTBY 4 @day ASC @country DESC\n\n\n\n\n\nGROUPBY reducers\n\n\nGROUPBY\n step work similarly to SQL \nGROUP BY\n clauses, and create groups of results based on one or more properties in each record. For each group, we return the \"group keys\", or the values common to all records in the group, by which they were grouped together - along with the results of zero or more \nREDUCE\n clauses.\n\n\nEach \nGROUPBY\n step in the pipeline may be accompanied by zero or more \nREDUCE\n clauses. Reducers apply some accumulation function to each record in the group and reduce them into a single record representing the group. When we are finished processing all the records upstream of the \nGROUPBY\n step, each group emits its reduced record. \n\n\nFor example, the simplest reducer is COUNT, which simply counts the number of records in each group. \n\n\nIf multiple \nREDUCE\n clauses exist for a single \nGROUPBY\n step, each reducer works independently on each result and writes its final output once. Each reducer may have its own alias determined using the \nAS\n optional parameter. If \nAS\n is not specified, the alias is the reduce function and its parameters, e.g. \ncount_distinct(foo,bar)\n.\n\n\nSupported GROUPBY reducers\n\n\nCOUNT\n\n\nFormat\n\n\nREDUCE COUNT 0\n\n\n\n\n\nDescription\n\n\nCount the number of records in each group \n\n\nCOUNT_DISTINCT\n\n\nFormat\n\n\nREDUCE COUNT_DISTINCT 1 {property}\n\n\n\n\n\nDescription\n\n\nCount the number of distinct values for \nproperty\n. \n\n\n\n\nNote\n\n\nThe reducer creates a hash-set per group, and hashes each record. This can be memory heavy if the groups are big.\n\n\n\n\nCOUNT_DISTINCTISH\n\n\nFormat\n \n\n\nREDUCE COUNT_DISTINCTISH 1 {property}\n\n\n\n\n\nDescription\n\n\nSame as COUNT_DISTINCT - but provide an approximation instead of an exact count, at the expense of less memory and CPU in big groups. \n\n\n\n\nNote\n\n\nThe reducer uses \nHyperLogLog\n counters per group, at ~3% error rate, and 1024 Bytes of constant space allocation per group. This means it is ideal for few huge groups and not ideal for many small groups. In the former case, it can be an order of magnitude faster and consume much less memory than COUNT_DISTINCT, but again, it does not fit every user case. \n\n\n\n\nSUM\n\n\nFormat\n\n\nREDUCE SUM 1 {property}\n\n\n\n\n\nDescription\n\n\nReturn the sum of all numeric values of a given property in a group. Non numeric values if the group are counted as 0.\n\n\nMIN\n\n\nFormat\n\n\nREDUCE MIN 1 {property}\n\n\n\n\n\nDescription\n\n\nReturn the minimal value of a property, whether it is a string, number or NULL.\n\n\nMAX\n\n\nFormat\n\n\nREDUCE MAX 1 {property}\n\n\n\n\n\nDescription\n\n\nReturn the maximal value of a property, whether it is a string, number or NULL.\n\n\nAVG\n\n\nFormat\n\n\nREDUCE AVG 1 {property}\n\n\n\n\n\nDescription\n\n\nReturn the average value of a numeric property. This is equivalent to reducing by sum and count, and later on applying the ratio of them as an APPLY step.\n\n\nSTDDEV\n\n\nFormat\n\n\nREDUCE STDDEV 1 {property}\n\n\n\n\n\nDescription\n\n\nReturn the \nstandard deviation\n of a numeric property in the group.\n\n\nQUANTILE\n\n\nFormat\n\n\nREDUCE QUANTILE 2 {property} {quantile}\n\n\n\n\n\nDescription\n\n\nReturn the value of a numeric property at a given quantile of the results. Quantile is expressed as a number between 0 and 1. For example, the median can be expressed as the quantile at 0.5, e.g. \nREDUCE QUANTILE 2 @foo 0.5 AS median\n .\n\n\nIf multiple quantiles are required, just repeat  the QUANTILE reducer for each quantile. e.g. \nREDUCE QUANTILE 2 @foo 0.5 AS median REDUCE QUANTILE 2 @foo 0.99 AS p99\n \n\n\nTOLIST\n\n\nFormat\n\n\nREDUCE TOLIST 1 {property}\n\n\n\n\n\nDescription\n\n\nMerge all \ndistinct\n values of a given property into a single array. \n\n\nFIRST_VALUE\n\n\nFormat\n\n\nREDUCE FIRST_VALUE {nargs} {property} [BY {property} [ASC|DESC]]\n\n\n\n\n\nDescription\n\n\nReturn the first or top value of a given property in the group, optionally by comparing that or another property. For example, you can extract the name of the oldest user in the group:\n\n\nREDUCE FIRST_VALUE 4 @name BY @age DESC\n\n\n\n\n\nIf no \nBY\n is specified, we return the first value we encounter in the group.\n\n\nIf you with to get the top or bottom value in the group sorted by the same value, you are better off using the \nMIN/MAX\n reducers, but the same effect will be achieved by doing \nREDUCE FIRST_VALUE 4 @foo BY @foo DESC\n.\n\n\nRANDOM_SAMPLE\n\n\nFormat\n\n\nREDUCE RANDOM_SAMPLE {nargs} {property} {sample_size}\n\n\n\n\n\nDescription\n\n\nPerform a reservoir sampling of the group elements with a given size, and return an array of the sampled items with an even distribution.\n\n\nAPPLY expressions\n\n\nAPPLY\n performs a 1-to-1 transformation on one or more properties in each record. It either stores the result as a new property down the pipeline, or replaces any property using this transformation. \n\n\nThe transformations are expressed as a combination of arithmetic expressions and built in functions. Evaluating functions and expressions is recursively nested and can be composed without limit. For example: \nsqrt(log(foo) * floor(@bar/baz)) + (3^@qaz % 6)\n or simply \n@foo/@bar\n.\n\n\nIf an expression or a function is applied to values that do not match the expected types, no error is emitted but a NULL value is set as the result. \n\n\nAPPLY steps must have an explicit alias determined by the \nAS\n parameter.\n\n\nLiterals inside expressions\n\n\n\n\nNumbers are expressed as integers or floating point numbers, i.e. \n2\n, \n3.141\n, \n-34\n, etc. \ninf\n and \n-inf\n are acceptable as well.\n\n\nStrings are quoted with either single or double quotes. Single quotes are acceptable inside strings quoted with double quotes and vice versa. Punctuation marks can be escaped with backslashes. e.g. \n\"foo's bar\"\n ,\n'foo\\'s bar'\n, \n\"foo \\\"bar\\\"\"\n .\n\n\nAny literal or sub expression can be wrapped in parentheses to resolve ambiguities of operator precedence.\n\n\n\n\nArithmetic operations\n\n\nFor numeric expressions and properties, we support addition (\n+\n), subtraction (\n-\n), multiplication (\n*\n), division (\n/\n), modulo (\n%\n) and power (\n^\n). We currently do not support bitwise logical operators.  \n\n\nNote that these operators apply only to numeric values and numeric sub expressions. Any attempt to multiply a string by a number, for instance, will result in a NULL output.\n\n\nList of numeric APPLY functions\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nlog(x)\n\n\nReturn the logarithm of a number, property or sub-expression\n\n\nlog(@foo)\n\n\n\n\n\n\nabs(x)\n\n\nReturn the absolute number of a numeric expression\n\n\nabs(@foo-@bar)\n\n\n\n\n\n\nceil(x)\n\n\nRound to the smallest value not less than x\n\n\nceil(@foo/3.14)\n\n\n\n\n\n\nfloor(x)\n\n\nRound to largest value not greater than x\n\n\nfloor(@foo/3.14)\n\n\n\n\n\n\nlog2(x)\n\n\nReturn the  logarithm of x to base 2\n\n\nlog2(2^@foo)\n\n\n\n\n\n\nexp(x)\n\n\nReturn the exponent of x, i.e. \ne^x\n\n\nexp(@foo)\n\n\n\n\n\n\nsqrt(x)\n\n\nReturn the square root of x\n\n\nsqrt(@foo)\n\n\n\n\n\n\n\n\nList of string APPLY functions\n\n\n\n\n\n\n\n\nFunction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nupper(s)\n\n\nReturn the uppercase conversion of s\n\n\nupper('hello world')\n\n\n\n\n\n\nlower(s)\n\n\nReturn the lowercase conversion of 2\n\n\nlower(\"HELLO WORLD\")\n\n\n\n\n\n\nsubstr(s, offset, count)\n\n\nReturn the substring of s, starting at \noffset\n and having \ncount\n characters. \nIf offset is negative, it represents the distance from the end of the string. \nIf count is -1, it means \"the rest of the string starting at offset\".\n\n\nsubstr(\"hello\", 0, 3)\n \n \nsubstr(\"hello\", -2, -1)\n\n\n\n\n\n\nformat( fmt, ...)\n\n\nUse the arguments following \nfmt\n to format a string. \nCurrently the only format argument supported is \n%s\n and it applies to all types of arguments.\n\n\nformat(\"Hello, %s, you are %s years old\", @name, @age)\n\n\n\n\n\n\nmatched_terms([max_terms=100])\n\n\nReturn the query terms that matched for each record (up to 100), as a list. If a limit is specified, we will return the first N matches we find - based on query order.\n\n\nmatched_terms()\n\n\n\n\n\n\nsplit(s, [sep=\",\"], [strip=\" \"])\n\n\nSplit a string by any character in the string sep, and strip any characters in strip. If only s is specified, we split by commas and strip spaces. The output is an array.\n\n\nsplit(\"foo,bar\")\n\n\n\n\n\n\n\n\nList of date/time APPLY functions\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntimefmt(x, [fmt])\n\n\nReturn a formatted time string based on a numeric timestamp value x. \n See \nstrftime\n for formatting options. \nNot specifying \nfmt\n is equivalent to \n%FT%TZ\n.\n\n\n\n\n\n\nparsetime(timesharing, [fmt])\n\n\nThe opposite of timefmt() - parse a time format using a given format string\n\n\n\n\n\n\nday(timestamp)\n\n\nRound a Unix timestamp to midnight (00:00) start of the current day.\n\n\n\n\n\n\nhour(timestamp)\n\n\nRound a Unix timestamp to the beginning of the current hour.\n\n\n\n\n\n\nminute(timestamp)\n\n\nRound a Unix timestamp to the beginning of the current minute.\n\n\n\n\n\n\nmonth(timestamp)\n\n\nRound a unix timestamp to the beginning of the current month.\n\n\n\n\n\n\ndayofweek(timestamp)\n\n\nConvert a Unix timestamp to the day number (Sunday = 0).\n\n\n\n\n\n\ndayofmonth(timestamp)\n\n\nConvert a Unix timestamp to the day of month number (1 .. 31).\n\n\n\n\n\n\ndayofyear(timestamp)\n\n\nConvert a Unix timestamp to the day of year number (0 .. 365).\n\n\n\n\n\n\nyear(timestamp)\n\n\nConvert a Unix timestamp to the current year (e.g. 2018).\n\n\n\n\n\n\nmonthofyear(timestamp)\n\n\nConvert a Unix timestamp to the current month (0 .. 11).\n\n\n\n\n\n\n\n\nFILTER expressions\n\n\nFILTER expressions filter the results using predicates relating to values in the result set.\n\n\nThe FILTER expressions are evaluated post-query and relate to the current state of the pipeline. Thus they can be useful to prune the results based on group calculations. Note that the filters are not indexed and will not speed the processing per se. \n\n\nFilter expressions follow the syntax of APPLY expressions, with the addition of the conditions \n==\n, \n!=\n, \n, \n=\n, \n, \n=\n. Two or more predicates can be combined with logical AND (\n) and OR (\n||\n). A single predicate can be negated with a NOT prefix (\n!\n).  \n\n\nFor example, filtering all results where the user name is 'foo' and the age is less than 20 is expressed  as:\n\n\nFT.AGGREGATE \n  ...\n  FILTER \n@name==\nfoo\n \n @age \n 20\n\n  ...\n\n\n\n\n\nSeveral filter steps can be added, although at the same stage in the pipeline, it is more efficient to combine several predicates into a single filter step.\n\n\nCursor API\n\n\nFT.AGGREGATE ... WITHCURSOR [COUNT {read size} MAXIDLE {idle timeout}]\nFT.CURSOR READ {idx} {cid} [COUNT {read size}]\nFT.CURSOR DEL {idx} {cid}\n\n\n\n\n\nYou can use cursors with \nFT.AGGREGATE\n, with the \nWITHCURSOR\n keyword. Cursors allow you to\nconsume only part of the response, allowing you to fetch additional results as needed.\nThis is much quicker than using \nLIMIT\n with offset, since the query is executed only\nonce, and its state is stored on the server.\n\n\nTo use cursors, specify the \nWITHCURSOR\n keyword in \nFT.AGGREGATE\n, e.g.\n\n\nFT.AGGREGATE idx * WITHCURSOR\n\n\n\n\n\nThis will return a response of an array with two elements. The first element is\nthe actual (partial) results, and the second is the cursor ID. The cursor ID\ncan then be fed to \nFT.CURSOR READ\n repeatedly, until the cursor ID is 0, in\nwhich case all results have been returned.\n\n\nTo read from an existing cursor, use \nFT.CURSOR READ\n, e.g.\n\n\nFT.CURSOR READ idx 342459320\n\n\n\n\n\nAssuming \n342459320\n is the cursor ID returned from the \nFT.AGGREGATE\n request.\n\n\nHere is an example in pseudo-code:\n\n\nresponse, cursor = FT.AGGREGATE \nidx\n \nredis\n \nWITHCURSOR\n;\nwhile (1) {\n  processResponse(response)\n  if (!cursor) {\n    break;\n  }\n  response, cursor = FT.CURSOR read \nidx\n cursor\n}\n\n\n\n\n\nNote that even if the cursor is 0, a partial result may still be returned.\n\n\nCursor settings\n\n\nRead size\n\n\nYou can control how many rows are read per each cursor fetch by using the\n\nCOUNT\n parameter. This parameter can be specified both in \nFT.AGGREGATE\n\n(immediately after \nWITHCURSOR\n) or in \nFT.CURSOR READ\n.\n\n\nFT.AGGREGATE idx query WITHCURSOR COUNT 10\n\n\n\n\n\nWill read 10 rows at a time.\n\n\nYou can override this setting by also specifying \nCOUNT\n in \nCURSOR READ\n, e.g.\n\n\nFT.CURSOR READ idx 342459320 COUNT 50\n\n\n\n\n\nWill return at most 50 results.\n\n\nThe default read size is 1000\n\n\nTimeouts and limits\n\n\nBecause cursors are stateful resources which occupy memory on the server, they\nhave a limited lifetime. In order to safeguard against orphaned/stale cursors,\ncursors have an idle timeout value. If no activity occurs on the cursor before\nthe idle timeout, the cursor is deleted. The idle timer resets to 0 whenever\nthe cursor is read from using \nCURSOR READ\n.\n\n\nThe default idle timeout is 30000 milliseconds (or 30 seconds). You can modify\nthe idle timeout using the \nMAXIDLE\n keyword when creating the cursor. Note that\nthe value cannot exceed the default 30s.\n\n\nFT.AGGREGATE idx query WITHCURSOR MAXIDLE 10000\n\n\n\n\n\nWill set the limit for 10 seconds.\n\n\nOther cursor commands\n\n\nCursors can be explicity deleted using the \nCURSOR DEL\n command, e.g.\n\n\nFT.CURSOR DEL idx 342459320\n\n\n\n\n\nNote that cursors are automatically deleted if all their results have been\nreturned, or if they have been timed out.\n\n\nAll idle cursors can be forcefully purged at once using \nFT.CURSOR GC idx 0\n command.\nBy default, RediSearch uses a lazy throttled approach to garbage collection, which\ncollects idle cursors every 500 operations, or every second - whichever is later.", 
            "title": "Aggregations"
        }, 
        {
            "location": "/Aggregations/#redisearch\"_\"aggregations", 
            "text": "Aggregations are a way to process the results of a search query, group, sort and transform them - and extract analytic insights from them. Much like aggregation queries in other databases and search engines, they can be used to create analytics reports, or perform  Faceted Search  style queries.   For example, indexing a web-server's logs, we can create a report for unique users by hour, country or any other breakdown; or create different reports for errors, warnings, etc.", 
            "title": "RediSearch Aggregations"
        }, 
        {
            "location": "/Aggregations/#core\"_\"concepts", 
            "text": "The basic idea of an aggregate query is this:   Perform a search query, filtering for records you wish to process.  Build a pipeline of operations that transform the results by zero or more steps of:  Group and Reduce : grouping by fields in the results, and applying reducer functions on each group.  Sort : sort the results based on one or more fields.  Apply Transformations : Apply mathematical and string functions on fields in the pipeline, optionally creating new fields or replacing existing ones  Limit : Limit the result, regardless of sorting the result.   Filter : Filter the results (post-query) based on predicates relating to its values.    The pipeline is dynamic and reentrant, and every operation can be repeated. For example, you can group by property X, sort the top 100 results by group size, then group by property Y and sort the results by some other property, then apply a transformation on the output.   Figure 1: Aggregation Pipeline Example", 
            "title": "Core concepts"
        }, 
        {
            "location": "/Aggregations/#aggregate\"_\"request\"_\"format", 
            "text": "The aggregate request's syntax is defined as follows:  FT . AGGREGATE \n   { index_name : string } \n   { query_string : string } \n   [ WITHSCHEMA ]   [ VERBATIM ] \n   [ LOAD   { nargs : integer }   { property : string }   ...] \n   [ GROUPBY \n     { nargs : integer }   { property : string }   ... \n     REDUCE \n       { FUNC : string } \n       { nargs : integer }   { arg : string }   ... \n       [ AS   { name : string } ] \n     ... \n   ]   ... \n   [ SORTBY \n     { nargs : integer }   { string }   ... \n     [ MAX   { num : integer } ]   ... \n   ]   ... \n   [ APPLY \n     { EXPR : string } \n     AS   { name : string } \n   ]   ... \n   [ FILTER   { EXPR : string } ]   ... \n   [ LIMIT   { offset : integer }   { num : integer }   ]   ...", 
            "title": "Aggregate request format"
        }, 
        {
            "location": "/Aggregations/#parameters\"_\"in\"_\"detail", 
            "text": "Parameters which may take a variable number of arguments are expressed in the\nform of  param {nargs} {property_1... property_N} . The first argument to the\nparameter is the number of arguments following the parameter. This allows\nRediSearch to avoid a parsing ambiguity in case one of your arguments has the\nname of another parameter. For example, to sort by first name, last name, and\ncountry, one would specify  SORTBY 6 firstName ASC lastName DESC country ASC .    index_name : The index the query is executed again.    query_string : The base filtering query that retrieves the documents. It follows  the exact same syntax  as the search query, including filters, unions, not, optional, etc.    LOAD {nargs} {property} \u2026 : Load document fields from the document HASH objects. This should be avoided as a general rule of thumb. Fields needed for aggregations should be stored as  SORTABLE , where they are available to the aggregation pipeline with very low latency. LOAD hurts the performance of aggregate queries considerably since every processed record needs to execute the equivalent of HMGET against a redis key, which when executed over millions of keys, amounts to very high processing times.     GROUPBY {nargs} {property} : Group the results in the pipeline based on one or more properties. Each group should have at least one reducer (See below), a function that handles the group entries, either counting them or performing multiple aggregate operations (see below).    REDUCE {func} {nargs} {arg} \u2026 [AS {name}] : Reduce the matching results in each group into a single record, using a reduction function. For example, COUNT will count the number of records in the group. See the Reducers section below for more details on available reducers.  The reducers can have their own property names using the  AS {name}  optional argument. If a name is not given, the resulting name will be the name of the reduce function and the group properties. For example, if a name is not given to COUNT_DISTINCT by property  @foo , the resulting name will be  count_distinct(@foo) .     SORTBY {nargs} {property} {ASC|DESC} [MAX {num}] : Sort the pipeline up until the point of SORTBY, using a list of properties. By default, sorting is ascending, but  ASC  or  DESC  can be added for each property.  nargs  is the number of sorting parameters, including ASC and DESC. for example:  SORTBY 4 @foo ASC @bar DESC .   MAX  is used to optimized sorting, by sorting only for the n-largest elements. Although it is not connected to  LIMIT , you usually need just  SORTBY \u2026 MAX  for common queries.     APPLY {expr} AS {name} : Apply a 1-to-1 transformation on one or more properties, and either store the result as a new property down the pipeline, or replace any property using this transformation.  expr  is an expression that can be used to perform arithmetic operations on numeric properties, or functions that can be applied on properties depending on their types (see below), or any combination thereof. For example:  APPLY \"sqrt(@foo)/log(@bar) + 5\" AS baz  will evaluate this expression dynamically for each record in the pipeline and store the result as a new property called baz, that can be referenced by further APPLY / SORTBY / GROUPBY / REDUCE operations down the pipeline.     LIMIT {offset} {num} . Limit the number of results to return just  num  results starting at index  offset  (zero based). AS mentioned above, it is much more efficient to use  SORTBY \u2026 MAX  if you are interested in just limiting the optput of a sort operation.  However, limit can be used to limit results without sorting, or for paging the n-largest results as determined by  SORTBY MAX . For example, getting results 50-100 of the top 100 results is most efficiently expressed as  SORTBY 1 @foo MAX 100 LIMIT 50 50 . Removing the MAX from SORTBY will result in the pipeline sorting  all  the records and then paging over results 50-100.     FILTER {expr} . Filter the results using predicate expressions relating to values in each result. They are is applied post-query and relate to the current state of the pipeline. See FILTER Expressions below for full details.", 
            "title": "Parameters in detail"
        }, 
        {
            "location": "/Aggregations/#quick\"_\"example", 
            "text": "Let's assume we have log of visits to our website, each record containing the following fields/properties:   url  (text, sortable)  timestamp  (numeric, sortable) - unix timestamp of visit entry.   country  (tag, sortable)  user_id  (text, sortable, not indexed)", 
            "title": "Quick example"
        }, 
        {
            "location": "/Aggregations/#example\"_\"1\"_\"unique\"_\"users\"_\"by\"_\"hour\"_\"ordered\"_\"chronologically", 
            "text": "First of all, we want  all  records in the index, because why not. The first step is to determine the index name and the filtering query. A filter query of  *  means \"get all records\":  FT.AGGREGATE myIndex  *   Now we want to group the results by hour. Since we have the visit times as unix timestamps in second resolution, we need to extract the hour component of the timestamp. So we first add an APPLY step, that strips the sub-hour information from the timestamp and stores is as a new property,  hour :  FT.AGGREGATE myIndex  * \n  APPLY  @timestamp - (@timestamp % 3600)  AS hour  Now we want to group the results by hour, and count the distinct user ids in each hour. This is done by a GROUPBY/REDUCE  step:  FT.AGGREGATE myIndex  * \n  APPLY  @timestamp - (@timestamp % 3600)  AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users  Now we'd like to sort the results by hour, ascending:  FT.AGGREGATE myIndex  * \n  APPLY  @timestamp - (@timestamp % 3600)  AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users\n\n  SORTBY 2 @hour ASC  And as a final step, we can format the hour as a human readable timestamp. This is done by calling the transformation function  timefmt  that formats unix timestamps. You can specify a format to be passed to the system's  strftime  function ( see documentation ), but not specifying one  is equivalent to specifying  %FT%TZ  to  strftime .  FT.AGGREGATE myIndex  * \n  APPLY  @timestamp - (@timestamp % 3600)  AS hour\n\n  GROUPBY 1 @hour\n    REDUCE COUNT_DISTINCT 1 @user_id AS num_users\n\n  SORTBY 2 @hour ASC\n\n  APPLY timefmt(@hour) AS hour", 
            "title": "Example 1: unique users by hour, ordered chronologically."
        }, 
        {
            "location": "/Aggregations/#example\"_\"2\"_\"sort\"_\"visits\"_\"to\"_\"a\"_\"specific\"_\"url\"_\"by\"_\"day\"_\"and\"_\"country", 
            "text": "In this example we filter by the url, transform the timestamp to its day part, and group by the day and country, simply counting the number of visits per group. sorting by day ascending and country descending.   FT.AGGREGATE myIndex  @url:\\ about.html\\ \n    APPLY  @timestamp - (@timestamp % 86400)  AS day\n    GROUPBY 2 @day @country\n        REDUCE count 0 AS num_visits \n    SORTBY 4 @day ASC @country DESC", 
            "title": "Example 2: Sort visits to a specific URL by day and country:"
        }, 
        {
            "location": "/Aggregations/#groupby\"_\"reducers", 
            "text": "GROUPBY  step work similarly to SQL  GROUP BY  clauses, and create groups of results based on one or more properties in each record. For each group, we return the \"group keys\", or the values common to all records in the group, by which they were grouped together - along with the results of zero or more  REDUCE  clauses.  Each  GROUPBY  step in the pipeline may be accompanied by zero or more  REDUCE  clauses. Reducers apply some accumulation function to each record in the group and reduce them into a single record representing the group. When we are finished processing all the records upstream of the  GROUPBY  step, each group emits its reduced record.   For example, the simplest reducer is COUNT, which simply counts the number of records in each group.   If multiple  REDUCE  clauses exist for a single  GROUPBY  step, each reducer works independently on each result and writes its final output once. Each reducer may have its own alias determined using the  AS  optional parameter. If  AS  is not specified, the alias is the reduce function and its parameters, e.g.  count_distinct(foo,bar) .", 
            "title": "GROUPBY reducers"
        }, 
        {
            "location": "/Aggregations/#supported\"_\"groupby\"_\"reducers", 
            "text": "", 
            "title": "Supported GROUPBY reducers"
        }, 
        {
            "location": "/Aggregations/#count", 
            "text": "Format  REDUCE COUNT 0  Description  Count the number of records in each group", 
            "title": "COUNT"
        }, 
        {
            "location": "/Aggregations/#count\"_\"distinct", 
            "text": "Format  REDUCE COUNT_DISTINCT 1 {property}  Description  Count the number of distinct values for  property .    Note  The reducer creates a hash-set per group, and hashes each record. This can be memory heavy if the groups are big.", 
            "title": "COUNT_DISTINCT"
        }, 
        {
            "location": "/Aggregations/#count\"_\"distinctish", 
            "text": "Format    REDUCE COUNT_DISTINCTISH 1 {property}  Description  Same as COUNT_DISTINCT - but provide an approximation instead of an exact count, at the expense of less memory and CPU in big groups.    Note  The reducer uses  HyperLogLog  counters per group, at ~3% error rate, and 1024 Bytes of constant space allocation per group. This means it is ideal for few huge groups and not ideal for many small groups. In the former case, it can be an order of magnitude faster and consume much less memory than COUNT_DISTINCT, but again, it does not fit every user case.", 
            "title": "COUNT_DISTINCTISH"
        }, 
        {
            "location": "/Aggregations/#sum", 
            "text": "Format  REDUCE SUM 1 {property}  Description  Return the sum of all numeric values of a given property in a group. Non numeric values if the group are counted as 0.", 
            "title": "SUM"
        }, 
        {
            "location": "/Aggregations/#min", 
            "text": "Format  REDUCE MIN 1 {property}  Description  Return the minimal value of a property, whether it is a string, number or NULL.", 
            "title": "MIN"
        }, 
        {
            "location": "/Aggregations/#max", 
            "text": "Format  REDUCE MAX 1 {property}  Description  Return the maximal value of a property, whether it is a string, number or NULL.", 
            "title": "MAX"
        }, 
        {
            "location": "/Aggregations/#avg", 
            "text": "Format  REDUCE AVG 1 {property}  Description  Return the average value of a numeric property. This is equivalent to reducing by sum and count, and later on applying the ratio of them as an APPLY step.", 
            "title": "AVG"
        }, 
        {
            "location": "/Aggregations/#stddev", 
            "text": "Format  REDUCE STDDEV 1 {property}  Description  Return the  standard deviation  of a numeric property in the group.", 
            "title": "STDDEV"
        }, 
        {
            "location": "/Aggregations/#quantile", 
            "text": "Format  REDUCE QUANTILE 2 {property} {quantile}  Description  Return the value of a numeric property at a given quantile of the results. Quantile is expressed as a number between 0 and 1. For example, the median can be expressed as the quantile at 0.5, e.g.  REDUCE QUANTILE 2 @foo 0.5 AS median  .  If multiple quantiles are required, just repeat  the QUANTILE reducer for each quantile. e.g.  REDUCE QUANTILE 2 @foo 0.5 AS median REDUCE QUANTILE 2 @foo 0.99 AS p99", 
            "title": "QUANTILE"
        }, 
        {
            "location": "/Aggregations/#tolist", 
            "text": "Format  REDUCE TOLIST 1 {property}  Description  Merge all  distinct  values of a given property into a single array.", 
            "title": "TOLIST"
        }, 
        {
            "location": "/Aggregations/#first\"_\"value", 
            "text": "Format  REDUCE FIRST_VALUE {nargs} {property} [BY {property} [ASC|DESC]]  Description  Return the first or top value of a given property in the group, optionally by comparing that or another property. For example, you can extract the name of the oldest user in the group:  REDUCE FIRST_VALUE 4 @name BY @age DESC  If no  BY  is specified, we return the first value we encounter in the group.  If you with to get the top or bottom value in the group sorted by the same value, you are better off using the  MIN/MAX  reducers, but the same effect will be achieved by doing  REDUCE FIRST_VALUE 4 @foo BY @foo DESC .", 
            "title": "FIRST_VALUE"
        }, 
        {
            "location": "/Aggregations/#random\"_\"sample", 
            "text": "Format  REDUCE RANDOM_SAMPLE {nargs} {property} {sample_size}  Description  Perform a reservoir sampling of the group elements with a given size, and return an array of the sampled items with an even distribution.", 
            "title": "RANDOM_SAMPLE"
        }, 
        {
            "location": "/Aggregations/#apply\"_\"expressions", 
            "text": "APPLY  performs a 1-to-1 transformation on one or more properties in each record. It either stores the result as a new property down the pipeline, or replaces any property using this transformation.   The transformations are expressed as a combination of arithmetic expressions and built in functions. Evaluating functions and expressions is recursively nested and can be composed without limit. For example:  sqrt(log(foo) * floor(@bar/baz)) + (3^@qaz % 6)  or simply  @foo/@bar .  If an expression or a function is applied to values that do not match the expected types, no error is emitted but a NULL value is set as the result.   APPLY steps must have an explicit alias determined by the  AS  parameter.", 
            "title": "APPLY expressions"
        }, 
        {
            "location": "/Aggregations/#literals\"_\"inside\"_\"expressions", 
            "text": "Numbers are expressed as integers or floating point numbers, i.e.  2 ,  3.141 ,  -34 , etc.  inf  and  -inf  are acceptable as well.  Strings are quoted with either single or double quotes. Single quotes are acceptable inside strings quoted with double quotes and vice versa. Punctuation marks can be escaped with backslashes. e.g.  \"foo's bar\"  , 'foo\\'s bar' ,  \"foo \\\"bar\\\"\"  .  Any literal or sub expression can be wrapped in parentheses to resolve ambiguities of operator precedence.", 
            "title": "Literals inside expressions"
        }, 
        {
            "location": "/Aggregations/#arithmetic\"_\"operations", 
            "text": "For numeric expressions and properties, we support addition ( + ), subtraction ( - ), multiplication ( * ), division ( / ), modulo ( % ) and power ( ^ ). We currently do not support bitwise logical operators.    Note that these operators apply only to numeric values and numeric sub expressions. Any attempt to multiply a string by a number, for instance, will result in a NULL output.", 
            "title": "Arithmetic operations"
        }, 
        {
            "location": "/Aggregations/#list\"_\"of\"_\"numeric\"_\"apply\"_\"functions", 
            "text": "Function  Description  Example      log(x)  Return the logarithm of a number, property or sub-expression  log(@foo)    abs(x)  Return the absolute number of a numeric expression  abs(@foo-@bar)    ceil(x)  Round to the smallest value not less than x  ceil(@foo/3.14)    floor(x)  Round to largest value not greater than x  floor(@foo/3.14)    log2(x)  Return the  logarithm of x to base 2  log2(2^@foo)    exp(x)  Return the exponent of x, i.e.  e^x  exp(@foo)    sqrt(x)  Return the square root of x  sqrt(@foo)", 
            "title": "List of numeric APPLY functions"
        }, 
        {
            "location": "/Aggregations/#list\"_\"of\"_\"string\"_\"apply\"_\"functions", 
            "text": "Function        upper(s)  Return the uppercase conversion of s  upper('hello world')    lower(s)  Return the lowercase conversion of 2  lower(\"HELLO WORLD\")    substr(s, offset, count)  Return the substring of s, starting at  offset  and having  count  characters.  If offset is negative, it represents the distance from the end of the string.  If count is -1, it means \"the rest of the string starting at offset\".  substr(\"hello\", 0, 3)     substr(\"hello\", -2, -1)    format( fmt, ...)  Use the arguments following  fmt  to format a string.  Currently the only format argument supported is  %s  and it applies to all types of arguments.  format(\"Hello, %s, you are %s years old\", @name, @age)    matched_terms([max_terms=100])  Return the query terms that matched for each record (up to 100), as a list. If a limit is specified, we will return the first N matches we find - based on query order.  matched_terms()    split(s, [sep=\",\"], [strip=\" \"])  Split a string by any character in the string sep, and strip any characters in strip. If only s is specified, we split by commas and strip spaces. The output is an array.  split(\"foo,bar\")", 
            "title": "List of string APPLY functions"
        }, 
        {
            "location": "/Aggregations/#list\"_\"of\"_\"datetime\"_\"apply\"_\"functions", 
            "text": "Function  Description      timefmt(x, [fmt])  Return a formatted time string based on a numeric timestamp value x.   See  strftime  for formatting options.  Not specifying  fmt  is equivalent to  %FT%TZ .    parsetime(timesharing, [fmt])  The opposite of timefmt() - parse a time format using a given format string    day(timestamp)  Round a Unix timestamp to midnight (00:00) start of the current day.    hour(timestamp)  Round a Unix timestamp to the beginning of the current hour.    minute(timestamp)  Round a Unix timestamp to the beginning of the current minute.    month(timestamp)  Round a unix timestamp to the beginning of the current month.    dayofweek(timestamp)  Convert a Unix timestamp to the day number (Sunday = 0).    dayofmonth(timestamp)  Convert a Unix timestamp to the day of month number (1 .. 31).    dayofyear(timestamp)  Convert a Unix timestamp to the day of year number (0 .. 365).    year(timestamp)  Convert a Unix timestamp to the current year (e.g. 2018).    monthofyear(timestamp)  Convert a Unix timestamp to the current month (0 .. 11).", 
            "title": "List of date/time APPLY functions"
        }, 
        {
            "location": "/Aggregations/#filter\"_\"expressions", 
            "text": "FILTER expressions filter the results using predicates relating to values in the result set.  The FILTER expressions are evaluated post-query and relate to the current state of the pipeline. Thus they can be useful to prune the results based on group calculations. Note that the filters are not indexed and will not speed the processing per se.   Filter expressions follow the syntax of APPLY expressions, with the addition of the conditions  == ,  != ,  ,  = ,  ,  = . Two or more predicates can be combined with logical AND ( ) and OR ( || ). A single predicate can be negated with a NOT prefix ( ! ).    For example, filtering all results where the user name is 'foo' and the age is less than 20 is expressed  as:  FT.AGGREGATE \n  ...\n  FILTER  @name== foo    @age   20 \n  ...  Several filter steps can be added, although at the same stage in the pipeline, it is more efficient to combine several predicates into a single filter step.", 
            "title": "FILTER expressions"
        }, 
        {
            "location": "/Aggregations/#cursor\"_\"api", 
            "text": "FT.AGGREGATE ... WITHCURSOR [COUNT {read size} MAXIDLE {idle timeout}]\nFT.CURSOR READ {idx} {cid} [COUNT {read size}]\nFT.CURSOR DEL {idx} {cid}  You can use cursors with  FT.AGGREGATE , with the  WITHCURSOR  keyword. Cursors allow you to\nconsume only part of the response, allowing you to fetch additional results as needed.\nThis is much quicker than using  LIMIT  with offset, since the query is executed only\nonce, and its state is stored on the server.  To use cursors, specify the  WITHCURSOR  keyword in  FT.AGGREGATE , e.g.  FT.AGGREGATE idx * WITHCURSOR  This will return a response of an array with two elements. The first element is\nthe actual (partial) results, and the second is the cursor ID. The cursor ID\ncan then be fed to  FT.CURSOR READ  repeatedly, until the cursor ID is 0, in\nwhich case all results have been returned.  To read from an existing cursor, use  FT.CURSOR READ , e.g.  FT.CURSOR READ idx 342459320  Assuming  342459320  is the cursor ID returned from the  FT.AGGREGATE  request.  Here is an example in pseudo-code:  response, cursor = FT.AGGREGATE  idx   redis   WITHCURSOR ;\nwhile (1) {\n  processResponse(response)\n  if (!cursor) {\n    break;\n  }\n  response, cursor = FT.CURSOR read  idx  cursor\n}  Note that even if the cursor is 0, a partial result may still be returned.", 
            "title": "Cursor API"
        }, 
        {
            "location": "/Aggregations/#cursor\"_\"settings", 
            "text": "", 
            "title": "Cursor settings"
        }, 
        {
            "location": "/Aggregations/#read\"_\"size", 
            "text": "You can control how many rows are read per each cursor fetch by using the COUNT  parameter. This parameter can be specified both in  FT.AGGREGATE \n(immediately after  WITHCURSOR ) or in  FT.CURSOR READ .  FT.AGGREGATE idx query WITHCURSOR COUNT 10  Will read 10 rows at a time.  You can override this setting by also specifying  COUNT  in  CURSOR READ , e.g.  FT.CURSOR READ idx 342459320 COUNT 50  Will return at most 50 results.  The default read size is 1000", 
            "title": "Read size"
        }, 
        {
            "location": "/Aggregations/#timeouts\"_\"and\"_\"limits", 
            "text": "Because cursors are stateful resources which occupy memory on the server, they\nhave a limited lifetime. In order to safeguard against orphaned/stale cursors,\ncursors have an idle timeout value. If no activity occurs on the cursor before\nthe idle timeout, the cursor is deleted. The idle timer resets to 0 whenever\nthe cursor is read from using  CURSOR READ .  The default idle timeout is 30000 milliseconds (or 30 seconds). You can modify\nthe idle timeout using the  MAXIDLE  keyword when creating the cursor. Note that\nthe value cannot exceed the default 30s.  FT.AGGREGATE idx query WITHCURSOR MAXIDLE 10000  Will set the limit for 10 seconds.", 
            "title": "Timeouts and limits"
        }, 
        {
            "location": "/Aggregations/#other\"_\"cursor\"_\"commands", 
            "text": "Cursors can be explicity deleted using the  CURSOR DEL  command, e.g.  FT.CURSOR DEL idx 342459320  Note that cursors are automatically deleted if all their results have been\nreturned, or if they have been timed out.  All idle cursors can be forcefully purged at once using  FT.CURSOR GC idx 0  command.\nBy default, RediSearch uses a lazy throttled approach to garbage collection, which\ncollects idle cursors every 500 operations, or every second - whichever is later.", 
            "title": "Other cursor commands"
        }, 
        {
            "location": "/Escaping/", 
            "text": "Controlling Text Tokenization and Escaping\n\n\nAt the moment, RediSearch uses a very simple tokenizer for documents and a slightly more sophisticated tokenizer for queries. Both allow a degree of control over string escaping and tokenization. \n\n\nNote: There is a different mechanism for tokenizing text and tag fields, this document refers only to text fields. For tag fields please refer to the \nTag Fields\n documentation. \n\n\nThe rules of text field tokenization\n\n\n\n\n\n\nAll punctuation marks and whitespaces (besides underscores) separate the document and queries into tokens. e.g. any character of \n,.\n{}[]\"':;!@#$%^\n*()-+=~\n will break the text into terms.  So the text \nfoo-bar.baz...bag\n will be tokenized into \n[foo, bar, baz, bag]\n\n\n\n\n\n\nEscaping separators in both queries and documents is done by prepending a backslash to any separator. e.g. the text \nhello\\-world hello-world\n will be tokenized as \n[hello-world, hello, world]\n. \nNOTE\n that in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as \nhello\\\\-world\n. \n\n\n\n\n\n\nUnderscores (\n_\n) are not used as separators in either document or query. So the text \nhello_world\n will remain as is after tokenization. \n\n\n\n\n\n\nRepeating spaces or punctuation marks are stripped. \n\n\n\n\n\n\nIn Latin characters, everything gets converted to lowercase.", 
            "title": "Tokenization and Escaping"
        }, 
        {
            "location": "/Escaping/#controlling\"_\"text\"_\"tokenization\"_\"and\"_\"escaping", 
            "text": "At the moment, RediSearch uses a very simple tokenizer for documents and a slightly more sophisticated tokenizer for queries. Both allow a degree of control over string escaping and tokenization.   Note: There is a different mechanism for tokenizing text and tag fields, this document refers only to text fields. For tag fields please refer to the  Tag Fields  documentation.", 
            "title": "Controlling Text Tokenization and Escaping"
        }, 
        {
            "location": "/Escaping/#the\"_\"rules\"_\"of\"_\"text\"_\"field\"_\"tokenization", 
            "text": "All punctuation marks and whitespaces (besides underscores) separate the document and queries into tokens. e.g. any character of  ,. {}[]\"':;!@#$%^ *()-+=~  will break the text into terms.  So the text  foo-bar.baz...bag  will be tokenized into  [foo, bar, baz, bag]    Escaping separators in both queries and documents is done by prepending a backslash to any separator. e.g. the text  hello\\-world hello-world  will be tokenized as  [hello-world, hello, world] .  NOTE  that in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as  hello\\\\-world .     Underscores ( _ ) are not used as separators in either document or query. So the text  hello_world  will remain as is after tokenization.     Repeating spaces or punctuation marks are stripped.     In Latin characters, everything gets converted to lowercase.", 
            "title": "The rules of text field tokenization"
        }, 
        {
            "location": "/Sorting/", 
            "text": "Sorting by Indexed Fields\n\n\nAs of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name.\n\n\nDeclaring Sortable Fields\n\n\nWhen creating the index with \nFT.CREATE\n, you can declare \nTEXT\n and \nNUMERIC\n properties to be \nSORTABLE\n. When a property is sortable, we can later decide to order the results by its values. For example, in the following schema:\n\n\n FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE\n\n\n\n\n\nThe fields \nlast_name\n and \nage\n are sortable, but \nfirst_name\n isn't. This means we can search by either first and/or last name, and sort by last name or age.\n\n\nNote on sortable TEXT fields\n\n\nIn the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.\n\n\nAlso, note that text fields get normalized and lowercased in a Unicode-safe way when stored for sorting and currently there is no way to change this behaviour. This means that \nAmerica\n and \namerica\n are considered equal in terms of sorting.\n\n\nSpecifying SORTBY\n\n\nIf an index includes sortable fields, you can add the \nSORTBY\n parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If \nWITHSCORES\n is specified along with \nSORTBY\n, the scores returned are simply the relative position of each result in the result set.\n\n\nThe syntax for \nSORTBY\n is:\n\n\nSORTBY {field_name} [ASC|DESC]\n\n\n\n\n\n\n\n\n\nfield_name must be a sortable field defined in the schema.\n\n\n\n\n\n\nASC\n means the order will be ascending, \nDESC\n that it will be descending.\n\n\n\n\n\n\nThe default ordering is \nASC\n if not specified otherwise.\n\n\n\n\n\n\nQuick example\n\n\n FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users\n\n FT.ADD users user1 1.0 FIELDS first_name \nalice\n last_name \njones\n age 35\n\n FT.ADD users user2 1.0 FIELDS first_name \nbob\n last_name \njones\n age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name\n\n FT.SEARCH users \n@last_name:jones\n SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age\n\n FT.SEARCH users \nalice jones\n SORTBY age ASC", 
            "title": "Sortable Values"
        }, 
        {
            "location": "/Sorting/#sorting\"_\"by\"_\"indexed\"_\"fields", 
            "text": "As of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name.", 
            "title": "Sorting by Indexed Fields"
        }, 
        {
            "location": "/Sorting/#declaring\"_\"sortable\"_\"fields", 
            "text": "When creating the index with  FT.CREATE , you can declare  TEXT  and  NUMERIC  properties to be  SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the following schema:   FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE  The fields  last_name  and  age  are sortable, but  first_name  isn't. This means we can search by either first and/or last name, and sort by last name or age.", 
            "title": "Declaring Sortable Fields"
        }, 
        {
            "location": "/Sorting/#note\"_\"on\"_\"sortable\"_\"text\"_\"fields", 
            "text": "In the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.  Also, note that text fields get normalized and lowercased in a Unicode-safe way when stored for sorting and currently there is no way to change this behaviour. This means that  America  and  america  are considered equal in terms of sorting.", 
            "title": "Note on sortable TEXT fields"
        }, 
        {
            "location": "/Sorting/#specifying\"_\"sortby", 
            "text": "If an index includes sortable fields, you can add the  SORTBY  parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If  WITHSCORES  is specified along with  SORTBY , the scores returned are simply the relative position of each result in the result set.  The syntax for  SORTBY  is:  SORTBY {field_name} [ASC|DESC]    field_name must be a sortable field defined in the schema.    ASC  means the order will be ascending,  DESC  that it will be descending.    The default ordering is  ASC  if not specified otherwise.", 
            "title": "Specifying SORTBY"
        }, 
        {
            "location": "/Sorting/#quick\"_\"example", 
            "text": "FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users  FT.ADD users user1 1.0 FIELDS first_name  alice  last_name  jones  age 35  FT.ADD users user2 1.0 FIELDS first_name  bob  last_name  jones  age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name  FT.SEARCH users  @last_name:jones  SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age  FT.SEARCH users  alice jones  SORTBY age ASC", 
            "title": "Quick example"
        }, 
        {
            "location": "/Tags/", 
            "text": "Tag Fields\n\n\nRediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text fields but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax.\n\n\nThe main differences between tag and full-text fields are:\n\n\n\n\n\n\nAn entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one.\n\n\n\n\n\n\nWe do not perform stemming on tag indexes.\n\n\n\n\n\n\nThe tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming.\n\n\n\n\n\n\nTags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document.\n\n\n\n\n\n\nThe index is much simpler and more compressed: We do not store frequencies, offset vectors of field flags. The index contains only document IDs encoded as deltas. This means that an entry in a tag index is usually one or two bytes long. This makes them very memory efficient and fast.\n\n\n\n\n\n\nAn unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024.\n\n\n\n\n\n\nCreating a tag field\n\n\nTag fields can be added to the schema in FT.ADD with the following syntax:\n\n\nFT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}]\n\n\n\n\n\nSEPARATOR defaults to a comma (\n,\n), and can be any printable ASCII character. For example:\n\n\nFT.CREATE idx SCHEMA tags TAG SEPARATOR \n;\n\n\n\n\n\n\nQuerying tag fields\n\n\nAs mentioned above, just searching for a tag without any modifiers will not retrieve documents\ncontaining it.\n\n\nThe syntax for matching tags in a query is as follows (the curly braces are part of the syntax in\nthis case):\n\n\n@\nfield_name\n:{ \ntag\n | \ntag\n | ...}\n\n\ne.g.\n\n\n    @tags:{hello world | foo bar}\n\n\n\n\n\nTag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc. For example:\n\n\nFT.SEARCH idx \n@title:hello @price:[0 100] @tags:{ foo bar | hello world }\n\n\n\n\n\nMultiple tags in a single filter\n\n\nNotice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing \nall\n tags, you should repeat the tag filter several times.\n\n\nFor example, imagine an index of travellers, with a tag field for the cities each traveller has visited:\n\n\nFT.CREATE myIndex SCHEMA name TEXT cities TAG\n\nFT.ADD myIndex user1 1.0 FIELDS name \nJohn Doe\n cities \nNew York, Barcelona, San Francisco\n\n\n\n\n\n\nFor this index, the following query will return all the people who visited \nat least one\n of the following cities:\n\n\nFT.SEARCH myIndex \n@cities:{ New York | Los Angeles | Barcelona }\n\n\n\n\n\n\nBut the next query will return all people who have visited \nall three cities\n:\n\n\n@cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }\n\n\n\n\n\nMulti-word tags And escaping\n\n\nTags can be composed multiple words, or include other punctuation marks other than the field's separator (\n,\n by default). Punctuation marks in tags should be escaped with a backslash (\n\\\n). \nNOTE:\n in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\-world.\n\n\nIt is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.\n\n\nThe following are identical:\n\n\n@tags:{foo\\ bar\\ baz | hello\\ world}\n\n@tags:{foo bar baz | hello world }", 
            "title": "Tag Fields"
        }, 
        {
            "location": "/Tags/#tag\"_\"fields", 
            "text": "RediSearch 0.91 adds a new kind of field - the Tag field. They are similar to full-text fields but use simpler tokenization and encoding in the index. The values in these fields cannot be accessed by general field-less search and can be used only with a special syntax.  The main differences between tag and full-text fields are:    An entire tag field index resides in a single Redis key and doesn't have a key per term as the full-text one.    We do not perform stemming on tag indexes.    The tokenization is simpler: The user can determine a separator (defaults to a comma) for multiple tags, and we only do whitespace trimming at the end of tags. Thus, tags can contain spaces, punctuation marks, accents, etc. The only two transformations we perform are lower-casing (for latin languages only as of now), and whitespace trimming.    Tags cannot be found from a general full-text search. If a document has a field called \"tags\" with the values \"foo\" and \"bar\", searching for foo or bar without a special tag modifier (see below) will not return this document.    The index is much simpler and more compressed: We do not store frequencies, offset vectors of field flags. The index contains only document IDs encoded as deltas. This means that an entry in a tag index is usually one or two bytes long. This makes them very memory efficient and fast.    An unlimited number of tag fields can be created per index, as long as the overall number of fields is under 1024.", 
            "title": "Tag Fields"
        }, 
        {
            "location": "/Tags/#creating\"_\"a\"_\"tag\"_\"field", 
            "text": "Tag fields can be added to the schema in FT.ADD with the following syntax:  FT.CREATE ... SCHEMA ... {field_name} TAG [SEPARATOR {sep}]  SEPARATOR defaults to a comma ( , ), and can be any printable ASCII character. For example:  FT.CREATE idx SCHEMA tags TAG SEPARATOR  ;", 
            "title": "Creating a tag field"
        }, 
        {
            "location": "/Tags/#querying\"_\"tag\"_\"fields", 
            "text": "As mentioned above, just searching for a tag without any modifiers will not retrieve documents\ncontaining it.  The syntax for matching tags in a query is as follows (the curly braces are part of the syntax in\nthis case):  @ field_name :{  tag  |  tag  | ...}  e.g.      @tags:{hello world | foo bar}  Tag clauses can be combined into any sub-clause, used as negative expressions, optional expressions, etc. For example:  FT.SEARCH idx  @title:hello @price:[0 100] @tags:{ foo bar | hello world }", 
            "title": "Querying tag fields"
        }, 
        {
            "location": "/Tags/#multiple\"_\"tags\"_\"in\"_\"a\"_\"single\"_\"filter", 
            "text": "Notice that multiple tags in the same clause create a union of documents containing either tags. To create an intersection of documents containing  all  tags, you should repeat the tag filter several times.  For example, imagine an index of travellers, with a tag field for the cities each traveller has visited:  FT.CREATE myIndex SCHEMA name TEXT cities TAG\n\nFT.ADD myIndex user1 1.0 FIELDS name  John Doe  cities  New York, Barcelona, San Francisco   For this index, the following query will return all the people who visited  at least one  of the following cities:  FT.SEARCH myIndex  @cities:{ New York | Los Angeles | Barcelona }   But the next query will return all people who have visited  all three cities :  @cities:{ New York } @cities:{Los Angeles} @cities:{ Barcelona }", 
            "title": "Multiple tags in a single filter"
        }, 
        {
            "location": "/Tags/#multi-word\"_\"tags\"_\"and\"_\"escaping", 
            "text": "Tags can be composed multiple words, or include other punctuation marks other than the field's separator ( ,  by default). Punctuation marks in tags should be escaped with a backslash ( \\ ).  NOTE:  in most languages you will need an extra backslash when formatting the document or query, to signify an actual backslash, so the actual text in redis-cli for example, will be entered as hello\\-world.  It is also recommended (but not mandatory) to escape spaces; The reason is that if a multi-word tag includes stopwords, it will create a syntax error. So tags like \"to be or not to be\" should be escaped as \"to\\ be\\ or\\ not\\ to\\ be\". For good measure, you can escape all spaces within tags.  The following are identical:  @tags:{foo\\ bar\\ baz | hello\\ world}\n\n@tags:{foo bar baz | hello world }", 
            "title": "Multi-word tags And escaping"
        }, 
        {
            "location": "/Highlight/", 
            "text": "Highlighting API\n\n\nThe highlighting API allows you to have only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters.\n\n\nRediSearch implements high performance highlighting and summarization algorithms, with the following API: \n\n\nCommand syntax\n\n\nFT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}]\n    HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]\n\n\n\n\n\nThere are two sub-commands commands used for highlighting. One is \nHIGHLIGHT\n which surrounds matching text with an open and/or close tag, and the other is \nSUMMARIZE\n which splits a field into contextual fragments surrounding the found terms. It is possible to summarize a field, highlight a field, or perform both actions in the same query.\n\n\nSummarization\n\n\nFT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}]\n\n\n\n\n\nSummarization  will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context.\n\n\nRediSearch can perform summarization using the \nSUMMARIZE\n keyword. If no additional arguments are passed, all \nreturned fields\n are summarized using built-in defaults.\n\n\nThe \nSUMMARIZE\n keyword accepts the following arguments:\n\n\n\n\n\n\nFIELDS\n: If present, must be the first argument. This should be followed\n    by the number of fields to summarize, which itself is followed by a list of\n    fields. Each field present is summarized. If no \nFIELDS\n directive is passed,\n    then \nall\n fields returned are summarized.\n\n\n\n\n\n\nFRAGS\n: How many fragments should be returned. If not specified, a default of 3 is used.\n\n\n\n\n\n\nLEN\n The number of context words each fragment should contain. Context\n    words surround the found term. A higher value will return a larger block of\n    text. If not specified, the default value is 20.\n\n\n\n\n\n\nSEPARATOR\n The string used to divide between individual summary snippets.\n    The default is \n...\n which is common among search engines; but you may\n    override this with any other string if you desire to programmatically divide them\n    later on. You may use a newline sequence, as newlines are stripped from the\n    result body anyway (thus, it will not be conflated with an embedded newline\n    in the text)\n\n\n\n\n\n\nHighlighting\n\n\nFT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]\n\n\n\n\n\nHighlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently.\n\n\nRediSearch can perform highlighting using the \nHIGHLIGHT\n keyword. If no additional arguments are passed, all \nreturned fields\n are highlighted using built-in defaults.\n\n\nThe \nHIGHLIGHT\n keyword accepts the following arguments:\n\n\n\n\n\n\nFIELDS\n If present, must be the first argument. This should be followed\n    by the number of fields to highlight, which itself is followed by a list of\n    fields. Each field present is highlighted. If no \nFIELDS\n directive is passed,\n    then \nall\n fields returned are highlighted.\n\n\n\n\n\n\nTAGS\n If present, must be followed by two strings; the first is prepended\n    to each term match, and the second is appended to it. If no \nTAGS\n are\n    specified, a built-in tag value is appended and prepended.\n\n\n\n\n\n\nField selection\n\n\nIf no specific fields are passed to the \nRETURN\n, \nSUMMARIZE\n, or \nHIGHLIGHT\n keywords, then all of a document's fields are returned. However, if any of these keywords contain a \nFIELD\n directive, then the \nSEARCH\n command will only return the sum total of all fields enumerated in any of those directives.\n\n\nThe \nRETURN\n keyword is treated specially, as it overrides any fields specified in \nSUMMARIZE\n or \nHIGHLIGHT\n.\n\n\nIn the command \nRETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz\n, the fields \nfoo\n is returned as-is, while \nbar\n and \nbaz\n are not returned, because \nRETURN\n was specified, but did not include those fields.\n\n\nIn the command \nSUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz\n, \nbar\n is returned summarized and \nbaz\n is returned highlighted.", 
            "title": "Highlighting Results"
        }, 
        {
            "location": "/Highlight/#highlighting\"_\"api", 
            "text": "The highlighting API allows you to have only the relevant portions of document matching a search query returned as a result. This allows users to quickly see how a document relates to their query, with the search terms highlighted, usually in bold letters.  RediSearch implements high performance highlighting and summarization algorithms, with the following API:", 
            "title": "Highlighting API"
        }, 
        {
            "location": "/Highlight/#command\"_\"syntax", 
            "text": "FT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepstr}]\n    HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]  There are two sub-commands commands used for highlighting. One is  HIGHLIGHT  which surrounds matching text with an open and/or close tag, and the other is  SUMMARIZE  which splits a field into contextual fragments surrounding the found terms. It is possible to summarize a field, highlight a field, or perform both actions in the same query.", 
            "title": "Command syntax"
        }, 
        {
            "location": "/Highlight/#summarization", 
            "text": "FT.SEARCH ...\n    SUMMARIZE [FIELDS {num} {field}] [FRAGS {numFrags}] [LEN {fragLen}] [SEPARATOR {sepStr}]  Summarization  will fragment the text into smaller sized snippets; each snippet will contain the found term(s) and some additional surrounding context.  RediSearch can perform summarization using the  SUMMARIZE  keyword. If no additional arguments are passed, all  returned fields  are summarized using built-in defaults.  The  SUMMARIZE  keyword accepts the following arguments:    FIELDS : If present, must be the first argument. This should be followed\n    by the number of fields to summarize, which itself is followed by a list of\n    fields. Each field present is summarized. If no  FIELDS  directive is passed,\n    then  all  fields returned are summarized.    FRAGS : How many fragments should be returned. If not specified, a default of 3 is used.    LEN  The number of context words each fragment should contain. Context\n    words surround the found term. A higher value will return a larger block of\n    text. If not specified, the default value is 20.    SEPARATOR  The string used to divide between individual summary snippets.\n    The default is  ...  which is common among search engines; but you may\n    override this with any other string if you desire to programmatically divide them\n    later on. You may use a newline sequence, as newlines are stripped from the\n    result body anyway (thus, it will not be conflated with an embedded newline\n    in the text)", 
            "title": "Summarization"
        }, 
        {
            "location": "/Highlight/#highlighting", 
            "text": "FT.SEARCH ... HIGHLIGHT [FIELDS {num} {field}] [TAGS {openTag} {closeTag}]  Highlighting will highlight the found term (and its variants) with a user-defined tag. This may be used to display the matched text in a different typeface using a markup language, or to otherwise make the text appear differently.  RediSearch can perform highlighting using the  HIGHLIGHT  keyword. If no additional arguments are passed, all  returned fields  are highlighted using built-in defaults.  The  HIGHLIGHT  keyword accepts the following arguments:    FIELDS  If present, must be the first argument. This should be followed\n    by the number of fields to highlight, which itself is followed by a list of\n    fields. Each field present is highlighted. If no  FIELDS  directive is passed,\n    then  all  fields returned are highlighted.    TAGS  If present, must be followed by two strings; the first is prepended\n    to each term match, and the second is appended to it. If no  TAGS  are\n    specified, a built-in tag value is appended and prepended.", 
            "title": "Highlighting"
        }, 
        {
            "location": "/Highlight/#field\"_\"selection", 
            "text": "If no specific fields are passed to the  RETURN ,  SUMMARIZE , or  HIGHLIGHT  keywords, then all of a document's fields are returned. However, if any of these keywords contain a  FIELD  directive, then the  SEARCH  command will only return the sum total of all fields enumerated in any of those directives.  The  RETURN  keyword is treated specially, as it overrides any fields specified in  SUMMARIZE  or  HIGHLIGHT .  In the command  RETURN 1 foo SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz , the fields  foo  is returned as-is, while  bar  and  baz  are not returned, because  RETURN  was specified, but did not include those fields.  In the command  SUMMARIZE FIELDS 1 bar HIGHLIGHT FIELDS 1 baz ,  bar  is returned summarized and  baz  is returned highlighted.", 
            "title": "Field selection"
        }, 
        {
            "location": "/Scoring/", 
            "text": "Scoring in RediSearch\n\n\nRediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use \nsortable fields\n. Scoring functions are specified by adding the \nSCORER {scorer_name}\n argument to a search query.\n\n\nIf you prefer a custom scoring function, it is possible to add more functions using the \nExtension API\n.\n\n\nThese are the pre-bundled scoring functions available in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a \nSCORER\n argument in \nFT.SEARCH\n.\n\n\nTFIDF (default)\n\n\nBasic \nTF-IDF scoring\n with a few extra features thrown inside:\n\n\n\n\n\n\nFor each term in each result, we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is \nnormalized by the highest term frequency in each document\n.\n\n\n\n\n\n\nWe multiply the total TF-IDF for the query term by the a priory document score given on \nFT.ADD\n.\n\n\n\n\n\n\nWe give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penalty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared - \n1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...)\n.\n\n\n\n\n\n\nSo for N terms in document D, \nT1...Tn\n, the resulting score could be described with this python function:\n\n\ndef\n \nget_score\n(\nterms\n,\n \ndoc\n):\n\n    \n# the sum of tf-idf\n\n    \nscore\n \n=\n \n0\n\n\n    \n# the distance penalty for all terms\n\n    \ndist_penalty\n \n=\n \n0\n\n\n    \nfor\n \ni\n,\n \nterm\n \nin\n \nenumerate\n(\nterms\n):\n\n        \n# tf normalized by maximum frequency\n\n        \ntf\n \n=\n \ndoc\n.\nfreq\n(\nterm\n)\n \n/\n \ndoc\n.\nmax_freq\n\n\n        \n# idf is global for the index, and not calculated each time in real life\n\n        \nidf\n \n=\n \nlog2\n(\n1\n \n+\n \ntotal_docs\n \n/\n \ndocs_with_term\n(\nterm\n))\n\n\n        \nscore\n \n+=\n \ntf\n*\nidf\n\n\n        \n# sum up the distance penalty\n\n        \nif\n \ni\n \n \n0\n:\n\n            \ndist_penalty\n \n+=\n \nmin_distance\n(\nterm\n,\n \nterms\n[\ni\n-\n1\n])\n**\n2\n\n\n    \n# multiply the score by the document score\n\n    \nscore\n \n*=\n \ndoc\n.\nscore\n\n\n    \n# divide the score by the root of the cumulative distance\n\n    \nif\n \nlen\n(\nterms\n)\n \n \n1\n:\n\n        \nscore\n \n/=\n \nsqrt\n(\ndist_penalty\n)\n\n\n    \nreturn\n \nscore\n\n\n\n\n\n\nTFIDF.DOCNORM\n\n\nIdentical to the default TFIDF scorer, with one important distinction:\n\n\nTerm frequencies are normalized by the length of the document (expressed as the total number of terms). The length is weighted, so that if a document contains two terms, one in a field that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER TFIDF.DOCNORM\n\n\n\n\n\nBM25\n\n\nA variation on the basic TF-IDF scorer, see \nthis Wikipedia article for more info\n.\n\n\nWe also multiply the relevance score for each document by the a priory document score and apply a penalty based on slop as in TFIDF.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER BM25\n\n\n\n\n\nDISMAX\n\n\nA simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied.\n\n\nIt is not a 1 to 1 implementation of \nSolr's DISMAX algorithm\n but follows it in broad terms.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER DISMAX\n\n\n\n\n\nDOCSCORE\n\n\nA scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further.\n\n\nFT.SEARCH myIndex \nfoo\n SCORER DOCSCORE\n\n\n\n\n\nHAMMING\n\n\nScoring by the (inverse) Hamming Distance between the documents' payload and the query payload. Since we are interested in the \nnearest\n neighbors, we inverse the hamming distance (\n1/(1+d)\n) so that a distance of 0 gives a perfect score of 1 and is the highest rank.\n\n\nThis works only if:\n\n\n\n\nThe document has a payload.\n\n\nThe query has a payload.\n\n\nBoth are \nexactly the same length\n.\n\n\n\n\nPayloads are binary-safe, and having payloads with a length that's a multiple of 64 bits yields slightly faster results.\n\n\nExample:\n\n\n127.0.0.1:6379\n FT.CREATE idx SCHEMA foo TEXT\nOK\n127.0.0.1:6379\n FT.ADD idx 1 1 PAYLOAD \naaaabbbb\n FIELDS foo hello\nOK\n127.0.0.1:6379\n FT.ADD idx 2 1 PAYLOAD \naaaacccc\n FIELDS foo bar\nOK\n\n127.0.0.1:6379\n FT.SEARCH idx \n*\n PAYLOAD \naaaabbbc\n SCORER HAMMING WITHSCORES\n1) (integer) 2\n2) \n1\n\n3) \n0.5\n // hamming distance of 1 --\n 1/(1+1) == 0.5\n4) 1) \nfoo\n\n   2) \nhello\n\n5) \n2\n\n6) \n0.25\n // hamming distance of 3 --\n 1/(1+3) == 0.25\n7) 1) \nfoo\n\n   2) \nbar", 
            "title": "Scoring Documents"
        }, 
        {
            "location": "/Scoring/#scoring\"_\"in\"_\"redisearch", 
            "text": "RediSearch comes with a few very basic scoring functions to evaluate document relevance. They are all based on document scores and term frequency. This is regardless of the ability to use  sortable fields . Scoring functions are specified by adding the  SCORER {scorer_name}  argument to a search query.  If you prefer a custom scoring function, it is possible to add more functions using the  Extension API .  These are the pre-bundled scoring functions available in RediSearch and how they work. Each function is mentioned by registered name, that can be passed as a  SCORER  argument in  FT.SEARCH .", 
            "title": "Scoring in RediSearch"
        }, 
        {
            "location": "/Scoring/#tfidf\"_\"default", 
            "text": "Basic  TF-IDF scoring  with a few extra features thrown inside:    For each term in each result, we calculate the TF-IDF score of that term to that document. Frequencies are weighted based on field weights that are pre-determined, and each term's frequency is  normalized by the highest term frequency in each document .    We multiply the total TF-IDF for the query term by the a priory document score given on  FT.ADD .    We give a penalty to each result based on \"slop\" or cumulative distance between the search terms: exact matches will get no penalty, but matches where the search terms are distant see their score reduced significantly. For each 2-gram of consecutive terms, we find the minimal distance between them. The penalty is the square root of the sum of the distances, squared -  1/sqrt(d(t2-t1)^2 + d(t3-t2)^2 + ...) .    So for N terms in document D,  T1...Tn , the resulting score could be described with this python function:  def   get_score ( terms ,   doc ): \n     # the sum of tf-idf \n     score   =   0 \n\n     # the distance penalty for all terms \n     dist_penalty   =   0 \n\n     for   i ,   term   in   enumerate ( terms ): \n         # tf normalized by maximum frequency \n         tf   =   doc . freq ( term )   /   doc . max_freq \n\n         # idf is global for the index, and not calculated each time in real life \n         idf   =   log2 ( 1   +   total_docs   /   docs_with_term ( term )) \n\n         score   +=   tf * idf \n\n         # sum up the distance penalty \n         if   i     0 : \n             dist_penalty   +=   min_distance ( term ,   terms [ i - 1 ]) ** 2 \n\n     # multiply the score by the document score \n     score   *=   doc . score \n\n     # divide the score by the root of the cumulative distance \n     if   len ( terms )     1 : \n         score   /=   sqrt ( dist_penalty ) \n\n     return   score", 
            "title": "TFIDF (default)"
        }, 
        {
            "location": "/Scoring/#tfidfdocnorm", 
            "text": "Identical to the default TFIDF scorer, with one important distinction:  Term frequencies are normalized by the length of the document (expressed as the total number of terms). The length is weighted, so that if a document contains two terms, one in a field that has a weight 1 and one in a field with a weight of 5, the total frequency is 6, not 2.  FT.SEARCH myIndex  foo  SCORER TFIDF.DOCNORM", 
            "title": "TFIDF.DOCNORM"
        }, 
        {
            "location": "/Scoring/#bm25", 
            "text": "A variation on the basic TF-IDF scorer, see  this Wikipedia article for more info .  We also multiply the relevance score for each document by the a priory document score and apply a penalty based on slop as in TFIDF.  FT.SEARCH myIndex  foo  SCORER BM25", 
            "title": "BM25"
        }, 
        {
            "location": "/Scoring/#dismax", 
            "text": "A simple scorer that sums up the frequencies of the matched terms; in the case of union clauses, it will give the maximum value of those matches. No other penalties or factors are applied.  It is not a 1 to 1 implementation of  Solr's DISMAX algorithm  but follows it in broad terms.  FT.SEARCH myIndex  foo  SCORER DISMAX", 
            "title": "DISMAX"
        }, 
        {
            "location": "/Scoring/#docscore", 
            "text": "A scoring function that just returns the a priory score of the document without applying any calculations to it. Since document scores can be updated, this can be useful if you'd like to use an external score and nothing further.  FT.SEARCH myIndex  foo  SCORER DOCSCORE", 
            "title": "DOCSCORE"
        }, 
        {
            "location": "/Scoring/#hamming", 
            "text": "Scoring by the (inverse) Hamming Distance between the documents' payload and the query payload. Since we are interested in the  nearest  neighbors, we inverse the hamming distance ( 1/(1+d) ) so that a distance of 0 gives a perfect score of 1 and is the highest rank.  This works only if:   The document has a payload.  The query has a payload.  Both are  exactly the same length .   Payloads are binary-safe, and having payloads with a length that's a multiple of 64 bits yields slightly faster results.  Example:  127.0.0.1:6379  FT.CREATE idx SCHEMA foo TEXT\nOK\n127.0.0.1:6379  FT.ADD idx 1 1 PAYLOAD  aaaabbbb  FIELDS foo hello\nOK\n127.0.0.1:6379  FT.ADD idx 2 1 PAYLOAD  aaaacccc  FIELDS foo bar\nOK\n\n127.0.0.1:6379  FT.SEARCH idx  *  PAYLOAD  aaaabbbc  SCORER HAMMING WITHSCORES\n1) (integer) 2\n2)  1 \n3)  0.5  // hamming distance of 1 --  1/(1+1) == 0.5\n4) 1)  foo \n   2)  hello \n5)  2 \n6)  0.25  // hamming distance of 3 --  1/(1+3) == 0.25\n7) 1)  foo \n   2)  bar", 
            "title": "HAMMING"
        }, 
        {
            "location": "/Extensions/", 
            "text": "Extending RediSearch\n\n\nRediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time.\n\n\nThere are two kinds of extension APIs at the moment: \n\n\n\n\nQuery Expanders\n, whose role is to expand query tokens (i.e. stemmers).\n\n\nScoring Functions\n, whose role is to rank search results in query time.\n\n\n\n\nRegistering and loading extensions\n\n\nExtensions should be compiled into .so files, and loaded into RediSearch on initialization of the module. \n\n\n\n\n\n\nCompiling \n\n\nExtensions should be compiled and linked as dynamic libraries. An example Makefile for an extension \ncan be found here\n. \n\n\nThat folder also contains an example extension that is used for testing and can be taken as a skeleton for implementing your own extension.\n\n\n\n\n\n\nLoading \n\n\nLoading an extension is done by apending \nEXTLOAD {path/to/ext.so}\n after the \nloadmodule\n configuration directive when loading RediSearch. For example:\n\n\nsh\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so\n\n\nThis causes RediSearch to automatically load the extension and register its expanders and scorers. \n\n\n\n\n\n\nInitializing an extension\n\n\nThe entry point of an extension is a function with the signature:\n\n\nint\n \nRS_ExtensionInit\n(\nRSExtensionCtx\n \n*\nctx\n);\n\n\n\n\n\n\nWhen loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers. \n\n\nIt should return REDISEARCH_ERR on error or REDISEARCH_OK on success.\n\n\nExample init function\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nint\n \nRS_ExtensionInit\n(\nRSExtensionCtx\n \n*\nctx\n)\n \n{\n\n\n  \n/* Register  a scoring function with an alias my_scorer and no special private data and free function */\n\n  \nif\n \n(\nctx\n-\nRegisterScoringFunction\n(\nmy_scorer\n,\n \nMyCustomScorer\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \n/* Register a query expander  */\n\n  \nif\n \n(\nctx\n-\nRegisterQueryExpander\n(\nmy_expander\n,\n \nMyExpander\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n\n      \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \nreturn\n \nREDISEARCH_OK\n;\n\n\n}\n\n\n\n\n\n\nCalling your custom functions\n\n\nWhen performing a query, you can tell RediSearch to use your scorers or expanders by specifying the SCORER or EXPANDER arguments, with the given alias.\ne.g.:\n\n\nFT.SEARCH my_index \nfoo bar\n EXPANDER my_expander SCORER my_scorer\n\n\n\n\n\nNOTE\n: Expander and scorer aliases are \ncase sensitive\n.\n\n\nThe query expander API\n\n\nAt the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.\n\n\nThe API for an expander is the following:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nMyQueryExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \n...\n\n\n}\n\n\n\n\n\n\nRSQueryExpanderCtx\n\n\nRSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:\n\n\ntypedef\n \nstruct\n \nRSQueryExpanderCtx\n \n{\n\n\n  \n/* Opaque query object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQuery\n \n*\nquery\n;\n\n\n  \n/* Opaque query node object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQueryNode\n \n**\ncurrentNode\n;\n\n\n  \n/* Private data of the extension, set on extension initialization */\n\n  \nvoid\n \n*\nprivdata\n;\n\n\n  \n/* The language of the query, defaults to \nenglish\n */\n\n  \nconst\n \nchar\n \n*\nlanguage\n;\n\n\n  \n/* ExpandToken allows the user to add an expansion of the token in the query, that will be\n\n\n   * union-merged with the given token in query time. str is the expanded string, len is its length,\n\n\n   * and flags is a 32 bit flag mask that can be used by the extension to set private information on\n\n\n   * the token */\n\n  \nvoid\n \n(\n*\nExpandToken\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nconst\n \nchar\n \n*\nstr\n,\n \nsize_t\n \nlen\n,\n\n                      \nRSTokenFlags\n \nflags\n);\n\n\n  \n/* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)\n\n\n   */\n\n  \nvoid\n \n(\n*\nSetPayload\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSPayload\n \npayload\n);\n\n\n\n}\n \nRSQueryExpanderCtx\n;\n\n\n\n\n\n\nRSToken\n\n\nRSToken represents a single query token to be expanded and is defined as:\n\n\n/* A token in the query. The expanders receive query tokens and can expand the query with more query\n\n\n * tokens */\n\n\ntypedef\n \nstruct\n \n{\n\n  \n/* The token string - which may or may not be NULL terminated */\n\n  \nconst\n \nchar\n \n*\nstr\n;\n\n  \n/* The token length */\n\n  \nsize_t\n \nlen\n;\n\n\n  \n/* 1 if the token is the result of query expansion */\n\n  \nuint8_t\n \nexpanded\n:\n1\n;\n\n\n  \n/* Extension specific token flags that can be examined later by the scoring function */\n\n  \nRSTokenFlags\n \nflags\n;\n\n\n}\n \nRSToken\n;\n\n\n\n\n\n\nThe scoring function API\n\n\nA scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.\n\n\nSince the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized. \n\n\nA scoring function is applied to each potential result (per document) and is implemented with the following signature:\n\n\ndouble\n \nMyScoringFunction\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nres\n,\n\n                                    \nRSDocumentMetadata\n \n*\ndmd\n,\n \ndouble\n \nminScore\n);\n\n\n\n\n\n\nRSScoringFunctionCtx is a context that implements some helper methods. \n\n\nRSIndexResult is the result information - containing the document id, frequency, terms, and offsets. \n\n\nRSDocumentMetadata is an object holding global information about the document, such as its a-priory score. \n\n\nminSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.\n\n\nThe return value of the function is double representing the final score of the result. \nReturning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. \nTo completely filter out a result and not count it in the totals, the scorer should return the special value \nRS_SCORE_FILTEROUT\n (which is internally set to negative infinity, or -1/0). \n\n\nRSScoringFunctionCtx\n\n\nThis is an object containing the following members:\n\n\n\n\nvoid *privdata\n: a pointer to an object set by the extension on initialization time.\n\n\nRSPayload payload\n: A Payload object set either by the query expander or the client.\n\n\nint GetSlop(RSIndexResult *res)\n: A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.\n\n\n\n\nRSIndexResult\n\n\nThis is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.\n\n\nSee redisearch.h for details\n\n\nRSDocumentMetadata\n\n\nThis is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function. \n\n\nExample query expander\n\n\nThis example query expander expands each token with the term foo:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nDummyExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \nctx\n-\nExpandToken\n(\nctx\n,\n \nstrdup\n(\nfoo\n),\n \nstrlen\n(\nfoo\n),\n \n0x1337\n);\n  \n\n}\n\n\n\n\n\n\nExample scoring function\n\n\nThis is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\ndouble\n \nTFIDFScorer\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nh\n,\n \nRSDocumentMetadata\n \n*\ndmd\n,\n\n                   \ndouble\n \nminScore\n)\n \n{\n\n  \n// no need to evaluate documents with score 0 \n\n  \nif\n \n(\ndmd\n-\nscore\n \n==\n \n0\n)\n \nreturn\n \n0\n;\n\n\n  \n// calculate sum(tf-idf) for each term in the result\n\n  \ndouble\n \ntfidf\n \n=\n \n0\n;\n\n  \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nh\n-\nnumRecords\n;\n \ni\n++\n)\n \n{\n\n    \n// take the term frequency and multiply by the term IDF, add that to the total\n\n    \ntfidf\n \n+=\n \n(\nfloat\n)\nh\n-\nrecords\n[\ni\n].\nfreq\n \n*\n \n(\nh\n-\nrecords\n[\ni\n].\nterm\n \n?\n \nh\n-\nrecords\n[\ni\n].\nterm\n-\nidf\n \n:\n \n0\n);\n\n  \n}\n\n  \n// normalize by the maximal frequency of any term in the document   \n\n  \ntfidf\n \n/=\n  \n(\ndouble\n)\ndmd\n-\nmaxFreq\n;\n\n\n  \n// multiply by the document score (between 0 and 1)\n\n  \ntfidf\n \n*=\n \ndmd\n-\nscore\n;\n\n\n  \n// no need to factor the slop if tfidf is already below minimal score\n\n  \nif\n \n(\ntfidf\n \n \nminScore\n)\n \n{\n\n    \nreturn\n \n0\n;\n\n  \n}\n\n\n  \n// get the slop and divide the result by it, making sure we prefer results with closer terms\n\n  \ntfidf\n \n/=\n \n(\ndouble\n)\nctx\n-\nGetSlop\n(\nh\n);\n\n\n  \nreturn\n \ntfidf\n;\n\n\n}", 
            "title": "Extension API"
        }, 
        {
            "location": "/Extensions/#extending\"_\"redisearch", 
            "text": "RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C (or a language that has an interface with C) and compiled into dynamic libraries that will be loaded at run-time.  There are two kinds of extension APIs at the moment:    Query Expanders , whose role is to expand query tokens (i.e. stemmers).  Scoring Functions , whose role is to rank search results in query time.", 
            "title": "Extending RediSearch"
        }, 
        {
            "location": "/Extensions/#registering\"_\"and\"_\"loading\"_\"extensions", 
            "text": "Extensions should be compiled into .so files, and loaded into RediSearch on initialization of the module.     Compiling   Extensions should be compiled and linked as dynamic libraries. An example Makefile for an extension  can be found here .   That folder also contains an example extension that is used for testing and can be taken as a skeleton for implementing your own extension.    Loading   Loading an extension is done by apending  EXTLOAD {path/to/ext.so}  after the  loadmodule  configuration directive when loading RediSearch. For example:  sh\n$ redis-server --loadmodule ./redisearch.so EXTLOAD ./ext/my_extension.so  This causes RediSearch to automatically load the extension and register its expanders and scorers.", 
            "title": "Registering and loading extensions"
        }, 
        {
            "location": "/Extensions/#initializing\"_\"an\"_\"extension", 
            "text": "The entry point of an extension is a function with the signature:  int   RS_ExtensionInit ( RSExtensionCtx   * ctx );   When loading the extension, RediSearch looks for this function and calls it. This function is responsible for registering and initializing the expanders and scorers.   It should return REDISEARCH_ERR on error or REDISEARCH_OK on success.", 
            "title": "Initializing an extension"
        }, 
        {
            "location": "/Extensions/#example\"_\"init\"_\"function", 
            "text": "#include   redisearch.h  //must be in the include path  int   RS_ExtensionInit ( RSExtensionCtx   * ctx )   { \n\n   /* Register  a scoring function with an alias my_scorer and no special private data and free function */ \n   if   ( ctx - RegisterScoringFunction ( my_scorer ,   MyCustomScorer ,   NULL ,   NULL )   ==   REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   /* Register a query expander  */ \n   if   ( ctx - RegisterQueryExpander ( my_expander ,   MyExpander ,   NULL ,   NULL )   == \n       REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   return   REDISEARCH_OK ;  }", 
            "title": "Example init function"
        }, 
        {
            "location": "/Extensions/#calling\"_\"your\"_\"custom\"_\"functions", 
            "text": "When performing a query, you can tell RediSearch to use your scorers or expanders by specifying the SCORER or EXPANDER arguments, with the given alias.\ne.g.:  FT.SEARCH my_index  foo bar  EXPANDER my_expander SCORER my_scorer  NOTE : Expander and scorer aliases are  case sensitive .", 
            "title": "Calling your custom functions"
        }, 
        {
            "location": "/Extensions/#the\"_\"query\"_\"expander\"_\"api", 
            "text": "At the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.  The API for an expander is the following:  #include   redisearch.h  //must be in the include path  void   MyQueryExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ...  }", 
            "title": "The query expander API"
        }, 
        {
            "location": "/Extensions/#rsqueryexpanderctx", 
            "text": "RSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:  typedef   struct   RSQueryExpanderCtx   { \n\n   /* Opaque query object used internally by the engine, and should not be accessed */ \n   struct   RSQuery   * query ; \n\n   /* Opaque query node object used internally by the engine, and should not be accessed */ \n   struct   RSQueryNode   ** currentNode ; \n\n   /* Private data of the extension, set on extension initialization */ \n   void   * privdata ; \n\n   /* The language of the query, defaults to  english  */ \n   const   char   * language ; \n\n   /* ExpandToken allows the user to add an expansion of the token in the query, that will be     * union-merged with the given token in query time. str is the expanded string, len is its length,     * and flags is a 32 bit flag mask that can be used by the extension to set private information on     * the token */ \n   void   ( * ExpandToken )( struct   RSQueryExpanderCtx   * ctx ,   const   char   * str ,   size_t   len , \n                       RSTokenFlags   flags ); \n\n   /* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)     */ \n   void   ( * SetPayload )( struct   RSQueryExpanderCtx   * ctx ,   RSPayload   payload );  }   RSQueryExpanderCtx ;", 
            "title": "RSQueryExpanderCtx"
        }, 
        {
            "location": "/Extensions/#rstoken", 
            "text": "RSToken represents a single query token to be expanded and is defined as:  /* A token in the query. The expanders receive query tokens and can expand the query with more query   * tokens */  typedef   struct   { \n   /* The token string - which may or may not be NULL terminated */ \n   const   char   * str ; \n   /* The token length */ \n   size_t   len ; \n\n   /* 1 if the token is the result of query expansion */ \n   uint8_t   expanded : 1 ; \n\n   /* Extension specific token flags that can be examined later by the scoring function */ \n   RSTokenFlags   flags ;  }   RSToken ;", 
            "title": "RSToken"
        }, 
        {
            "location": "/Extensions/#the\"_\"scoring\"_\"function\"_\"api", 
            "text": "A scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.  Since the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized.   A scoring function is applied to each potential result (per document) and is implemented with the following signature:  double   MyScoringFunction ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * res , \n                                     RSDocumentMetadata   * dmd ,   double   minScore );   RSScoringFunctionCtx is a context that implements some helper methods.   RSIndexResult is the result information - containing the document id, frequency, terms, and offsets.   RSDocumentMetadata is an object holding global information about the document, such as its a-priory score.   minSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.  The return value of the function is double representing the final score of the result. \nReturning 0 causes the result to be counted, but if there are results with a score greater than 0, they will appear above it. \nTo completely filter out a result and not count it in the totals, the scorer should return the special value  RS_SCORE_FILTEROUT  (which is internally set to negative infinity, or -1/0).", 
            "title": "The scoring function API"
        }, 
        {
            "location": "/Extensions/#rsscoringfunctionctx", 
            "text": "This is an object containing the following members:   void *privdata : a pointer to an object set by the extension on initialization time.  RSPayload payload : A Payload object set either by the query expander or the client.  int GetSlop(RSIndexResult *res) : A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.", 
            "title": "RSScoringFunctionCtx"
        }, 
        {
            "location": "/Extensions/#rsindexresult", 
            "text": "This is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.  See redisearch.h for details", 
            "title": "RSIndexResult"
        }, 
        {
            "location": "/Extensions/#rsdocumentmetadata", 
            "text": "This is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function.", 
            "title": "RSDocumentMetadata"
        }, 
        {
            "location": "/Extensions/#example\"_\"query\"_\"expander", 
            "text": "This example query expander expands each token with the term foo:  #include   redisearch.h  //must be in the include path  void   DummyExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ctx - ExpandToken ( ctx ,   strdup ( foo ),   strlen ( foo ),   0x1337 );    }", 
            "title": "Example query expander"
        }, 
        {
            "location": "/Extensions/#example\"_\"scoring\"_\"function", 
            "text": "This is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:  #include   redisearch.h  //must be in the include path  double   TFIDFScorer ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * h ,   RSDocumentMetadata   * dmd , \n                    double   minScore )   { \n   // no need to evaluate documents with score 0  \n   if   ( dmd - score   ==   0 )   return   0 ; \n\n   // calculate sum(tf-idf) for each term in the result \n   double   tfidf   =   0 ; \n   for   ( int   i   =   0 ;   i     h - numRecords ;   i ++ )   { \n     // take the term frequency and multiply by the term IDF, add that to the total \n     tfidf   +=   ( float ) h - records [ i ]. freq   *   ( h - records [ i ]. term   ?   h - records [ i ]. term - idf   :   0 ); \n   } \n   // normalize by the maximal frequency of any term in the document    \n   tfidf   /=    ( double ) dmd - maxFreq ; \n\n   // multiply by the document score (between 0 and 1) \n   tfidf   *=   dmd - score ; \n\n   // no need to factor the slop if tfidf is already below minimal score \n   if   ( tfidf     minScore )   { \n     return   0 ; \n   } \n\n   // get the slop and divide the result by it, making sure we prefer results with closer terms \n   tfidf   /=   ( double ) ctx - GetSlop ( h ); \n\n   return   tfidf ;  }", 
            "title": "Example scoring function"
        }, 
        {
            "location": "/Stemming/", 
            "text": "Stemming Support\n\n\nRediSearch supports stemming - that is adding the base form of a word to the index. This allows the query for \"going\" to also return results for \"go\" and \"gone\", for example.\n\n\nThe current stemming support is based on the Snowball stemmer library, which supports most European languages, as well as Arabic and other. We hope to include more languages soon (if you need a specific language support, please open an issue).\n\n\nFor further details see the \nSnowball Stemmer website\n.\n\n\nSupported languages\n\n\nThe following languages are supported and can be passed to the engine when indexing or querying, with lowercase letters:\n\n\n\n\narabic\n\n\ndanish\n\n\ndutch\n\n\nenglish\n\n\nfinnish\n\n\nfrench\n\n\ngerman\n\n\nhungarian\n\n\nitalian\n\n\nnorwegian\n\n\nportuguese\n\n\nromanian\n\n\nrussian\n\n\nspanish\n\n\nswedish\n\n\ntamil\n\n\nturkish\n\n\nchinese (see below)\n\n\n\n\nChinese support\n\n\nIndexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese.\n\n\nChinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match.\n\n\nRediSearch makes use of the \nFriso\n chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required.\n\n\nUsing custom dictionaries\n\n\nIf you wish to use a custom dictionary, you can do so at the module level when loading the module. The \nFRISOINI\n setting can point to the location of a \nfriso.ini\n file which contains the relevant settings and paths to the dictionary files.\n\n\nNote that there is no \"default\" friso.ini file location. RedisSearch comes with its own \nfriso.ini\n and dictionary files which are compiled into the module binary at build-time.", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#stemming\"_\"support", 
            "text": "RediSearch supports stemming - that is adding the base form of a word to the index. This allows the query for \"going\" to also return results for \"go\" and \"gone\", for example.  The current stemming support is based on the Snowball stemmer library, which supports most European languages, as well as Arabic and other. We hope to include more languages soon (if you need a specific language support, please open an issue).  For further details see the  Snowball Stemmer website .", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#supported\"_\"languages", 
            "text": "The following languages are supported and can be passed to the engine when indexing or querying, with lowercase letters:   arabic  danish  dutch  english  finnish  french  german  hungarian  italian  norwegian  portuguese  romanian  russian  spanish  swedish  tamil  turkish  chinese (see below)", 
            "title": "Supported languages"
        }, 
        {
            "location": "/Stemming/#chinese\"_\"support", 
            "text": "Indexing a Chinese document is different than indexing a document in most other languages because of how tokens are extracted. While most languages can have their tokens distinguished by separation characters and whitespace, this is not common in Chinese.  Chinese tokenization is done by scanning the input text and checking every character or sequence of characters against a dictionary of predefined terms and determining the most likely (based on the surrounding terms and characters) match.  RediSearch makes use of the  Friso  chinese tokenization library for this purpose. This is largely transparent to the user and often no additional configuration is required.", 
            "title": "Chinese support"
        }, 
        {
            "location": "/Stemming/#using\"_\"custom\"_\"dictionaries", 
            "text": "If you wish to use a custom dictionary, you can do so at the module level when loading the module. The  FRISOINI  setting can point to the location of a  friso.ini  file which contains the relevant settings and paths to the dictionary files.  Note that there is no \"default\" friso.ini file location. RedisSearch comes with its own  friso.ini  and dictionary files which are compiled into the module binary at build-time.", 
            "title": "Using custom dictionaries"
        }, 
        {
            "location": "/Synonyms/", 
            "text": "Synonyms Support\n\n\nOverview\n\n\nRediSearch supports synonyms - that is searching for synonyms words defined by the synonym data structure.\n\n\nThe synonym data structure is a set of groups, each group contains synonym terms. For example, the following synonym data structure contains three groups, each group contains three synonym terms:\n\n\n{boy, child, baby}\n{girl, child, baby}\n{man, person, adult}\n\n\n\n\n\nWhen these three groups are located inside the synonym data structure, it is possible to search for 'child' and receive documents contains 'boy', 'girl', 'child' and 'baby'.\n\n\nThe synonym search technique\n\n\nWe use a simple HashMap to map between the terms and the group ids. During building the index, we check if the current term appears in the synonym map, and if it does we take all the group ids that the term belongs to.\n\n\nFor each group id, we add another record to the inverted index called \"\\~\\\nid>\" that contains the same information as the term itself. When performing a search, we check if the searched term appears in the synonym map, and if it does we take all the group ids the term is belong to. For each group id, we search for \"\\~\\\nid>\" and return the combined results. This technique ensures that we return all the synonyms of a given term.\n\n\nHandling concurrency\n\n\nSince the indexing is performed in a separate thread, the synonyms map may change during the indexing, which in turn may cause data corruption or crashes during indexing/searches. To solve this issue, we create a read-only copy for indexing purposes. The read-only copy is maintained using ref count.\n\n\nAs long as the synonyms map does not change, the original synonym map holds a reference to its read-only copy so it will not be freed. Once the data inside the synonyms map has changed, the synonyms map decreses the reference count of its read only copy. This ensures that when all the indexers are done using the read only copy, then the read only copy will automatically freed. Also it ensures that the next time an indexer asks for a read-only copy, the synonyms map will create a new copy (contains the new data) and return it.", 
            "title": "Synonyms Support"
        }, 
        {
            "location": "/Synonyms/#synonyms\"_\"support", 
            "text": "", 
            "title": "Synonyms Support"
        }, 
        {
            "location": "/Synonyms/#overview", 
            "text": "RediSearch supports synonyms - that is searching for synonyms words defined by the synonym data structure.  The synonym data structure is a set of groups, each group contains synonym terms. For example, the following synonym data structure contains three groups, each group contains three synonym terms:  {boy, child, baby}\n{girl, child, baby}\n{man, person, adult}  When these three groups are located inside the synonym data structure, it is possible to search for 'child' and receive documents contains 'boy', 'girl', 'child' and 'baby'.", 
            "title": "Overview"
        }, 
        {
            "location": "/Synonyms/#the\"_\"synonym\"_\"search\"_\"technique", 
            "text": "We use a simple HashMap to map between the terms and the group ids. During building the index, we check if the current term appears in the synonym map, and if it does we take all the group ids that the term belongs to.  For each group id, we add another record to the inverted index called \"\\~\\ id>\" that contains the same information as the term itself. When performing a search, we check if the searched term appears in the synonym map, and if it does we take all the group ids the term is belong to. For each group id, we search for \"\\~\\ id>\" and return the combined results. This technique ensures that we return all the synonyms of a given term.", 
            "title": "The synonym search technique"
        }, 
        {
            "location": "/Synonyms/#handling\"_\"concurrency", 
            "text": "Since the indexing is performed in a separate thread, the synonyms map may change during the indexing, which in turn may cause data corruption or crashes during indexing/searches. To solve this issue, we create a read-only copy for indexing purposes. The read-only copy is maintained using ref count.  As long as the synonyms map does not change, the original synonym map holds a reference to its read-only copy so it will not be freed. Once the data inside the synonyms map has changed, the synonyms map decreses the reference count of its read only copy. This ensures that when all the indexers are done using the read only copy, then the read only copy will automatically freed. Also it ensures that the next time an indexer asks for a read-only copy, the synonyms map will create a new copy (contains the new data) and return it.", 
            "title": "Handling concurrency"
        }, 
        {
            "location": "/payloads/", 
            "text": "Document Payloads\n\n\nUsually, RediSearch stores documents as hash keys. But if you want to access some data for aggregation or scoring functions, we might want to store that data as an inline payload. This will allow us to evaluate properties of a document for scoring purposes at very low cost.\n\n\nSince the scoring functions already have access to the DocumentMetaData, which contains document flags and score, We can add custom payloads that can be evaluated in run-time.\n\n\nPayloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose of evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, if you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.\n\n\nAdding payloads for documents\n\n\nWhen inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload. This is done with the \nPAYLOAD\n keyword:\n\n\nFT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...\n\n\n\n\n\nEvaluating payloads in query time\n\n\nWhen implementing a scoring function, the signature of the function exposed is:\n\n\ndouble\n \n(\n*\nScoringFunction\n)(\nDocumentMetadata\n \n*\ndmd\n,\n \nIndexResult\n \n*\nh\n);\n\n\n\n\n\n\n\n\nNote\n\n\nCurrently, scoring functions cannot be dynamically added, and forking the engine and replacing them is required.\n\n\n\n\nDocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with arbitrary length:\n\n\ntypedef\n \nstruct\n  \n{\n\n    \nchar\n \n*\ndata\n,\n\n    \nuint32_t\n \nlen\n;\n\n\n}\n \nDocumentPayload\n;\n\n\n\n\n\n\nIf no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it. It is recommended to encode some metadata about the payload inside it, like a leading version number, etc.\n\n\nRetrieving payloads from documents\n\n\nWhen searching, it is possible to request the document payloads from the engine. \n\n\nThis is done by adding the keyword \nWITHPAYLOADS\n to \nFT.SEARCH\n. \n\n\nIf \nWITHPAYLOADS\n is set, the payloads follow the document id in the returned result. \nIf \nWITHSCORES\n is set as well, the payloads follow the scores. e.g.:\n\n\n127.0.0.1:6379\n FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379\n FT.ADD foo doc2 1.0 PAYLOAD \nhi there!\n FIELDS bar \nhello\n\nOK\n127.0.0.1:6379\n FT.SEARCH foo \nhello\n WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2) \ndoc2\n           # id\n3) \n1\n              # score\n4) \nhi there!\n      # payload\n5) 1) \nbar\n         # fields\n   2) \nhello", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#document\"_\"payloads", 
            "text": "Usually, RediSearch stores documents as hash keys. But if you want to access some data for aggregation or scoring functions, we might want to store that data as an inline payload. This will allow us to evaluate properties of a document for scoring purposes at very low cost.  Since the scoring functions already have access to the DocumentMetaData, which contains document flags and score, We can add custom payloads that can be evaluated in run-time.  Payloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose of evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, if you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#adding\"_\"payloads\"_\"for\"_\"documents", 
            "text": "When inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload. This is done with the  PAYLOAD  keyword:  FT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...", 
            "title": "Adding payloads for documents"
        }, 
        {
            "location": "/payloads/#evaluating\"_\"payloads\"_\"in\"_\"query\"_\"time", 
            "text": "When implementing a scoring function, the signature of the function exposed is:  double   ( * ScoringFunction )( DocumentMetadata   * dmd ,   IndexResult   * h );    Note  Currently, scoring functions cannot be dynamically added, and forking the engine and replacing them is required.   DocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with arbitrary length:  typedef   struct    { \n     char   * data , \n     uint32_t   len ;  }   DocumentPayload ;   If no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it. It is recommended to encode some metadata about the payload inside it, like a leading version number, etc.", 
            "title": "Evaluating payloads in query time"
        }, 
        {
            "location": "/payloads/#retrieving\"_\"payloads\"_\"from\"_\"documents", 
            "text": "When searching, it is possible to request the document payloads from the engine.   This is done by adding the keyword  WITHPAYLOADS  to  FT.SEARCH .   If  WITHPAYLOADS  is set, the payloads follow the document id in the returned result. \nIf  WITHSCORES  is set as well, the payloads follow the scores. e.g.:  127.0.0.1:6379  FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379  FT.ADD foo doc2 1.0 PAYLOAD  hi there!  FIELDS bar  hello \nOK\n127.0.0.1:6379  FT.SEARCH foo  hello  WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2)  doc2            # id\n3)  1               # score\n4)  hi there!       # payload\n5) 1)  bar          # fields\n   2)  hello", 
            "title": "Retrieving payloads from documents"
        }, 
        {
            "location": "/Spellcheck/", 
            "text": "Query Spelling Correction\n\n\nQuery spelling correction, a.k.a \"did you mean\", provides suggestions for misspelled search terms. For example, the term 'reids' may be a misspelled 'redis'.\n\n\nIn such cases and as of v1.4 RediSearch can be used for generating alternatives to misspelled query terms. A misspelled term is a full text term (i.e., a word) that is:\n\n\n\n\nNot a stop word\n\n\nNot in the index\n\n\nAt least 3 characters long\n\n\n\n\nThe alternatives for a misspelled term are generated from the corpus of already-indexed terms and, optionally, one or more custom dictionaries. Alternatives become spelling suggestions based on their respective Levenshtein distances (LD) from the misspelled term. Each spelling suggestion is given a normalized score based on its occurances in the index.\n\n\nTo obtain the spelling corrections for a query, refer to the documentation of the \nFT.SPELLCHECK\n command.\n\n\nCustom dictionaries\n\n\nA dictionary is a set of terms. Dictionaries can be added with terms, have terms deleted from them and have their entire contents dumped using the \nFT.DICTADD\n, \nFT.DICTDEL\n and \nFT.DICTDUMP\n commands, respectively.\n\n\nDictionaries can be used to modify the behavior of RediSearch's query spelling correction, by including or excluding their contents from potential spelling correction suggestions.\n\n\nWhen used for term inclusion, the terms in a dictionary can be provided as spelling suggestions regardless their occurances (or lack of) in the index. Scores of suggestions from inclusion dictionaries are always 0. \n\n\nConversely, terms in an exlusion dictionary will never be returned as spelling alternatives.", 
            "title": "Spelling Correction"
        }, 
        {
            "location": "/Spellcheck/#query\"_\"spelling\"_\"correction", 
            "text": "Query spelling correction, a.k.a \"did you mean\", provides suggestions for misspelled search terms. For example, the term 'reids' may be a misspelled 'redis'.  In such cases and as of v1.4 RediSearch can be used for generating alternatives to misspelled query terms. A misspelled term is a full text term (i.e., a word) that is:   Not a stop word  Not in the index  At least 3 characters long   The alternatives for a misspelled term are generated from the corpus of already-indexed terms and, optionally, one or more custom dictionaries. Alternatives become spelling suggestions based on their respective Levenshtein distances (LD) from the misspelled term. Each spelling suggestion is given a normalized score based on its occurances in the index.  To obtain the spelling corrections for a query, refer to the documentation of the  FT.SPELLCHECK  command.", 
            "title": "Query Spelling Correction"
        }, 
        {
            "location": "/Spellcheck/#custom\"_\"dictionaries", 
            "text": "A dictionary is a set of terms. Dictionaries can be added with terms, have terms deleted from them and have their entire contents dumped using the  FT.DICTADD ,  FT.DICTDEL  and  FT.DICTDUMP  commands, respectively.  Dictionaries can be used to modify the behavior of RediSearch's query spelling correction, by including or excluding their contents from potential spelling correction suggestions.  When used for term inclusion, the terms in a dictionary can be provided as spelling suggestions regardless their occurances (or lack of) in the index. Scores of suggestions from inclusion dictionaries are always 0.   Conversely, terms in an exlusion dictionary will never be returned as spelling alternatives.", 
            "title": "Custom dictionaries"
        }, 
        {
            "location": "/Phonetic_Matching/", 
            "text": "Phonetic Matching\n\n\nPhonetic matching, a.k.a \"Jon or John\", allows searching for terms based on their pronounciation. This capability can be a useful tool when searching for names of people.\n\n\nPhonetic matching is based on the use of a phonetic algorithm. A phonetic algorithm transforms the input term to an approximate representation of its pronounciation. This allows indexing terms, and consequently searching, by their pronounciation.\n\n\nAs of v1.4 RediSearch provides phonetic matching via the definition of text fields with the \nPHONETIC\n attribute. This causes the terms in such fields to be indexed both by their textual value as well as their phonetic approximation.\n\n\nPerforming a search on \nPHONETIC\n fields will, by default, also return results for phonetically similar terms. This behavior can be controlled with the \n$phonetic\n query attribute\n.\n\n\nPhonetic algorithms support\n\n\nRediSearch currently supports a single phonetic algorithm, the \nDouble Metaphone\n (DM). It uses the implementation at \nslacy/double-metaphone\n, which provides general support for Latin languages.", 
            "title": "Phonetic Matching"
        }, 
        {
            "location": "/Phonetic_Matching/#phonetic\"_\"matching", 
            "text": "Phonetic matching, a.k.a \"Jon or John\", allows searching for terms based on their pronounciation. This capability can be a useful tool when searching for names of people.  Phonetic matching is based on the use of a phonetic algorithm. A phonetic algorithm transforms the input term to an approximate representation of its pronounciation. This allows indexing terms, and consequently searching, by their pronounciation.  As of v1.4 RediSearch provides phonetic matching via the definition of text fields with the  PHONETIC  attribute. This causes the terms in such fields to be indexed both by their textual value as well as their phonetic approximation.  Performing a search on  PHONETIC  fields will, by default, also return results for phonetically similar terms. This behavior can be controlled with the  $phonetic  query attribute .", 
            "title": "Phonetic Matching"
        }, 
        {
            "location": "/Phonetic_Matching/#phonetic\"_\"algorithms\"_\"support", 
            "text": "RediSearch currently supports a single phonetic algorithm, the  Double Metaphone  (DM). It uses the implementation at  slacy/double-metaphone , which provides general support for Latin languages.", 
            "title": "Phonetic algorithms support"
        }, 
        {
            "location": "/Clients/", 
            "text": "RediSearch Client Libraries\n\n\nRediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages. \n\n\nWhile it is possible and simple to use the raw Redis commands API, in most cases it's easier to just use a client library abstracting it. \n\n\nCurrently available Libraries\n\n\n\n\n\n\n\n\nLanguage\n\n\nLibrary\n\n\nAuthor\n\n\nLicense\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nredisearch-py\n\n\nRedis Labs\n\n\nBSD\n\n\nUsually the most up-to-date client library\n\n\n\n\n\n\nJava\n\n\nJRediSearch\n\n\nRedis Labs\n\n\nBSD\n\n\n\n\n\n\n\n\nGo\n\n\nredisearch-go\n\n\nRedis Labs\n\n\nBSD\n\n\nIncomplete API\n\n\n\n\n\n\nJavaScript\n\n\nRedRediSearch\n\n\nKyle J. Davis\n\n\nMIT\n\n\nPartial API, compatible with \nReds\n\n\n\n\n\n\nC#\n\n\nNRediSearch\n\n\nMarc Gravell\n\n\nMIT\n\n\nPart of StackExchange.Redis\n\n\n\n\n\n\nPHP\n\n\nredisearch-php\n\n\nEthan Hann\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby on Rails\n\n\nredi_search_rails\n\n\nDmitry Polyakovsky\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby\n\n\nredisearch-rb\n\n\nVictor Ruiz\n\n\nMIT", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/Clients/#redisearch\"_\"client\"_\"libraries", 
            "text": "RediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages.   While it is possible and simple to use the raw Redis commands API, in most cases it's easier to just use a client library abstracting it.", 
            "title": "RediSearch Client Libraries"
        }, 
        {
            "location": "/Clients/#currently\"_\"available\"_\"libraries", 
            "text": "Language  Library  Author  License  Comments      Python  redisearch-py  Redis Labs  BSD  Usually the most up-to-date client library    Java  JRediSearch  Redis Labs  BSD     Go  redisearch-go  Redis Labs  BSD  Incomplete API    JavaScript  RedRediSearch  Kyle J. Davis  MIT  Partial API, compatible with  Reds    C#  NRediSearch  Marc Gravell  MIT  Part of StackExchange.Redis    PHP  redisearch-php  Ethan Hann  MIT     Ruby on Rails  redi_search_rails  Dmitry Polyakovsky  MIT     Ruby  redisearch-rb  Victor Ruiz  MIT", 
            "title": "Currently available Libraries"
        }, 
        {
            "location": "/python_client/", 
            "text": "Package redisearch Documentation\n\n\nOverview\n\n\nredisearch-py\n is a python search engine library that utilizes the RediSearch Redis Module API.\n\n\nIt is the \"official\" client of RediSearch, and should be regarded as its canonical client implementation.\n\n\nThe source code can be found at \nhttp://github.com/RedisLabs/redisearch-py\n\n\nExample: Using the Python Client\n\n\nfrom\n \nredisearch\n \nimport\n \nClient\n,\n \nTextField\n,\n \nNumericField\n,\n \nQuery\n\n\n\n# Creating a client with a given index name\n\n\nclient\n \n=\n \nClient\n(\nmyIndex\n)\n\n\n\n# Creating the index definition and schema\n\n\nclient\n.\ncreate_index\n([\nTextField\n(\ntitle\n,\n \nweight\n=\n5.0\n),\n \nTextField\n(\nbody\n)])\n\n\n\n# Indexing a document\n\n\nclient\n.\nadd_document\n(\ndoc1\n,\n \ntitle\n \n=\n \nRediSearch\n,\n \nbody\n \n=\n \nRedisearch implements a search engine on top of redis\n)\n\n\n\n# Simple search\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n)\n\n\n\n# the result has the total number of results, and a list of documents\n\n\nprint\n \nres\n.\ntotal\n \n# \n1\n\n\nprint\n \nres\n.\ndocs\n[\n0\n]\n.\ntitle\n\n\n\n# Searching with snippets\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n,\n \nsnippet_sizes\n \n=\n \n{\nbody\n:\n \n50\n})\n\n\n\n# Searching with complex parameters:\n\n\nq\n \n=\n \nQuery\n(\nsearch engine\n)\n.\nverbatim\n()\n.\nno_content\n()\n.\npaging\n(\n0\n,\n5\n)\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nq\n)\n\n\n\n\n\n\nExample: Using the Auto Completer Client:\n\n\n# Using the auto-completer\n\n\nac\n \n=\n \nAutoCompleter\n(\nac\n)\n\n\n\n# Adding some terms\n\n\nac\n.\nadd_suggestions\n(\nSuggestion\n(\nfoo\n,\n \n5.0\n),\n \nSuggestion\n(\nbar\n,\n \n1.0\n))\n\n\n\n# Getting suggestions\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n)\n \n# returns nothing\n\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n,\n \nfuzzy\n \n=\n \nTrue\n)\n \n# returns [\nfoo\n]\n\n\n\n\n\n\nInstalling\n\n\n\n\n\n\nInstall Redis 4.0 or above\n\n\n\n\n\n\nInstall RediSearch\n\n\n\n\n\n\nInstall the python client\n\n\n\n\n\n\n$ pip install redisearch\n\n\n\n\n\nClass AutoCompleter\n\n\nA client to RediSearch's AutoCompleter API\n\n\nIt provides prefix searches with optionally fuzzy matching of prefixes    \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nkey\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new AutoCompleter client for the given key, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_suggestions\n\n\ndef\n \nadd_suggestions\n(\nself\n,\n \n*\nsuggestions\n,\n \n**\nkwargs\n)\n\n\n\n\n\n\nAdd suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.\n\n\nIf kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores\n\n\ndelete\n\n\ndef\n \ndelete\n(\nself\n,\n \nstring\n)\n\n\n\n\n\n\nDelete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise\n\n\nget_suggestions\n\n\ndef\n \nget_suggestions\n(\nself\n,\n \nprefix\n,\n \nfuzzy\n=\nFalse\n,\n \nnum\n=\n10\n,\n \nwith_scores\n=\nFalse\n,\n \nwith_payloads\n=\nFalse\n)\n\n\n\n\n\n\nGet a list of suggestions from the AutoCompleter, for a given prefix\n\n\nParameters:\n\n\n\n\nprefix\n: the prefix we are searching. \nMust be valid ascii or utf-8\n\n\nfuzzy\n: If set to true, the prefix search is done in fuzzy mode.\n    \nNOTE\n: Running fuzzy searches on short (\n3 letters) prefixes can be very slow, and even scan the entire index.\n\n\nwith_scores\n: if set to true, we also return the (refactored) score of each suggestion.\n  This is normally not needed, and is NOT the original score inserted into the index\n\n\nwith_payloads\n: Return suggestion payloads\n\n\nnum\n: The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.\n\n\n\n\nReturns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.\n\n\nlen\n\n\ndef\n \nlen\n(\nself\n)\n\n\n\n\n\n\nReturn the number of entries in the AutoCompleter index\n\n\nClass Client\n\n\nA client for the RediSearch module.\nIt abstracts the API of the module and lets you just use the engine\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nindex_name\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new Client for the given index_name, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \npartial\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a single document to the index.\n\n\nParameters\n\n\n\n\ndoc_id\n: the id of the saved document.\n\n\nnosave\n: if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.\n\n\nscore\n: the document ranking, between 0.0 and 1.0\n\n\npayload\n: optional inner-index payload we can save for fast access in scoring functions\n\n\nreplace\n: if True, and the document already is in the index, we perform an update and reindex the document\n\n\npartial\n: if True, the fields specified will be added to the existing document.\n               This has the added benefit that any fields specified with \nno_index\n\n               will not be reindexed again. Implies \nreplace\n\n\nfields\n kwargs dictionary of the document fields to be saved and/or indexed.\n             NOTE: Geo points should be encoded as strings of \"lon,lat\"\n\n\n\n\nbatch_indexer\n\n\ndef\n \nbatch_indexer\n(\nself\n,\n \nchunk_size\n=\n100\n)\n\n\n\n\n\n\nCreate a new batch indexer from the client with a given chunk size\n\n\ncreate_index\n\n\ndef\n \ncreate_index\n(\nself\n,\n \nfields\n,\n \nno_term_offsets\n=\nFalse\n,\n \nno_field_flags\n=\nFalse\n,\n \nstopwords\n=\nNone\n)\n\n\n\n\n\n\nCreate the search index. Creating an existing index just updates its properties\n\n\nParameters:\n\n\n\n\nfields\n: a list of TextField or NumericField objects\n\n\nno_term_offsets\n: If true, we will not save term offsets in the index\n\n\nno_field_flags\n: If true, we will not save field flags that allow searching in specific fields\n\n\nstopwords\n: If not None, we create the index with this custom stopword list. The list can be empty\n\n\n\n\ndelete_document\n\n\ndef\n \ndelete_document\n(\nself\n,\n \ndoc_id\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nDelete a document from index\nReturns 1 if the document was deleted, 0 if not\n\n\ndrop_index\n\n\ndef\n \ndrop_index\n(\nself\n)\n\n\n\n\n\n\nDrop the index if it exists\n\n\nexplain\n\n\ndef\n \nexplain\n(\nself\n,\n \nquery\n)\n\n\n\n\n\n\ninfo\n\n\ndef\n \ninfo\n(\nself\n)\n\n\n\n\n\n\nGet info an stats about the the current index, including the number of documents, memory consumption, etc\n\n\nload_document\n\n\ndef\n \nload_document\n(\nself\n,\n \nid\n)\n\n\n\n\n\n\nLoad a single document by id\n\n\nsearch\n\n\ndef\n \nsearch\n(\nself\n,\n \nquery\n)\n\n\n\n\n\n\nSearch the index for a given query, and return a result of documents\n\n\nParameters\n\n\n\n\nquery\n: the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format\n\n\nsnippet_sizes\n: A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}\n\n\n\n\nClass BatchIndexer\n\n\nA batch indexer allows you to automatically batch\ndocument indexing in pipelines, flushing it every N documents.\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nclient\n,\n \nchunk_size\n=\n1000\n)\n\n\n\n\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \npartial\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a document to the batch query\n\n\ncommit\n\n\ndef\n \ncommit\n(\nself\n)\n\n\n\n\n\n\nManually commit and flush the batch indexing query\n\n\nClass Document\n\n\nRepresents a single document in a result set\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nid\n,\n \npayload\n=\nNone\n,\n \n**\nfields\n)\n\n\n\n\n\n\nClass GeoField\n\n\nGeoField is used to define a geo-indexing field in a schema definition\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass GeoFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nlon\n,\n \nlat\n,\n \nradius\n,\n \nunit\n=\nkm\n)\n\n\n\n\n\n\nClass NumericField\n\n\nNumericField is used to define a numeric field in a schema definition\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nsortable\n=\nFalse\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass NumericFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nminval\n,\n \nmaxval\n,\n \nminExclusive\n=\nFalse\n,\n \nmaxExclusive\n=\nFalse\n)\n\n\n\n\n\n\nClass Query\n\n\nQuery is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.\n\n\nThe setter functions return the query object, so they can be chained,\ni.e. \nQuery(\"foo\").verbatim().filter(...)\n etc.\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nquery_string\n)\n\n\n\n\n\n\nCreate a new query object.\nThe query string is set in the constructor, and other options have setter functions.\n\n\nadd_filter\n\n\ndef\n \nadd_filter\n(\nself\n,\n \nflt\n)\n\n\n\n\n\n\nAdd a numeric or geo filter to the query.\n\nCurrently only one of each filter is supported by the engine\n\n\n\n\nflt\n: A NumericFilter or GeoFilter object, used on a corresponding field\n\n\n\n\nget_args\n\n\ndef\n \nget_args\n(\nself\n)\n\n\n\n\n\n\nFormat the redis arguments for this query and return them\n\n\nhighlight\n\n\ndef\n \nhighlight\n(\nself\n,\n \nfields\n=\nNone\n,\n \ntags\n=\nNone\n)\n\n\n\n\n\n\nApply specified markup to matched term(s) within the returned field(s)\n\n\n\n\nfields\n If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted\n\n\ntags\n A list of two strings to surround the match.\n\n\n\n\nin_order\n\n\ndef\n \nin_order\n(\nself\n)\n\n\n\n\n\n\nMatch only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'\n\n\nlimit_fields\n\n\ndef\n \nlimit_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nLimit the search to specific TEXT fields only\n\n\n\n\nfields\n: A list of strings, case sensitive field names from the defined schema\n\n\n\n\nlimit_ids\n\n\ndef\n \nlimit_ids\n(\nself\n,\n \n*\nids\n)\n\n\n\n\n\n\nLimit the results to a specific set of pre-known document ids of any length\n\n\nno_content\n\n\ndef\n \nno_content\n(\nself\n)\n\n\n\n\n\n\nSet the query to only return ids and not the document content\n\n\nno_stopwords\n\n\ndef\n \nno_stopwords\n(\nself\n)\n\n\n\n\n\n\nPrevent the query from being filtered for stopwords.\nOnly useful in very big queries that you are certain contain no stopwords.\n\n\npaging\n\n\ndef\n \npaging\n(\nself\n,\n \noffset\n,\n \nnum\n)\n\n\n\n\n\n\nSet the paging for the query (defaults to 0..10).\n\n\n\n\noffset\n: Paging offset for the results. Defaults to 0\n\n\nnum\n: How many results do we want\n\n\n\n\nquery_string\n\n\ndef\n \nquery_string\n(\nself\n)\n\n\n\n\n\n\nReturn the query string of this query only\n\n\nreturn_fields\n\n\ndef\n \nreturn_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nOnly return values from these fields\n\n\nslop\n\n\ndef\n \nslop\n(\nself\n,\n \nslop\n)\n\n\n\n\n\n\nAllow a maximum of N intervening non matched terms between phrase terms (0 means exact phrase)\n\n\nsort_by\n\n\ndef\n \nsort_by\n(\nself\n,\n \nfield\n,\n \nasc\n=\nTrue\n)\n\n\n\n\n\n\nAdd a sortby field to the query\n\n\n\n\nfield\n - the name of the field to sort by\n\n\nasc\n - when \nTrue\n, sorting will be done in ascending order\n\n\n\n\nsummarize\n\n\ndef\n \nsummarize\n(\nself\n,\n \nfields\n=\nNone\n,\n \ncontext_len\n=\nNone\n,\n \nnum_frags\n=\nNone\n,\n \nsep\n=\nNone\n)\n\n\n\n\n\n\nReturn an abridged format of the field, containing only the segments of\nthe field which contain the matching term(s).\n\n\nIf \nfields\n is specified, then only the mentioned fields are\nsummarized; otherwise all results are summarized.\n\n\nServer side defaults are used for each option (except \nfields\n) if not specified\n\n\n\n\nfields\n List of fields to summarize. All fields are summarized if not specified\n\n\ncontext_len\n Amount of context to include with each fragment\n\n\nnum_frags\n Number of fragments per document\n\n\nsep\n Separator string to separate fragments\n\n\n\n\nverbatim\n\n\ndef\n \nverbatim\n(\nself\n)\n\n\n\n\n\n\nSet the query to be verbatim, i.e. use no query expansion or stemming\n\n\nwith_payloads\n\n\ndef\n \nwith_payloads\n(\nself\n)\n\n\n\n\n\n\nAsk the engine to return document payloads\n\n\nClass Result\n\n\nRepresents the result of a search query, and has an array of Document objects\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nres\n,\n \nhascontent\n,\n \nduration\n=\n0\n,\n \nhas_payload\n=\nFalse\n)\n\n\n\n\n\n\n\n\nsnippets\n: An optional dictionary of the form {field: snippet_size} for snippet formatting\n\n\n\n\nClass SortbyField\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nasc\n=\nTrue\n)\n\n\n\n\n\n\nClass Suggestion\n\n\nRepresents a single suggestion being sent or returned from the auto complete server\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nstring\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n)\n\n\n\n\n\n\nClass TagField\n\n\nTagField is a tag-indexing field with simpler compression and tokenization.\nSee https://oss.redislabs.com/redisearch/Tags/\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nseparator\n=\n,\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass TextField\n\n\nTextField is used to define a text field in a schema definition\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nweight\n=\n1.0\n,\n \nsortable\n=\nFalse\n,\n \nno_stem\n=\nFalse\n,\n \nno_index\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)", 
            "title": "Python API"
        }, 
        {
            "location": "/python_client/#package\"_\"redisearch\"_\"documentation", 
            "text": "", 
            "title": "Package redisearch Documentation"
        }, 
        {
            "location": "/python_client/#overview", 
            "text": "redisearch-py  is a python search engine library that utilizes the RediSearch Redis Module API.  It is the \"official\" client of RediSearch, and should be regarded as its canonical client implementation.  The source code can be found at  http://github.com/RedisLabs/redisearch-py", 
            "title": "Overview"
        }, 
        {
            "location": "/python_client/#example\"_\"using\"_\"the\"_\"python\"_\"client", 
            "text": "from   redisearch   import   Client ,   TextField ,   NumericField ,   Query  # Creating a client with a given index name  client   =   Client ( myIndex )  # Creating the index definition and schema  client . create_index ([ TextField ( title ,   weight = 5.0 ),   TextField ( body )])  # Indexing a document  client . add_document ( doc1 ,   title   =   RediSearch ,   body   =   Redisearch implements a search engine on top of redis )  # Simple search  res   =   client . search ( search engine )  # the result has the total number of results, and a list of documents  print   res . total   #  1  print   res . docs [ 0 ] . title  # Searching with snippets  res   =   client . search ( search engine ,   snippet_sizes   =   { body :   50 })  # Searching with complex parameters:  q   =   Query ( search engine ) . verbatim () . no_content () . paging ( 0 , 5 )  res   =   client . search ( q )", 
            "title": "Example: Using the Python Client"
        }, 
        {
            "location": "/python_client/#example\"_\"using\"_\"the\"_\"auto\"_\"completer\"_\"client", 
            "text": "# Using the auto-completer  ac   =   AutoCompleter ( ac )  # Adding some terms  ac . add_suggestions ( Suggestion ( foo ,   5.0 ),   Suggestion ( bar ,   1.0 ))  # Getting suggestions  suggs   =   ac . get_suggestions ( goo )   # returns nothing  suggs   =   ac . get_suggestions ( goo ,   fuzzy   =   True )   # returns [ foo ]", 
            "title": "Example: Using the Auto Completer Client:"
        }, 
        {
            "location": "/python_client/#installing", 
            "text": "Install Redis 4.0 or above    Install RediSearch    Install the python client    $ pip install redisearch", 
            "title": "Installing"
        }, 
        {
            "location": "/python_client/#class\"_\"autocompleter", 
            "text": "A client to RediSearch's AutoCompleter API  It provides prefix searches with optionally fuzzy matching of prefixes", 
            "title": "Class AutoCompleter"
        }, 
        {
            "location": "/python_client/#9595init9595", 
            "text": "def   __init__ ( self ,   key ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new AutoCompleter client for the given key, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95suggestions", 
            "text": "def   add_suggestions ( self ,   * suggestions ,   ** kwargs )   Add suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.  If kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores", 
            "title": "add_suggestions"
        }, 
        {
            "location": "/python_client/#delete", 
            "text": "def   delete ( self ,   string )   Delete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise", 
            "title": "delete"
        }, 
        {
            "location": "/python_client/#get95suggestions", 
            "text": "def   get_suggestions ( self ,   prefix ,   fuzzy = False ,   num = 10 ,   with_scores = False ,   with_payloads = False )   Get a list of suggestions from the AutoCompleter, for a given prefix", 
            "title": "get_suggestions"
        }, 
        {
            "location": "/python_client/#parameters", 
            "text": "prefix : the prefix we are searching.  Must be valid ascii or utf-8  fuzzy : If set to true, the prefix search is done in fuzzy mode.\n     NOTE : Running fuzzy searches on short ( 3 letters) prefixes can be very slow, and even scan the entire index.  with_scores : if set to true, we also return the (refactored) score of each suggestion.\n  This is normally not needed, and is NOT the original score inserted into the index  with_payloads : Return suggestion payloads  num : The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.   Returns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#len", 
            "text": "def   len ( self )   Return the number of entries in the AutoCompleter index", 
            "title": "len"
        }, 
        {
            "location": "/python_client/#class\"_\"client", 
            "text": "A client for the RediSearch module.\nIt abstracts the API of the module and lets you just use the engine", 
            "title": "Class Client"
        }, 
        {
            "location": "/python_client/#9595init9595_1", 
            "text": "def   __init__ ( self ,   index_name ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new Client for the given index_name, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   partial = False ,   ** fields )   Add a single document to the index.", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#parameters_1", 
            "text": "doc_id : the id of the saved document.  nosave : if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.  score : the document ranking, between 0.0 and 1.0  payload : optional inner-index payload we can save for fast access in scoring functions  replace : if True, and the document already is in the index, we perform an update and reindex the document  partial : if True, the fields specified will be added to the existing document.\n               This has the added benefit that any fields specified with  no_index \n               will not be reindexed again. Implies  replace  fields  kwargs dictionary of the document fields to be saved and/or indexed.\n             NOTE: Geo points should be encoded as strings of \"lon,lat\"", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#batch95indexer", 
            "text": "def   batch_indexer ( self ,   chunk_size = 100 )   Create a new batch indexer from the client with a given chunk size", 
            "title": "batch_indexer"
        }, 
        {
            "location": "/python_client/#create95index", 
            "text": "def   create_index ( self ,   fields ,   no_term_offsets = False ,   no_field_flags = False ,   stopwords = None )   Create the search index. Creating an existing index just updates its properties", 
            "title": "create_index"
        }, 
        {
            "location": "/python_client/#parameters_2", 
            "text": "fields : a list of TextField or NumericField objects  no_term_offsets : If true, we will not save term offsets in the index  no_field_flags : If true, we will not save field flags that allow searching in specific fields  stopwords : If not None, we create the index with this custom stopword list. The list can be empty", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#delete95document", 
            "text": "def   delete_document ( self ,   doc_id ,   conn = None )   Delete a document from index\nReturns 1 if the document was deleted, 0 if not", 
            "title": "delete_document"
        }, 
        {
            "location": "/python_client/#drop95index", 
            "text": "def   drop_index ( self )   Drop the index if it exists", 
            "title": "drop_index"
        }, 
        {
            "location": "/python_client/#explain", 
            "text": "def   explain ( self ,   query )", 
            "title": "explain"
        }, 
        {
            "location": "/python_client/#info", 
            "text": "def   info ( self )   Get info an stats about the the current index, including the number of documents, memory consumption, etc", 
            "title": "info"
        }, 
        {
            "location": "/python_client/#load95document", 
            "text": "def   load_document ( self ,   id )   Load a single document by id", 
            "title": "load_document"
        }, 
        {
            "location": "/python_client/#search", 
            "text": "def   search ( self ,   query )   Search the index for a given query, and return a result of documents", 
            "title": "search"
        }, 
        {
            "location": "/python_client/#parameters_3", 
            "text": "query : the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format  snippet_sizes : A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#class\"_\"batchindexer", 
            "text": "A batch indexer allows you to automatically batch\ndocument indexing in pipelines, flushing it every N documents.", 
            "title": "Class BatchIndexer"
        }, 
        {
            "location": "/python_client/#9595init9595_2", 
            "text": "def   __init__ ( self ,   client ,   chunk_size = 1000 )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document_1", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   partial = False ,   ** fields )   Add a document to the batch query", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#commit", 
            "text": "def   commit ( self )   Manually commit and flush the batch indexing query", 
            "title": "commit"
        }, 
        {
            "location": "/python_client/#class\"_\"document", 
            "text": "Represents a single document in a result set", 
            "title": "Class Document"
        }, 
        {
            "location": "/python_client/#9595init9595_3", 
            "text": "def   __init__ ( self ,   id ,   payload = None ,   ** fields )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"geofield", 
            "text": "GeoField is used to define a geo-indexing field in a schema definition", 
            "title": "Class GeoField"
        }, 
        {
            "location": "/python_client/#9595init9595_4", 
            "text": "def   __init__ ( self ,   name )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"geofilter", 
            "text": "None", 
            "title": "Class GeoFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_5", 
            "text": "def   __init__ ( self ,   field ,   lon ,   lat ,   radius ,   unit = km )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"numericfield", 
            "text": "NumericField is used to define a numeric field in a schema definition", 
            "title": "Class NumericField"
        }, 
        {
            "location": "/python_client/#9595init9595_6", 
            "text": "def   __init__ ( self ,   name ,   sortable = False ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_1", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"numericfilter", 
            "text": "None", 
            "title": "Class NumericFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_7", 
            "text": "def   __init__ ( self ,   field ,   minval ,   maxval ,   minExclusive = False ,   maxExclusive = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"query", 
            "text": "Query is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.  The setter functions return the query object, so they can be chained,\ni.e.  Query(\"foo\").verbatim().filter(...)  etc.", 
            "title": "Class Query"
        }, 
        {
            "location": "/python_client/#9595init9595_8", 
            "text": "def   __init__ ( self ,   query_string )   Create a new query object.\nThe query string is set in the constructor, and other options have setter functions.", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95filter", 
            "text": "def   add_filter ( self ,   flt )   Add a numeric or geo filter to the query. Currently only one of each filter is supported by the engine   flt : A NumericFilter or GeoFilter object, used on a corresponding field", 
            "title": "add_filter"
        }, 
        {
            "location": "/python_client/#get95args", 
            "text": "def   get_args ( self )   Format the redis arguments for this query and return them", 
            "title": "get_args"
        }, 
        {
            "location": "/python_client/#highlight", 
            "text": "def   highlight ( self ,   fields = None ,   tags = None )   Apply specified markup to matched term(s) within the returned field(s)   fields  If specified then only those mentioned fields are highlighted, otherwise all fields are highlighted  tags  A list of two strings to surround the match.", 
            "title": "highlight"
        }, 
        {
            "location": "/python_client/#in95order", 
            "text": "def   in_order ( self )   Match only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'", 
            "title": "in_order"
        }, 
        {
            "location": "/python_client/#limit95fields", 
            "text": "def   limit_fields ( self ,   * fields )   Limit the search to specific TEXT fields only   fields : A list of strings, case sensitive field names from the defined schema", 
            "title": "limit_fields"
        }, 
        {
            "location": "/python_client/#limit95ids", 
            "text": "def   limit_ids ( self ,   * ids )   Limit the results to a specific set of pre-known document ids of any length", 
            "title": "limit_ids"
        }, 
        {
            "location": "/python_client/#no95content", 
            "text": "def   no_content ( self )   Set the query to only return ids and not the document content", 
            "title": "no_content"
        }, 
        {
            "location": "/python_client/#no95stopwords", 
            "text": "def   no_stopwords ( self )   Prevent the query from being filtered for stopwords.\nOnly useful in very big queries that you are certain contain no stopwords.", 
            "title": "no_stopwords"
        }, 
        {
            "location": "/python_client/#paging", 
            "text": "def   paging ( self ,   offset ,   num )   Set the paging for the query (defaults to 0..10).   offset : Paging offset for the results. Defaults to 0  num : How many results do we want", 
            "title": "paging"
        }, 
        {
            "location": "/python_client/#query95string", 
            "text": "def   query_string ( self )   Return the query string of this query only", 
            "title": "query_string"
        }, 
        {
            "location": "/python_client/#return95fields", 
            "text": "def   return_fields ( self ,   * fields )   Only return values from these fields", 
            "title": "return_fields"
        }, 
        {
            "location": "/python_client/#slop", 
            "text": "def   slop ( self ,   slop )   Allow a maximum of N intervening non matched terms between phrase terms (0 means exact phrase)", 
            "title": "slop"
        }, 
        {
            "location": "/python_client/#sort95by", 
            "text": "def   sort_by ( self ,   field ,   asc = True )   Add a sortby field to the query   field  - the name of the field to sort by  asc  - when  True , sorting will be done in ascending order", 
            "title": "sort_by"
        }, 
        {
            "location": "/python_client/#summarize", 
            "text": "def   summarize ( self ,   fields = None ,   context_len = None ,   num_frags = None ,   sep = None )   Return an abridged format of the field, containing only the segments of\nthe field which contain the matching term(s).  If  fields  is specified, then only the mentioned fields are\nsummarized; otherwise all results are summarized.  Server side defaults are used for each option (except  fields ) if not specified   fields  List of fields to summarize. All fields are summarized if not specified  context_len  Amount of context to include with each fragment  num_frags  Number of fragments per document  sep  Separator string to separate fragments", 
            "title": "summarize"
        }, 
        {
            "location": "/python_client/#verbatim", 
            "text": "def   verbatim ( self )   Set the query to be verbatim, i.e. use no query expansion or stemming", 
            "title": "verbatim"
        }, 
        {
            "location": "/python_client/#with95payloads", 
            "text": "def   with_payloads ( self )   Ask the engine to return document payloads", 
            "title": "with_payloads"
        }, 
        {
            "location": "/python_client/#class\"_\"result", 
            "text": "Represents the result of a search query, and has an array of Document objects", 
            "title": "Class Result"
        }, 
        {
            "location": "/python_client/#9595init9595_9", 
            "text": "def   __init__ ( self ,   res ,   hascontent ,   duration = 0 ,   has_payload = False )    snippets : An optional dictionary of the form {field: snippet_size} for snippet formatting", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"sortbyfield", 
            "text": "None", 
            "title": "Class SortbyField"
        }, 
        {
            "location": "/python_client/#9595init9595_10", 
            "text": "def   __init__ ( self ,   field ,   asc = True )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"suggestion", 
            "text": "Represents a single suggestion being sent or returned from the auto complete server", 
            "title": "Class Suggestion"
        }, 
        {
            "location": "/python_client/#9595init9595_11", 
            "text": "def   __init__ ( self ,   string ,   score = 1.0 ,   payload = None )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class\"_\"tagfield", 
            "text": "TagField is a tag-indexing field with simpler compression and tokenization.\nSee https://oss.redislabs.com/redisearch/Tags/", 
            "title": "Class TagField"
        }, 
        {
            "location": "/python_client/#9595init9595_12", 
            "text": "def   __init__ ( self ,   name ,   separator = , ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_2", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class\"_\"textfield", 
            "text": "TextField is used to define a text field in a schema definition", 
            "title": "Class TextField"
        }, 
        {
            "location": "/python_client/#9595init9595_13", 
            "text": "def   __init__ ( self ,   name ,   weight = 1.0 ,   sortable = False ,   no_stem = False ,   no_index = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_3", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/java_client/", 
            "text": "JRediSearch - RediSearch Java Client\n\n\nhttps://github.com/RedisLabs/JRediSearch\n\n\nOverview\n\n\nJRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis. \n\n\nSee full documentation at \nhttps://github.com/RedisLabs/JRediSearch\n.\n\n\nUsage example\n\n\nInitializing the client:\n\n\nimport\n \nio.redisearch.client.Client\n;\n\n\nimport\n \nio.redisearch.Document\n;\n\n\nimport\n \nio.redisearch.SearchResult\n;\n\n\nimport\n \nio.redisearch.Query\n;\n\n\nimport\n \nio.redisearch.Schema\n;\n\n\n\n...\n\n\n\nClient\n \nclient\n \n=\n \nnew\n \nClient\n(\ntestung\n,\n \nlocalhost\n,\n \n6379\n);\n\n\n\n\n\n\nDefining a schema for an index and creating it:\n\n\nSchema\n \nsc\n \n=\n \nnew\n \nSchema\n()\n\n                \n.\naddTextField\n(\ntitle\n,\n \n5.0\n)\n\n                \n.\naddTextField\n(\nbody\n,\n \n1.0\n)\n\n                \n.\naddNumericField\n(\nprice\n);\n\n\n\nclient\n.\ncreateIndex\n(\nsc\n,\n \nClient\n.\nIndexOptions\n.\nDefault\n());\n\n\n\n\n\n\nAdding documents to the index:\n\n\nMap\nString\n,\n \nObject\n \nfields\n \n=\n \nnew\n \nHashMap\n();\n\n\nfields\n.\nput\n(\ntitle\n,\n \nhello world\n);\n\n\nfields\n.\nput\n(\nbody\n,\n \nlorem ipsum\n);\n\n\nfields\n.\nput\n(\nprice\n,\n \n1337\n);\n\n\n\nclient\n.\naddDocument\n(\ndoc1\n,\n \nfields\n);\n\n\n\n\n\n\nSearching the index:\n\n\n// Creating a complex query\n\n\nQuery\n \nq\n \n=\n \nnew\n \nQuery\n(\nhello world\n)\n\n                    \n.\naddFilter\n(\nnew\n \nQuery\n.\nNumericFilter\n(\nprice\n,\n \n0\n,\n \n1000\n))\n\n                    \n.\nlimit\n(\n0\n,\n5\n);\n\n\n\n// actual search\n\n\nSearchResult\n \nres\n \n=\n \nclient\n.\nsearch\n(\nq\n);", 
            "title": "Java API"
        }, 
        {
            "location": "/java_client/#jredisearch\"_\"-\"_\"redisearch\"_\"java\"_\"client", 
            "text": "https://github.com/RedisLabs/JRediSearch", 
            "title": "JRediSearch - RediSearch Java Client"
        }, 
        {
            "location": "/java_client/#overview", 
            "text": "JRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis.   See full documentation at  https://github.com/RedisLabs/JRediSearch .", 
            "title": "Overview"
        }, 
        {
            "location": "/java_client/#usage\"_\"example", 
            "text": "Initializing the client:  import   io.redisearch.client.Client ;  import   io.redisearch.Document ;  import   io.redisearch.SearchResult ;  import   io.redisearch.Query ;  import   io.redisearch.Schema ;  ...  Client   client   =   new   Client ( testung ,   localhost ,   6379 );   Defining a schema for an index and creating it:  Schema   sc   =   new   Schema () \n                 . addTextField ( title ,   5.0 ) \n                 . addTextField ( body ,   1.0 ) \n                 . addNumericField ( price );  client . createIndex ( sc ,   Client . IndexOptions . Default ());   Adding documents to the index:  Map String ,   Object   fields   =   new   HashMap ();  fields . put ( title ,   hello world );  fields . put ( body ,   lorem ipsum );  fields . put ( price ,   1337 );  client . addDocument ( doc1 ,   fields );   Searching the index:  // Creating a complex query  Query   q   =   new   Query ( hello world ) \n                     . addFilter ( new   Query . NumericFilter ( price ,   0 ,   1000 )) \n                     . limit ( 0 , 5 );  // actual search  SearchResult   res   =   client . search ( q );", 
            "title": "Usage example"
        }, 
        {
            "location": "/go_client/", 
            "text": "redisearch\n\n\n--\n    import \"github.com/RedisLabs/redisearch-go/redisearch\"\n\n\nPackage redisearch provides a Go client for the RediSearch search engine.\n\n\nFor the full documentation of RediSearch, see\n\nhttps://oss.redislabs.com/redisearch\n\n\nExample Usage\n\n\n    \nimport\n \n(\n\n      \ngithub.com/RedisLabs/redisearch-go/redisearch\n\n      \nlog\n\n      \nfmt\n\n    \n)\n\n\n    \nfunc\n \nExampleClient\n()\n \n{\n\n      \n// Create a client. By default a client is schemaless\n\n      \n// unless a schema is provided when creating the index\n\n      \nc\n \n:=\n \ncreateClient\n(\nmyIndex\n)\n\n\n      \n// Create a schema\n\n      \nsc\n \n:=\n \nredisearch\n.\nNewSchema\n(\nredisearch\n.\nDefaultOptions\n).\n\n        \nAddField\n(\nredisearch\n.\nNewTextField\n(\nbody\n)).\n\n        \nAddField\n(\nredisearch\n.\nNewTextFieldOptions\n(\ntitle\n,\n \nredisearch\n.\nTextFieldOptions\n{\nWeight\n:\n \n5.0\n,\n \nSortable\n:\n \ntrue\n})).\n\n        \nAddField\n(\nredisearch\n.\nNewNumericField\n(\ndate\n))\n\n\n      \n// Drop an existing index. If the index does not exist an error is returned\n\n      \nc\n.\nDrop\n()\n\n\n      \n// Create the index with the given schema\n\n      \nif\n \nerr\n \n:=\n \nc\n.\nCreateIndex\n(\nsc\n);\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n      \n}\n\n\n      \n// Create a document with an id and given score\n\n      \ndoc\n \n:=\n \nredisearch\n.\nNewDocument\n(\ndoc1\n,\n \n1.0\n)\n\n      \ndoc\n.\nSet\n(\ntitle\n,\n \nHello world\n).\n\n        \nSet\n(\nbody\n,\n \nfoo bar\n).\n\n        \nSet\n(\ndate\n,\n \ntime\n.\nNow\n().\nUnix\n())\n\n\n      \n// Index the document. The API accepts multiple documents at a time\n\n      \nif\n \nerr\n \n:=\n \nc\n.\nIndexOptions\n(\nredisearch\n.\nDefaultIndexingOptions\n,\n \ndoc\n);\n \nerr\n \n!=\n \nnil\n \n{\n\n        \nlog\n.\nFatal\n(\nerr\n)\n\n      \n}\n\n\n      \n// Searching with limit and sorting\n\n      \ndocs\n,\n \ntotal\n,\n \nerr\n \n:=\n \nc\n.\nSearch\n(\nredisearch\n.\nNewQuery\n(\nhello world\n).\n\n        \nLimit\n(\n0\n,\n \n2\n).\n\n        \nSetReturnFields\n(\ntitle\n))\n\n\n      \nfmt\n.\nPrintln\n(\ndocs\n[\n0\n].\nId\n,\n \ndocs\n[\n0\n].\nProperties\n[\ntitle\n],\n \ntotal\n,\n \nerr\n)\n\n      \n// Output: doc1 Hello world 1 \nnil\n\n    \n}\n\n\n\n\n\n\nUsage\n\n\nvar\n \nDefaultIndexingOptions\n \n=\n \nIndexingOptions\n{\n\n    \nLanguage\n:\n \n,\n\n    \nNoSave\n:\n   \nfalse\n,\n\n    \nReplace\n:\n  \nfalse\n,\n\n    \nPartial\n:\n  \nfalse\n,\n\n\n}\n\n\n\n\n\n\nDefaultIndexingOptions are the default options for document indexing\n\n\nvar\n \nDefaultOptions\n \n=\n \nOptions\n{\n\n    \nNoSave\n:\n          \nfalse\n,\n\n    \nNoFieldFlags\n:\n    \nfalse\n,\n\n    \nNoFrequencies\n:\n   \nfalse\n,\n\n    \nNoOffsetVectors\n:\n \nfalse\n,\n\n    \nStopwords\n:\n       \nnil\n,\n\n\n}\n\n\n\n\n\n\nDefaultOptions represents the default options\n\n\ntype Autocompleter\n\n\ntype\n \nAutocompleter\n \nstruct\n \n{\n\n\n}\n\n\n\n\n\n\nAutocompleter implements a redisearch auto-completer API\n\n\nfunc  NewAutocompleter\n\n\nfunc\n \nNewAutocompleter\n(\naddr\n,\n \nname\n \nstring\n)\n \n*\nAutocompleter\n\n\n\n\n\n\nNewAutocompleter creates a new Autocompleter with the given host and key name\n\n\nfunc (*Autocompleter) AddTerms\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nAddTerms\n(\nterms\n \n...\nSuggestion\n)\n \nerror\n\n\n\n\n\n\nAddTerms pushes new term suggestions to the index\n\n\nfunc (*Autocompleter) Delete\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nDelete\n()\n \nerror\n\n\n\n\n\n\nDelete deletes the Autocompleter key for this AC\n\n\nfunc (*Autocompleter) Suggest\n\n\nfunc\n \n(\na\n \n*\nAutocompleter\n)\n \nSuggest\n(\nprefix\n \nstring\n,\n \nnum\n \nint\n,\n \nfuzzy\n \nbool\n)\n \n([]\nSuggestion\n,\n \nerror\n)\n\n\n\n\n\n\nSuggest gets completion suggestions from the Autocompleter dictionary to the\ngiven prefix. If fuzzy is set, we also complete for prefixes that are in 1\nLevenshten distance from the given prefix\n\n\ntype Client\n\n\ntype\n \nClient\n \nstruct\n \n{\n\n\n}\n\n\n\n\n\n\nClient is an interface to redisearch's redis commands\n\n\nfunc  NewClient\n\n\nfunc\n \nNewClient\n(\naddr\n,\n \nname\n \nstring\n)\n \n*\nClient\n\n\n\n\n\n\nNewClient creates a new client connecting to the redis host, and using the given\nname as key prefix. Addr can be a single host:port pair, or a comma separated\nlist of host:port,host:port... In the case of multiple hosts we create a\nmulti-pool and select connections at random\n\n\nfunc (*Client) CreateIndex\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nCreateIndex\n(\ns\n \n*\nSchema\n)\n \nerror\n\n\n\n\n\n\nCreateIndex configues the index and creates it on redis\n\n\nfunc (*Client) Drop\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nDrop\n()\n \nerror\n\n\n\n\n\n\nDrop the Currentl just flushes the DB - note that this will delete EVERYTHING on\nthe redis instance\n\n\nfunc (*Client) Explain\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nExplain\n(\nq\n \n*\nQuery\n)\n \n(\nstring\n,\n \nerror\n)\n\n\n\n\n\n\nExplain Return a textual string explaining the query\n\n\nfunc (*Client) Index\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nIndex\n(\ndocs\n \n...\nDocument\n)\n \nerror\n\n\n\n\n\n\nIndex indexes a list of documents with the default options\n\n\nfunc (*Client) IndexOptions\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nIndexOptions\n(\nopts\n \nIndexingOptions\n,\n \ndocs\n \n...\nDocument\n)\n \nerror\n\n\n\n\n\n\nIndexOptions indexes multiple documents on the index, with optional Options\npassed to options\n\n\nfunc (*Client) Info\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nInfo\n()\n \n(\n*\nIndexInfo\n,\n \nerror\n)\n\n\n\n\n\n\nInfo - Get information about the index. This can also be used to check if the\nindex exists\n\n\nfunc (*Client) Search\n\n\nfunc\n \n(\ni\n \n*\nClient\n)\n \nSearch\n(\nq\n \n*\nQuery\n)\n \n(\ndocs\n \n[]\nDocument\n,\n \ntotal\n \nint\n,\n \nerr\n \nerror\n)\n\n\n\n\n\n\nSearch searches the index for the given query, and returns documents, the total\nnumber of results, or an error if something went wrong\n\n\ntype ConnPool\n\n\ntype\n \nConnPool\n \ninterface\n \n{\n\n    \nGet\n()\n \nredis\n.\nConn\n\n\n}\n\n\n\n\n\n\ntype Document\n\n\ntype\n \nDocument\n \nstruct\n \n{\n\n    \nId\n         \nstring\n\n    \nScore\n      \nfloat32\n\n    \nPayload\n    \n[]\nbyte\n\n    \nProperties\n \nmap\n[\nstring\n]\ninterface\n{}\n\n\n}\n\n\n\n\n\n\nDocument represents a single document to be indexed or returned from a query.\nBesides a score and id, the Properties are completely arbitrary\n\n\nfunc  NewDocument\n\n\nfunc\n \nNewDocument\n(\nid\n \nstring\n,\n \nscore\n \nfloat32\n)\n \nDocument\n\n\n\n\n\n\nNewDocument creates a document with the specific id and score\n\n\nfunc (*Document) EstimateSize\n\n\nfunc\n \n(\nd\n \n*\nDocument\n)\n \nEstimateSize\n()\n \n(\nsz\n \nint\n)\n\n\n\n\n\n\nfunc (Document) Set\n\n\nfunc\n \n(\nd\n \nDocument\n)\n \nSet\n(\nname\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nDocument\n\n\n\n\n\n\nSet sets a property and its value in the document\n\n\nfunc (*Document) SetPayload\n\n\nfunc\n \n(\nd\n \n*\nDocument\n)\n \nSetPayload\n(\npayload\n \n[]\nbyte\n)\n\n\n\n\n\n\nSetPayload Sets the document payload\n\n\ntype DocumentList\n\n\ntype\n \nDocumentList\n \n[]\nDocument\n\n\n\n\n\n\nDocumentList is used to sort documents by descending score\n\n\nfunc (DocumentList) Len\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nLen\n()\n \nint\n\n\n\n\n\n\nfunc (DocumentList) Less\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nLess\n(\ni\n,\n \nj\n \nint\n)\n \nbool\n\n\n\n\n\n\nfunc (DocumentList) Sort\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nSort\n()\n\n\n\n\n\n\nSort the DocumentList\n\n\nfunc (DocumentList) Swap\n\n\nfunc\n \n(\nl\n \nDocumentList\n)\n \nSwap\n(\ni\n,\n \nj\n \nint\n)\n\n\n\n\n\n\ntype Field\n\n\ntype\n \nField\n \nstruct\n \n{\n\n    \nName\n     \nstring\n\n    \nType\n     \nFieldType\n\n    \nSortable\n \nbool\n\n    \nOptions\n  \ninterface\n{}\n\n\n}\n\n\n\n\n\n\nField represents a single field's Schema\n\n\nfunc  NewNumericField\n\n\nfunc\n \nNewNumericField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewNumericField creates a new numeric field with the given name\n\n\nfunc  NewNumericFieldOptions\n\n\nfunc\n \nNewNumericFieldOptions\n(\nname\n \nstring\n,\n \noptions\n \nNumericFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewNumericFieldOptions defines a numeric field with additional options\n\n\nfunc  NewSortableNumericField\n\n\nfunc\n \nNewSortableNumericField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewSortableNumericField creates a new numeric field with the given name and a\nsortable flag\n\n\nfunc  NewSortableTextField\n\n\nfunc\n \nNewSortableTextField\n(\nname\n \nstring\n,\n \nweight\n \nfloat32\n)\n \nField\n\n\n\n\n\n\nNewSortableTextField creates a text field with the sortable flag set\n\n\nfunc  NewTagField\n\n\nfunc\n \nNewTagField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewTagField creates a new text field with default options (separator: ,)\n\n\nfunc  NewTagFieldOptions\n\n\nfunc\n \nNewTagFieldOptions\n(\nname\n \nstring\n,\n \nopts\n \nTagFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewTagFieldOptions creates a new tag field with the given options\n\n\nfunc  NewTextField\n\n\nfunc\n \nNewTextField\n(\nname\n \nstring\n)\n \nField\n\n\n\n\n\n\nNewTextField creates a new text field with the given weight\n\n\nfunc  NewTextFieldOptions\n\n\nfunc\n \nNewTextFieldOptions\n(\nname\n \nstring\n,\n \nopts\n \nTextFieldOptions\n)\n \nField\n\n\n\n\n\n\nNewTextFieldOptions creates a new text field with given options\n(weight/sortable)\n\n\ntype FieldType\n\n\ntype\n \nFieldType\n \nint\n\n\n\n\n\n\nFieldType is an enumeration of field/property types\n\n\nconst\n \n(\n\n    \n// TextField full-text field\n\n    \nTextField\n \nFieldType\n \n=\n \niota\n\n\n    \n// NumericField numeric range field\n\n    \nNumericField\n\n\n    \n// GeoField geo-indexed point field\n\n    \nGeoField\n\n\n    \n// TagField is a field used for compact indexing of comma separated values\n\n    \nTagField\n\n\n)\n\n\n\n\n\n\ntype Flag\n\n\ntype\n \nFlag\n \nuint64\n\n\n\n\n\n\nFlag is a type for query flags\n\n\nconst\n \n(\n\n    \n// Treat the terms verbatim and do not perform expansion\n\n    \nQueryVerbatim\n \nFlag\n \n=\n \n0x1\n\n\n    \n// Do not load any content from the documents, return just IDs\n\n    \nQueryNoContent\n \nFlag\n \n=\n \n0x2\n\n\n    \n// Fetch document scores as well as IDs and fields\n\n    \nQueryWithScores\n \nFlag\n \n=\n \n0x4\n\n\n    \n// The query terms must appear in order in the document\n\n    \nQueryInOrder\n \nFlag\n \n=\n \n0x08\n\n\n    \n// Fetch document payloads as well as fields. See documentation for payloads on redisearch.io\n\n    \nQueryWithPayloads\n \nFlag\n \n=\n \n0x10\n\n\n    \nDefaultOffset\n \n=\n \n0\n\n    \nDefaultNum\n    \n=\n \n10\n\n\n)\n\n\n\n\n\n\nQuery Flags\n\n\ntype HighlightOptions\n\n\ntype\n \nHighlightOptions\n \nstruct\n \n{\n\n    \nFields\n \n[]\nstring\n\n    \nTags\n   \n[\n2\n]\nstring\n\n\n}\n\n\n\n\n\n\nHighlightOptions represents the options to higlight specific document fields.\nSee http://redisearch.io/Highlight/\n\n\ntype IndexInfo\n\n\ntype\n \nIndexInfo\n \nstruct\n \n{\n\n    \nSchema\n               \nSchema\n\n    \nName\n                 \nstring\n  \n`redis:\nindex_name\n`\n\n    \nDocCount\n             \nuint64\n  \n`redis:\nnum_docs\n`\n\n    \nRecordCount\n          \nuint64\n  \n`redis:\nnum_records\n`\n\n    \nTermCount\n            \nuint64\n  \n`redis:\nnum_terms\n`\n\n    \nMaxDocID\n             \nuint64\n  \n`redis:\nmax_doc_id\n`\n\n    \nInvertedIndexSizeMB\n  \nfloat64\n \n`redis:\ninverted_sz_mb\n`\n\n    \nOffsetVectorSizeMB\n   \nfloat64\n \n`redis:\noffset_vector_sz_mb\n`\n\n    \nDocTableSizeMB\n       \nfloat64\n \n`redis:\ndoc_table_size_mb\n`\n\n    \nKeyTableSizeMB\n       \nfloat64\n \n`redis:\nkey_table_size_mb\n`\n\n    \nRecordsPerDocAvg\n     \nfloat64\n \n`redis:\nrecords_per_doc_avg\n`\n\n    \nBytesPerRecordAvg\n    \nfloat64\n \n`redis:\nbytes_per_record_avg\n`\n\n    \nOffsetsPerTermAvg\n    \nfloat64\n \n`redis:\noffsets_per_term_avg\n`\n\n    \nOffsetBitsPerTermAvg\n \nfloat64\n \n`redis:\noffset_bits_per_record_avg\n`\n\n\n}\n\n\n\n\n\n\nIndexInfo - Structure showing information about an existing index\n\n\ntype IndexingOptions\n\n\ntype\n \nIndexingOptions\n \nstruct\n \n{\n\n    \nLanguage\n \nstring\n\n    \nNoSave\n   \nbool\n\n    \nReplace\n  \nbool\n\n    \nPartial\n  \nbool\n\n\n}\n\n\n\n\n\n\nIndexingOptions represent the options for indexing a single document\n\n\ntype MultiError\n\n\ntype\n \nMultiError\n \n[]\nerror\n\n\n\n\n\n\nMultiError Represents one or more errors\n\n\nfunc  NewMultiError\n\n\nfunc\n \nNewMultiError\n(\nlen\n \nint\n)\n \nMultiError\n\n\n\n\n\n\nNewMultiError initializes a multierror with the given len, and all sub-errors\nset to nil\n\n\nfunc (MultiError) Error\n\n\nfunc\n \n(\ne\n \nMultiError\n)\n \nError\n()\n \nstring\n\n\n\n\n\n\nError returns a string representation of the error, in this case it just chains\nall the sub errors if they are not nil\n\n\ntype MultiHostPool\n\n\ntype\n \nMultiHostPool\n \nstruct\n \n{\n\n    \nsync\n.\nMutex\n\n\n}\n\n\n\n\n\n\nfunc  NewMultiHostPool\n\n\nfunc\n \nNewMultiHostPool\n(\nhosts\n \n[]\nstring\n)\n \n*\nMultiHostPool\n\n\n\n\n\n\nfunc (*MultiHostPool) Get\n\n\nfunc\n \n(\np\n \n*\nMultiHostPool\n)\n \nGet\n()\n \nredis\n.\nConn\n\n\n\n\n\n\ntype NumericFieldOptions\n\n\ntype\n \nNumericFieldOptions\n \nstruct\n \n{\n\n    \nSortable\n \nbool\n\n    \nNoIndex\n  \nbool\n\n\n}\n\n\n\n\n\n\nNumericFieldOptions Options for numeric fields\n\n\ntype Operator\n\n\ntype\n \nOperator\n \nstring\n\n\n\n\n\n\nconst\n \n(\n\n    \nEq\n \nOperator\n \n=\n \n=\n\n\n    \nGt\n  \nOperator\n \n=\n \n\n    \nGte\n \nOperator\n \n=\n \n=\n\n\n    \nLt\n  \nOperator\n \n=\n \n\n    \nLte\n \nOperator\n \n=\n \n=\n\n\n    \nBetween\n          \nOperator\n \n=\n \nBETWEEN\n\n    \nBetweenInclusive\n \nOperator\n \n=\n \nBETWEEEN_EXCLUSIVE\n\n\n)\n\n\n\n\n\n\ntype Options\n\n\ntype\n \nOptions\n \nstruct\n \n{\n\n\n    \n// If set, we will not save the documents contents, just index them, for fetching ids only\n\n    \nNoSave\n \nbool\n\n\n    \nNoFieldFlags\n \nbool\n\n\n    \nNoFrequencies\n \nbool\n\n\n    \nNoOffsetVectors\n \nbool\n\n\n    \nStopwords\n \n[]\nstring\n\n\n}\n\n\n\n\n\n\nOptions are flags passed to the the abstract Index call, which receives them as\ninterface{}, allowing for implementation specific options\n\n\ntype Paging\n\n\ntype\n \nPaging\n \nstruct\n \n{\n\n    \nOffset\n \nint\n\n    \nNum\n    \nint\n\n\n}\n\n\n\n\n\n\nPaging represents the offset paging of a search result\n\n\ntype Predicate\n\n\ntype\n \nPredicate\n \nstruct\n \n{\n\n    \nProperty\n \nstring\n\n    \nOperator\n \nOperator\n\n    \nValue\n    \n[]\ninterface\n{}\n\n\n}\n\n\n\n\n\n\nfunc  Equals\n\n\nfunc\n \nEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  GreaterThan\n\n\nfunc\n \nGreaterThan\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  GreaterThanEquals\n\n\nfunc\n \nGreaterThanEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  InRange\n\n\nfunc\n \nInRange\n(\nproperty\n \nstring\n,\n \nmin\n,\n \nmax\n \ninterface\n{},\n \ninclusive\n \nbool\n)\n \nPredicate\n\n\n\n\n\n\nfunc  LessThan\n\n\nfunc\n \nLessThan\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  LessThanEquals\n\n\nfunc\n \nLessThanEquals\n(\nproperty\n \nstring\n,\n \nvalue\n \ninterface\n{})\n \nPredicate\n\n\n\n\n\n\nfunc  NewPredicate\n\n\nfunc\n \nNewPredicate\n(\nproperty\n \nstring\n,\n \noperator\n \nOperator\n,\n \nvalues\n \n...\ninterface\n{})\n \nPredicate\n\n\n\n\n\n\ntype Query\n\n\ntype\n \nQuery\n \nstruct\n \n{\n\n    \nRaw\n \nstring\n\n\n    \nPaging\n \nPaging\n\n    \nFlags\n  \nFlag\n\n    \nSlop\n   \nint\n\n\n    \nFilters\n       \n[]\nPredicate\n\n    \nInKeys\n        \n[]\nstring\n\n    \nReturnFields\n  \n[]\nstring\n\n    \nLanguage\n      \nstring\n\n    \nExpander\n      \nstring\n\n    \nScorer\n        \nstring\n\n    \nPayload\n       \n[]\nbyte\n\n    \nSortBy\n        \n*\nSortingKey\n\n    \nHighlightOpts\n \n*\nHighlightOptions\n\n    \nSummarizeOpts\n \n*\nSummaryOptions\n\n\n}\n\n\n\n\n\n\nQuery is a single search query and all its parameters and predicates\n\n\nfunc  NewQuery\n\n\nfunc\n \nNewQuery\n(\nraw\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nNewQuery creates a new query for a given index with the given search term. For\ncurrently the index parameter is ignored\n\n\nfunc (*Query) Highlight\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nHighlight\n(\nfields\n \n[]\nstring\n,\n \nopenTag\n,\n \ncloseTag\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nHighlight sets highighting on given fields. Highlighting marks all the query\nterms with the given open and close tags (i.e. \n and \n for HTML)\n\n\nfunc (*Query) Limit\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nLimit\n(\noffset\n,\n \nnum\n \nint\n)\n \n*\nQuery\n\n\n\n\n\n\nLimit sets the paging offset and limit for the query\n\n\nfunc (*Query) SetExpander\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetExpander\n(\nexp\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetExpander sets a custom user query expander to be used\n\n\nfunc (*Query) SetFlags\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetFlags\n(\nflags\n \nFlag\n)\n \n*\nQuery\n\n\n\n\n\n\nSetFlags sets the query's optional flags\n\n\nfunc (*Query) SetInKeys\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetInKeys\n(\nkeys\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetInKeys sets the INKEYS argument of the query - limiting the search to a given\nset of IDs\n\n\nfunc (*Query) SetLanguage\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetLanguage\n(\nlang\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetLanguage sets the query language, used by the stemmer to expand the query\n\n\nfunc (*Query) SetPayload\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetPayload\n(\npayload\n \n[]\nbyte\n)\n \n*\nQuery\n\n\n\n\n\n\nSetPayload sets a binary payload to the query, that can be used by custom\nscoring functions\n\n\nfunc (*Query) SetReturnFields\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetReturnFields\n(\nfields\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetReturnFields sets the fields that should be returned from each result. By\ndefault we return everything\n\n\nfunc (*Query) SetScorer\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetScorer\n(\nscorer\n \nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSetScorer sets an alternative scoring function to be used. The only pre-compiled\nsupported one at the moment is DISMAX\n\n\nfunc (*Query) SetSortBy\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSetSortBy\n(\nfield\n \nstring\n,\n \nascending\n \nbool\n)\n \n*\nQuery\n\n\n\n\n\n\nSetSortBy sets the sorting key for the query\n\n\nfunc (*Query) Summarize\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSummarize\n(\nfields\n \n...\nstring\n)\n \n*\nQuery\n\n\n\n\n\n\nSummarize sets summarization on the given list of fields. It will instruct the\nengine to extract the most relevant snippets from the fields and return them as\nthe field content. This function works with the default values of the engine,\nand only sets the fields. There is a function that accepts all options -\nSummarizeOptions\n\n\nfunc (*Query) SummarizeOptions\n\n\nfunc\n \n(\nq\n \n*\nQuery\n)\n \nSummarizeOptions\n(\nopts\n \nSummaryOptions\n)\n \n*\nQuery\n\n\n\n\n\n\nSummarizeOptions sets summarization on the given list of fields. It will\ninstruct the engine to extract the most relevant snippets from the fields and\nreturn them as the field content.\n\n\nThis function accepts advanced settings for snippet length, separators and\nnumber of snippets\n\n\ntype Schema\n\n\ntype\n \nSchema\n \nstruct\n \n{\n\n    \nFields\n  \n[]\nField\n\n    \nOptions\n \nOptions\n\n\n}\n\n\n\n\n\n\nSchema represents an index schema Schema, or how the index would treat documents\nsent to it.\n\n\nfunc  NewSchema\n\n\nfunc\n \nNewSchema\n(\nopts\n \nOptions\n)\n \n*\nSchema\n\n\n\n\n\n\nNewSchema creates a new Schema object\n\n\nfunc (*Schema) AddField\n\n\nfunc\n \n(\nm\n \n*\nSchema\n)\n \nAddField\n(\nf\n \nField\n)\n \n*\nSchema\n\n\n\n\n\n\nAddField adds a field to the Schema object\n\n\ntype SingleHostPool\n\n\ntype\n \nSingleHostPool\n \nstruct\n \n{\n\n    \n*\nredis\n.\nPool\n\n\n}\n\n\n\n\n\n\nfunc  NewSingleHostPool\n\n\nfunc\n \nNewSingleHostPool\n(\nhost\n \nstring\n)\n \n*\nSingleHostPool\n\n\n\n\n\n\ntype SortingKey\n\n\ntype\n \nSortingKey\n \nstruct\n \n{\n\n    \nField\n     \nstring\n\n    \nAscending\n \nbool\n\n\n}\n\n\n\n\n\n\nSortingKey represents the sorting option if the query needs to be sorted based\non a sortable fields and not a ranking function. See\nhttp://redisearch.io/Sorting/\n\n\ntype Suggestion\n\n\ntype\n \nSuggestion\n \nstruct\n \n{\n\n    \nTerm\n    \nstring\n\n    \nScore\n   \nfloat64\n\n    \nPayload\n \nstring\n\n\n}\n\n\n\n\n\n\nSuggestion is a single suggestion being added or received from the Autocompleter\n\n\ntype SuggestionList\n\n\ntype\n \nSuggestionList\n \n[]\nSuggestion\n\n\n\n\n\n\nSuggestionList is a sortable list of suggestions returned from an engine\n\n\nfunc (SuggestionList) Len\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nLen\n()\n \nint\n\n\n\n\n\n\nfunc (SuggestionList) Less\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nLess\n(\ni\n,\n \nj\n \nint\n)\n \nbool\n\n\n\n\n\n\nfunc (SuggestionList) Sort\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nSort\n()\n\n\n\n\n\n\nSort the SuggestionList\n\n\nfunc (SuggestionList) Swap\n\n\nfunc\n \n(\nl\n \nSuggestionList\n)\n \nSwap\n(\ni\n,\n \nj\n \nint\n)\n\n\n\n\n\n\ntype SummaryOptions\n\n\ntype\n \nSummaryOptions\n \nstruct\n \n{\n\n    \nFields\n       \n[]\nstring\n\n    \nFragmentLen\n  \nint\n    \n// default 20\n\n    \nNumFragments\n \nint\n    \n// default 3\n\n    \nSeparator\n    \nstring\n \n// default \n...\n\n\n}\n\n\n\n\n\n\nSummaryOptions represents the configuration used to create field summaries. See\nhttp://redisearch.io/Highlight/\n\n\ntype TagFieldOptions\n\n\ntype\n \nTagFieldOptions\n \nstruct\n \n{\n\n    \n// Separator is the custom separator between tags. defaults to comma (,)\n\n    \nSeparator\n \nbyte\n\n    \nNoIndex\n   \nbool\n\n\n}\n\n\n\n\n\n\nTagFieldOptions options for indexing tag fields\n\n\ntype TextFieldOptions\n\n\ntype\n \nTextFieldOptions\n \nstruct\n \n{\n\n    \nWeight\n   \nfloat32\n\n    \nSortable\n \nbool\n\n    \nNoStem\n   \nbool\n\n    \nNoIndex\n  \nbool\n\n\n}\n\n\n\n\n\n\nTextFieldOptions Options for text fields - weight and stemming enabled/disabled.", 
            "title": "Go API"
        }, 
        {
            "location": "/go_client/#redisearch", 
            "text": "--\n    import \"github.com/RedisLabs/redisearch-go/redisearch\"  Package redisearch provides a Go client for the RediSearch search engine.  For the full documentation of RediSearch, see https://oss.redislabs.com/redisearch", 
            "title": "redisearch"
        }, 
        {
            "location": "/go_client/#example\"_\"usage", 
            "text": "import   ( \n       github.com/RedisLabs/redisearch-go/redisearch \n       log \n       fmt \n     ) \n\n     func   ExampleClient ()   { \n       // Create a client. By default a client is schemaless \n       // unless a schema is provided when creating the index \n       c   :=   createClient ( myIndex ) \n\n       // Create a schema \n       sc   :=   redisearch . NewSchema ( redisearch . DefaultOptions ). \n         AddField ( redisearch . NewTextField ( body )). \n         AddField ( redisearch . NewTextFieldOptions ( title ,   redisearch . TextFieldOptions { Weight :   5.0 ,   Sortable :   true })). \n         AddField ( redisearch . NewNumericField ( date )) \n\n       // Drop an existing index. If the index does not exist an error is returned \n       c . Drop () \n\n       // Create the index with the given schema \n       if   err   :=   c . CreateIndex ( sc );   err   !=   nil   { \n         log . Fatal ( err ) \n       } \n\n       // Create a document with an id and given score \n       doc   :=   redisearch . NewDocument ( doc1 ,   1.0 ) \n       doc . Set ( title ,   Hello world ). \n         Set ( body ,   foo bar ). \n         Set ( date ,   time . Now (). Unix ()) \n\n       // Index the document. The API accepts multiple documents at a time \n       if   err   :=   c . IndexOptions ( redisearch . DefaultIndexingOptions ,   doc );   err   !=   nil   { \n         log . Fatal ( err ) \n       } \n\n       // Searching with limit and sorting \n       docs ,   total ,   err   :=   c . Search ( redisearch . NewQuery ( hello world ). \n         Limit ( 0 ,   2 ). \n         SetReturnFields ( title )) \n\n       fmt . Println ( docs [ 0 ]. Id ,   docs [ 0 ]. Properties [ title ],   total ,   err ) \n       // Output: doc1 Hello world 1  nil \n     }", 
            "title": "Example Usage"
        }, 
        {
            "location": "/go_client/#usage", 
            "text": "var   DefaultIndexingOptions   =   IndexingOptions { \n     Language :   , \n     NoSave :     false , \n     Replace :    false , \n     Partial :    false ,  }   DefaultIndexingOptions are the default options for document indexing  var   DefaultOptions   =   Options { \n     NoSave :            false , \n     NoFieldFlags :      false , \n     NoFrequencies :     false , \n     NoOffsetVectors :   false , \n     Stopwords :         nil ,  }   DefaultOptions represents the default options", 
            "title": "Usage"
        }, 
        {
            "location": "/go_client/#type\"_\"autocompleter", 
            "text": "type   Autocompleter   struct   {  }   Autocompleter implements a redisearch auto-completer API", 
            "title": "type Autocompleter"
        }, 
        {
            "location": "/go_client/#func\"_\"newautocompleter", 
            "text": "func   NewAutocompleter ( addr ,   name   string )   * Autocompleter   NewAutocompleter creates a new Autocompleter with the given host and key name", 
            "title": "func  NewAutocompleter"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"addterms", 
            "text": "func   ( a   * Autocompleter )   AddTerms ( terms   ... Suggestion )   error   AddTerms pushes new term suggestions to the index", 
            "title": "func (*Autocompleter) AddTerms"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"delete", 
            "text": "func   ( a   * Autocompleter )   Delete ()   error   Delete deletes the Autocompleter key for this AC", 
            "title": "func (*Autocompleter) Delete"
        }, 
        {
            "location": "/go_client/#func\"_\"autocompleter\"_\"suggest", 
            "text": "func   ( a   * Autocompleter )   Suggest ( prefix   string ,   num   int ,   fuzzy   bool )   ([] Suggestion ,   error )   Suggest gets completion suggestions from the Autocompleter dictionary to the\ngiven prefix. If fuzzy is set, we also complete for prefixes that are in 1\nLevenshten distance from the given prefix", 
            "title": "func (*Autocompleter) Suggest"
        }, 
        {
            "location": "/go_client/#type\"_\"client", 
            "text": "type   Client   struct   {  }   Client is an interface to redisearch's redis commands", 
            "title": "type Client"
        }, 
        {
            "location": "/go_client/#func\"_\"newclient", 
            "text": "func   NewClient ( addr ,   name   string )   * Client   NewClient creates a new client connecting to the redis host, and using the given\nname as key prefix. Addr can be a single host:port pair, or a comma separated\nlist of host:port,host:port... In the case of multiple hosts we create a\nmulti-pool and select connections at random", 
            "title": "func  NewClient"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"createindex", 
            "text": "func   ( i   * Client )   CreateIndex ( s   * Schema )   error   CreateIndex configues the index and creates it on redis", 
            "title": "func (*Client) CreateIndex"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"drop", 
            "text": "func   ( i   * Client )   Drop ()   error   Drop the Currentl just flushes the DB - note that this will delete EVERYTHING on\nthe redis instance", 
            "title": "func (*Client) Drop"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"explain", 
            "text": "func   ( i   * Client )   Explain ( q   * Query )   ( string ,   error )   Explain Return a textual string explaining the query", 
            "title": "func (*Client) Explain"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"index", 
            "text": "func   ( i   * Client )   Index ( docs   ... Document )   error   Index indexes a list of documents with the default options", 
            "title": "func (*Client) Index"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"indexoptions", 
            "text": "func   ( i   * Client )   IndexOptions ( opts   IndexingOptions ,   docs   ... Document )   error   IndexOptions indexes multiple documents on the index, with optional Options\npassed to options", 
            "title": "func (*Client) IndexOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"info", 
            "text": "func   ( i   * Client )   Info ()   ( * IndexInfo ,   error )   Info - Get information about the index. This can also be used to check if the\nindex exists", 
            "title": "func (*Client) Info"
        }, 
        {
            "location": "/go_client/#func\"_\"client\"_\"search", 
            "text": "func   ( i   * Client )   Search ( q   * Query )   ( docs   [] Document ,   total   int ,   err   error )   Search searches the index for the given query, and returns documents, the total\nnumber of results, or an error if something went wrong", 
            "title": "func (*Client) Search"
        }, 
        {
            "location": "/go_client/#type\"_\"connpool", 
            "text": "type   ConnPool   interface   { \n     Get ()   redis . Conn  }", 
            "title": "type ConnPool"
        }, 
        {
            "location": "/go_client/#type\"_\"document", 
            "text": "type   Document   struct   { \n     Id           string \n     Score        float32 \n     Payload      [] byte \n     Properties   map [ string ] interface {}  }   Document represents a single document to be indexed or returned from a query.\nBesides a score and id, the Properties are completely arbitrary", 
            "title": "type Document"
        }, 
        {
            "location": "/go_client/#func\"_\"newdocument", 
            "text": "func   NewDocument ( id   string ,   score   float32 )   Document   NewDocument creates a document with the specific id and score", 
            "title": "func  NewDocument"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"estimatesize", 
            "text": "func   ( d   * Document )   EstimateSize ()   ( sz   int )", 
            "title": "func (*Document) EstimateSize"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"set", 
            "text": "func   ( d   Document )   Set ( name   string ,   value   interface {})   Document   Set sets a property and its value in the document", 
            "title": "func (Document) Set"
        }, 
        {
            "location": "/go_client/#func\"_\"document\"_\"setpayload", 
            "text": "func   ( d   * Document )   SetPayload ( payload   [] byte )   SetPayload Sets the document payload", 
            "title": "func (*Document) SetPayload"
        }, 
        {
            "location": "/go_client/#type\"_\"documentlist", 
            "text": "type   DocumentList   [] Document   DocumentList is used to sort documents by descending score", 
            "title": "type DocumentList"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"len", 
            "text": "func   ( l   DocumentList )   Len ()   int", 
            "title": "func (DocumentList) Len"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"less", 
            "text": "func   ( l   DocumentList )   Less ( i ,   j   int )   bool", 
            "title": "func (DocumentList) Less"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"sort", 
            "text": "func   ( l   DocumentList )   Sort ()   Sort the DocumentList", 
            "title": "func (DocumentList) Sort"
        }, 
        {
            "location": "/go_client/#func\"_\"documentlist\"_\"swap", 
            "text": "func   ( l   DocumentList )   Swap ( i ,   j   int )", 
            "title": "func (DocumentList) Swap"
        }, 
        {
            "location": "/go_client/#type\"_\"field", 
            "text": "type   Field   struct   { \n     Name       string \n     Type       FieldType \n     Sortable   bool \n     Options    interface {}  }   Field represents a single field's Schema", 
            "title": "type Field"
        }, 
        {
            "location": "/go_client/#func\"_\"newnumericfield", 
            "text": "func   NewNumericField ( name   string )   Field   NewNumericField creates a new numeric field with the given name", 
            "title": "func  NewNumericField"
        }, 
        {
            "location": "/go_client/#func\"_\"newnumericfieldoptions", 
            "text": "func   NewNumericFieldOptions ( name   string ,   options   NumericFieldOptions )   Field   NewNumericFieldOptions defines a numeric field with additional options", 
            "title": "func  NewNumericFieldOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"newsortablenumericfield", 
            "text": "func   NewSortableNumericField ( name   string )   Field   NewSortableNumericField creates a new numeric field with the given name and a\nsortable flag", 
            "title": "func  NewSortableNumericField"
        }, 
        {
            "location": "/go_client/#func\"_\"newsortabletextfield", 
            "text": "func   NewSortableTextField ( name   string ,   weight   float32 )   Field   NewSortableTextField creates a text field with the sortable flag set", 
            "title": "func  NewSortableTextField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtagfield", 
            "text": "func   NewTagField ( name   string )   Field   NewTagField creates a new text field with default options (separator: ,)", 
            "title": "func  NewTagField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtagfieldoptions", 
            "text": "func   NewTagFieldOptions ( name   string ,   opts   TagFieldOptions )   Field   NewTagFieldOptions creates a new tag field with the given options", 
            "title": "func  NewTagFieldOptions"
        }, 
        {
            "location": "/go_client/#func\"_\"newtextfield", 
            "text": "func   NewTextField ( name   string )   Field   NewTextField creates a new text field with the given weight", 
            "title": "func  NewTextField"
        }, 
        {
            "location": "/go_client/#func\"_\"newtextfieldoptions", 
            "text": "func   NewTextFieldOptions ( name   string ,   opts   TextFieldOptions )   Field   NewTextFieldOptions creates a new text field with given options\n(weight/sortable)", 
            "title": "func  NewTextFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"fieldtype", 
            "text": "type   FieldType   int   FieldType is an enumeration of field/property types  const   ( \n     // TextField full-text field \n     TextField   FieldType   =   iota \n\n     // NumericField numeric range field \n     NumericField \n\n     // GeoField geo-indexed point field \n     GeoField \n\n     // TagField is a field used for compact indexing of comma separated values \n     TagField  )", 
            "title": "type FieldType"
        }, 
        {
            "location": "/go_client/#type\"_\"flag", 
            "text": "type   Flag   uint64   Flag is a type for query flags  const   ( \n     // Treat the terms verbatim and do not perform expansion \n     QueryVerbatim   Flag   =   0x1 \n\n     // Do not load any content from the documents, return just IDs \n     QueryNoContent   Flag   =   0x2 \n\n     // Fetch document scores as well as IDs and fields \n     QueryWithScores   Flag   =   0x4 \n\n     // The query terms must appear in order in the document \n     QueryInOrder   Flag   =   0x08 \n\n     // Fetch document payloads as well as fields. See documentation for payloads on redisearch.io \n     QueryWithPayloads   Flag   =   0x10 \n\n     DefaultOffset   =   0 \n     DefaultNum      =   10  )   Query Flags", 
            "title": "type Flag"
        }, 
        {
            "location": "/go_client/#type\"_\"highlightoptions", 
            "text": "type   HighlightOptions   struct   { \n     Fields   [] string \n     Tags     [ 2 ] string  }   HighlightOptions represents the options to higlight specific document fields.\nSee http://redisearch.io/Highlight/", 
            "title": "type HighlightOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"indexinfo", 
            "text": "type   IndexInfo   struct   { \n     Schema                 Schema \n     Name                   string    `redis: index_name ` \n     DocCount               uint64    `redis: num_docs ` \n     RecordCount            uint64    `redis: num_records ` \n     TermCount              uint64    `redis: num_terms ` \n     MaxDocID               uint64    `redis: max_doc_id ` \n     InvertedIndexSizeMB    float64   `redis: inverted_sz_mb ` \n     OffsetVectorSizeMB     float64   `redis: offset_vector_sz_mb ` \n     DocTableSizeMB         float64   `redis: doc_table_size_mb ` \n     KeyTableSizeMB         float64   `redis: key_table_size_mb ` \n     RecordsPerDocAvg       float64   `redis: records_per_doc_avg ` \n     BytesPerRecordAvg      float64   `redis: bytes_per_record_avg ` \n     OffsetsPerTermAvg      float64   `redis: offsets_per_term_avg ` \n     OffsetBitsPerTermAvg   float64   `redis: offset_bits_per_record_avg `  }   IndexInfo - Structure showing information about an existing index", 
            "title": "type IndexInfo"
        }, 
        {
            "location": "/go_client/#type\"_\"indexingoptions", 
            "text": "type   IndexingOptions   struct   { \n     Language   string \n     NoSave     bool \n     Replace    bool \n     Partial    bool  }   IndexingOptions represent the options for indexing a single document", 
            "title": "type IndexingOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"multierror", 
            "text": "type   MultiError   [] error   MultiError Represents one or more errors", 
            "title": "type MultiError"
        }, 
        {
            "location": "/go_client/#func\"_\"newmultierror", 
            "text": "func   NewMultiError ( len   int )   MultiError   NewMultiError initializes a multierror with the given len, and all sub-errors\nset to nil", 
            "title": "func  NewMultiError"
        }, 
        {
            "location": "/go_client/#func\"_\"multierror\"_\"error", 
            "text": "func   ( e   MultiError )   Error ()   string   Error returns a string representation of the error, in this case it just chains\nall the sub errors if they are not nil", 
            "title": "func (MultiError) Error"
        }, 
        {
            "location": "/go_client/#type\"_\"multihostpool", 
            "text": "type   MultiHostPool   struct   { \n     sync . Mutex  }", 
            "title": "type MultiHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"newmultihostpool", 
            "text": "func   NewMultiHostPool ( hosts   [] string )   * MultiHostPool", 
            "title": "func  NewMultiHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"multihostpool\"_\"get", 
            "text": "func   ( p   * MultiHostPool )   Get ()   redis . Conn", 
            "title": "func (*MultiHostPool) Get"
        }, 
        {
            "location": "/go_client/#type\"_\"numericfieldoptions", 
            "text": "type   NumericFieldOptions   struct   { \n     Sortable   bool \n     NoIndex    bool  }   NumericFieldOptions Options for numeric fields", 
            "title": "type NumericFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"operator", 
            "text": "type   Operator   string   const   ( \n     Eq   Operator   =   = \n\n     Gt    Operator   =   \n     Gte   Operator   =   = \n\n     Lt    Operator   =   \n     Lte   Operator   =   = \n\n     Between            Operator   =   BETWEEN \n     BetweenInclusive   Operator   =   BETWEEEN_EXCLUSIVE  )", 
            "title": "type Operator"
        }, 
        {
            "location": "/go_client/#type\"_\"options", 
            "text": "type   Options   struct   { \n\n     // If set, we will not save the documents contents, just index them, for fetching ids only \n     NoSave   bool \n\n     NoFieldFlags   bool \n\n     NoFrequencies   bool \n\n     NoOffsetVectors   bool \n\n     Stopwords   [] string  }   Options are flags passed to the the abstract Index call, which receives them as\ninterface{}, allowing for implementation specific options", 
            "title": "type Options"
        }, 
        {
            "location": "/go_client/#type\"_\"paging", 
            "text": "type   Paging   struct   { \n     Offset   int \n     Num      int  }   Paging represents the offset paging of a search result", 
            "title": "type Paging"
        }, 
        {
            "location": "/go_client/#type\"_\"predicate", 
            "text": "type   Predicate   struct   { \n     Property   string \n     Operator   Operator \n     Value      [] interface {}  }", 
            "title": "type Predicate"
        }, 
        {
            "location": "/go_client/#func\"_\"equals", 
            "text": "func   Equals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  Equals"
        }, 
        {
            "location": "/go_client/#func\"_\"greaterthan", 
            "text": "func   GreaterThan ( property   string ,   value   interface {})   Predicate", 
            "title": "func  GreaterThan"
        }, 
        {
            "location": "/go_client/#func\"_\"greaterthanequals", 
            "text": "func   GreaterThanEquals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  GreaterThanEquals"
        }, 
        {
            "location": "/go_client/#func\"_\"inrange", 
            "text": "func   InRange ( property   string ,   min ,   max   interface {},   inclusive   bool )   Predicate", 
            "title": "func  InRange"
        }, 
        {
            "location": "/go_client/#func\"_\"lessthan", 
            "text": "func   LessThan ( property   string ,   value   interface {})   Predicate", 
            "title": "func  LessThan"
        }, 
        {
            "location": "/go_client/#func\"_\"lessthanequals", 
            "text": "func   LessThanEquals ( property   string ,   value   interface {})   Predicate", 
            "title": "func  LessThanEquals"
        }, 
        {
            "location": "/go_client/#func\"_\"newpredicate", 
            "text": "func   NewPredicate ( property   string ,   operator   Operator ,   values   ... interface {})   Predicate", 
            "title": "func  NewPredicate"
        }, 
        {
            "location": "/go_client/#type\"_\"query", 
            "text": "type   Query   struct   { \n     Raw   string \n\n     Paging   Paging \n     Flags    Flag \n     Slop     int \n\n     Filters         [] Predicate \n     InKeys          [] string \n     ReturnFields    [] string \n     Language        string \n     Expander        string \n     Scorer          string \n     Payload         [] byte \n     SortBy          * SortingKey \n     HighlightOpts   * HighlightOptions \n     SummarizeOpts   * SummaryOptions  }   Query is a single search query and all its parameters and predicates", 
            "title": "type Query"
        }, 
        {
            "location": "/go_client/#func\"_\"newquery", 
            "text": "func   NewQuery ( raw   string )   * Query   NewQuery creates a new query for a given index with the given search term. For\ncurrently the index parameter is ignored", 
            "title": "func  NewQuery"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"highlight", 
            "text": "func   ( q   * Query )   Highlight ( fields   [] string ,   openTag ,   closeTag   string )   * Query   Highlight sets highighting on given fields. Highlighting marks all the query\nterms with the given open and close tags (i.e.   and   for HTML)", 
            "title": "func (*Query) Highlight"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"limit", 
            "text": "func   ( q   * Query )   Limit ( offset ,   num   int )   * Query   Limit sets the paging offset and limit for the query", 
            "title": "func (*Query) Limit"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setexpander", 
            "text": "func   ( q   * Query )   SetExpander ( exp   string )   * Query   SetExpander sets a custom user query expander to be used", 
            "title": "func (*Query) SetExpander"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setflags", 
            "text": "func   ( q   * Query )   SetFlags ( flags   Flag )   * Query   SetFlags sets the query's optional flags", 
            "title": "func (*Query) SetFlags"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setinkeys", 
            "text": "func   ( q   * Query )   SetInKeys ( keys   ... string )   * Query   SetInKeys sets the INKEYS argument of the query - limiting the search to a given\nset of IDs", 
            "title": "func (*Query) SetInKeys"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setlanguage", 
            "text": "func   ( q   * Query )   SetLanguage ( lang   string )   * Query   SetLanguage sets the query language, used by the stemmer to expand the query", 
            "title": "func (*Query) SetLanguage"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setpayload", 
            "text": "func   ( q   * Query )   SetPayload ( payload   [] byte )   * Query   SetPayload sets a binary payload to the query, that can be used by custom\nscoring functions", 
            "title": "func (*Query) SetPayload"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setreturnfields", 
            "text": "func   ( q   * Query )   SetReturnFields ( fields   ... string )   * Query   SetReturnFields sets the fields that should be returned from each result. By\ndefault we return everything", 
            "title": "func (*Query) SetReturnFields"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setscorer", 
            "text": "func   ( q   * Query )   SetScorer ( scorer   string )   * Query   SetScorer sets an alternative scoring function to be used. The only pre-compiled\nsupported one at the moment is DISMAX", 
            "title": "func (*Query) SetScorer"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"setsortby", 
            "text": "func   ( q   * Query )   SetSortBy ( field   string ,   ascending   bool )   * Query   SetSortBy sets the sorting key for the query", 
            "title": "func (*Query) SetSortBy"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"summarize", 
            "text": "func   ( q   * Query )   Summarize ( fields   ... string )   * Query   Summarize sets summarization on the given list of fields. It will instruct the\nengine to extract the most relevant snippets from the fields and return them as\nthe field content. This function works with the default values of the engine,\nand only sets the fields. There is a function that accepts all options -\nSummarizeOptions", 
            "title": "func (*Query) Summarize"
        }, 
        {
            "location": "/go_client/#func\"_\"query\"_\"summarizeoptions", 
            "text": "func   ( q   * Query )   SummarizeOptions ( opts   SummaryOptions )   * Query   SummarizeOptions sets summarization on the given list of fields. It will\ninstruct the engine to extract the most relevant snippets from the fields and\nreturn them as the field content.  This function accepts advanced settings for snippet length, separators and\nnumber of snippets", 
            "title": "func (*Query) SummarizeOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"schema", 
            "text": "type   Schema   struct   { \n     Fields    [] Field \n     Options   Options  }   Schema represents an index schema Schema, or how the index would treat documents\nsent to it.", 
            "title": "type Schema"
        }, 
        {
            "location": "/go_client/#func\"_\"newschema", 
            "text": "func   NewSchema ( opts   Options )   * Schema   NewSchema creates a new Schema object", 
            "title": "func  NewSchema"
        }, 
        {
            "location": "/go_client/#func\"_\"schema\"_\"addfield", 
            "text": "func   ( m   * Schema )   AddField ( f   Field )   * Schema   AddField adds a field to the Schema object", 
            "title": "func (*Schema) AddField"
        }, 
        {
            "location": "/go_client/#type\"_\"singlehostpool", 
            "text": "type   SingleHostPool   struct   { \n     * redis . Pool  }", 
            "title": "type SingleHostPool"
        }, 
        {
            "location": "/go_client/#func\"_\"newsinglehostpool", 
            "text": "func   NewSingleHostPool ( host   string )   * SingleHostPool", 
            "title": "func  NewSingleHostPool"
        }, 
        {
            "location": "/go_client/#type\"_\"sortingkey", 
            "text": "type   SortingKey   struct   { \n     Field       string \n     Ascending   bool  }   SortingKey represents the sorting option if the query needs to be sorted based\non a sortable fields and not a ranking function. See\nhttp://redisearch.io/Sorting/", 
            "title": "type SortingKey"
        }, 
        {
            "location": "/go_client/#type\"_\"suggestion", 
            "text": "type   Suggestion   struct   { \n     Term      string \n     Score     float64 \n     Payload   string  }   Suggestion is a single suggestion being added or received from the Autocompleter", 
            "title": "type Suggestion"
        }, 
        {
            "location": "/go_client/#type\"_\"suggestionlist", 
            "text": "type   SuggestionList   [] Suggestion   SuggestionList is a sortable list of suggestions returned from an engine", 
            "title": "type SuggestionList"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"len", 
            "text": "func   ( l   SuggestionList )   Len ()   int", 
            "title": "func (SuggestionList) Len"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"less", 
            "text": "func   ( l   SuggestionList )   Less ( i ,   j   int )   bool", 
            "title": "func (SuggestionList) Less"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"sort", 
            "text": "func   ( l   SuggestionList )   Sort ()   Sort the SuggestionList", 
            "title": "func (SuggestionList) Sort"
        }, 
        {
            "location": "/go_client/#func\"_\"suggestionlist\"_\"swap", 
            "text": "func   ( l   SuggestionList )   Swap ( i ,   j   int )", 
            "title": "func (SuggestionList) Swap"
        }, 
        {
            "location": "/go_client/#type\"_\"summaryoptions", 
            "text": "type   SummaryOptions   struct   { \n     Fields         [] string \n     FragmentLen    int      // default 20 \n     NumFragments   int      // default 3 \n     Separator      string   // default  ...  }   SummaryOptions represents the configuration used to create field summaries. See\nhttp://redisearch.io/Highlight/", 
            "title": "type SummaryOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"tagfieldoptions", 
            "text": "type   TagFieldOptions   struct   { \n     // Separator is the custom separator between tags. defaults to comma (,) \n     Separator   byte \n     NoIndex     bool  }   TagFieldOptions options for indexing tag fields", 
            "title": "type TagFieldOptions"
        }, 
        {
            "location": "/go_client/#type\"_\"textfieldoptions", 
            "text": "type   TextFieldOptions   struct   { \n     Weight     float32 \n     Sortable   bool \n     NoStem     bool \n     NoIndex    bool  }   TextFieldOptions Options for text fields - weight and stemming enabled/disabled.", 
            "title": "type TextFieldOptions"
        }, 
        {
            "location": "/design/gc/", 
            "text": "Garbage Collection in RediSearch\n\n\n1. The Need For GC\n\n\n\n\nDeleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.\n\n\nThis means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.\n\n\nThus all inverted index entries belonging to this document id are just garbage. \n\n\nWe do not want to go and explicitly delete them when deleting a document because it will make this operation very long and depending on the length of the document.\n\n\nOn top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.\n\n\n\n\nAll of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory. \n\n\nThus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.\n\n\n2. Garbage Collecting a Single Term Index\n\n\nA single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage. \n\n\nThe algorithm is pretty simple: \n\n\n\n\nCreate a reader and writer for each block\n\n\nRead each block's records one by one\n\n\nIf no record is invalid, do nothing\n\n\nOnce we found a garbage record, we advance the reader but not the writer.\n\n\nOnce we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.\n\n\n\n\nPseudo code:\n\n\nforeach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer\ns tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length\n\n\n\n\n\nNOTE\n: Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.\n\n\n2.1 Garbage Collection on Numeric Indexes\n\n\nNumeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.\n\n\n3. GC And Concurrency\n\n\nSince RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us). \n\n\nIt GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.\n\n\nThis means, however, that we need to consider a few things:\n\n\n\n\n\n\nFrom the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.\n\n\n\n\n\n\nFrom the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.\n\n\n\n\n\n\nTo solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:\n\n Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed. \n\n Before starting an index iterator, we copy the index's gc marker to the iterator's context.\n\n After waking up from sleep in the iterator, we check the gc markers in both objects.\n\n If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower. \n\n\nTo solve 2 is simpler: \n\n The GC will of course operate only while the GIL is locked.\n\n The GC will never yield execution while in the middle of a block.\n\n The GC will check whether the key has been deleted while it slept.\n\n The GC will get a new pointer to the next block on each read, assuring the pointer is safe.\n\n\n4. Scheduling Garbage Collection\n\n\nWhile the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively. \n\n\nSo the GC will use sampling of random terms and collect them. \n\n\nThis leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage. \n\n\nSolving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.\n\n\nThus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.\n\n\nSolving 1 can be done in the following way:\n\n\nWe start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.\n\n\nThe frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "Garbage Collection"
        }, 
        {
            "location": "/design/gc/#garbage\"_\"collection\"_\"in\"_\"redisearch", 
            "text": "", 
            "title": "Garbage Collection in RediSearch"
        }, 
        {
            "location": "/design/gc/#1\"_\"the\"_\"need\"_\"for\"_\"gc", 
            "text": "Deleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.  This means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.  Thus all inverted index entries belonging to this document id are just garbage.   We do not want to go and explicitly delete them when deleting a document because it will make this operation very long and depending on the length of the document.  On top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.   All of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory.   Thus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.", 
            "title": "1. The Need For GC"
        }, 
        {
            "location": "/design/gc/#2\"_\"garbage\"_\"collecting\"_\"a\"_\"single\"_\"term\"_\"index", 
            "text": "A single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage.   The algorithm is pretty simple:    Create a reader and writer for each block  Read each block's records one by one  If no record is invalid, do nothing  Once we found a garbage record, we advance the reader but not the writer.  Once we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.   Pseudo code:  foreach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer s tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length  NOTE : Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.", 
            "title": "2. Garbage Collecting a Single Term Index"
        }, 
        {
            "location": "/design/gc/#21\"_\"garbage\"_\"collection\"_\"on\"_\"numeric\"_\"indexes", 
            "text": "Numeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.", 
            "title": "2.1 Garbage Collection on Numeric Indexes"
        }, 
        {
            "location": "/design/gc/#3\"_\"gc\"_\"and\"_\"concurrency", 
            "text": "Since RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us).   It GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.  This means, however, that we need to consider a few things:    From the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.    From the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.    To solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:  Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed.   Before starting an index iterator, we copy the index's gc marker to the iterator's context.  After waking up from sleep in the iterator, we check the gc markers in both objects.  If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower.   To solve 2 is simpler:   The GC will of course operate only while the GIL is locked.  The GC will never yield execution while in the middle of a block.  The GC will check whether the key has been deleted while it slept.  The GC will get a new pointer to the next block on each read, assuring the pointer is safe.", 
            "title": "3. GC And Concurrency"
        }, 
        {
            "location": "/design/gc/#4\"_\"scheduling\"_\"garbage\"_\"collection", 
            "text": "While the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively.   So the GC will use sampling of random terms and collect them.   This leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage.   Solving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.  Thus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.  Solving 1 can be done in the following way:  We start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.  The frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "4. Scheduling Garbage Collection"
        }, 
        {
            "location": "/Threading/", 
            "text": "Multi-Threading in RediSearch\n\n\nBy Dvir Volk, July 2017\n\n\n1. One Thread To Rule Them All\n\n\nRedis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with. \n\n\nWhile keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like \nZUNIONSTORE\n, \nLRANGE\n, \nSINTER\n and of course the infamous \nKEYS\n, can block Redis for seconds or minutes, depending on the size of data they are handling. \n\n\n2. RediSearch and the Single Thread Issue\n\n\nRediSearch\n is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine. \n\n\nWhile it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. \n\n\nThink, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, \nwhich is impossible with current hardware\n. The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.\n\n\nSo taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.\n\n\n3. Enter the Redis GIL\n\n\nLuckily, Redis BDFL \nSalvatore Sanfilippo\n has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API - \nThread Safe Contexts\n and the \nGlobal Lock\n.\n\n\nThe idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the \nGlobal Lock\n when it needs to access Redis data, operate on it, and release it. \n\n\nWe still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.\n\n\n4. Making Search Concurrent\n\n\nUp until now, the flow of a search query was simple - the query would arrive at a \nCommand Handler\n callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result. \n\n\nTo allow concurrency, we adapted the following design:\n\n\n\n\n\n\nRediSearch has a thread pool for running concurrent search queries. \n\n\n\n\n\n\nWhen a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.\n\n\n\n\n\n\nThe thread pool runs a query processing function in its own thread.\n\n\n\n\n\n\nThe function locks the Redis Global lock, and starts executing the query.\n\n\n\n\n\n\nSince the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).\n\n\n\n\n\n\nIf enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.\n\n\n\n\n\n\nWhen the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state. \n\n\n\n\n\n\nThus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently. \n\n\n\n\nFigure 1: Serial vs. Concurrent Search\n\n\n\n\nOn the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.\n\n\n\n\nThe same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.\n\n\nAs a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.\n\n\n5. The Effect of Concurrency\n\n\nWhile this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running \nKEYS *\n in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!\n\n\nThere is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\". \n\n\nThis is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation. \n\n\nTo enable safe mode and disable query concurrency, you can configure RediSearch at load time: \nredis-server --loadmodule redisearch.so SAFEMODE\n in command line, or by adding \nloadmodule redisearch.so SAFEMODE\n to your redis.conf - depending on how you load the module.\n\n\n6. Some Numbers!\n\n\nI've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.\n\n\n\n\nBenchmark Setup\n\n\n\n\nThe data-set consists of about 1,000,000 Reddit comments.\n\n\nTwo clients using Redis-benchmark were running  - first separately, then in parallel:\n\n\nOne client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.\n\n\nOne client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).\n\n\nBoth clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.\n\n\n\n\n\n\nThe Results:\n\n\n\n\n\n\n\n\nNote\n\n\nWhile we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries. \n\n\n\n\n7. Parting Words\n\n\nThis little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.\n\n\nFor RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#multi-threading\"_\"in\"_\"redisearch", 
            "text": "By Dvir Volk, July 2017", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#1\"_\"one\"_\"thread\"_\"to\"_\"rule\"_\"them\"_\"all", 
            "text": "Redis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with.   While keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like  ZUNIONSTORE ,  LRANGE ,  SINTER  and of course the infamous  KEYS , can block Redis for seconds or minutes, depending on the size of data they are handling.", 
            "title": "1. One Thread To Rule Them All"
        }, 
        {
            "location": "/Threading/#2\"_\"redisearch\"_\"and\"_\"the\"_\"single\"_\"thread\"_\"issue", 
            "text": "RediSearch  is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine.   While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked.   Think, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond,  which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.  So taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.", 
            "title": "2. RediSearch and the Single Thread Issue"
        }, 
        {
            "location": "/Threading/#3\"_\"enter\"_\"the\"_\"redis\"_\"gil", 
            "text": "Luckily, Redis BDFL  Salvatore Sanfilippo  has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API -  Thread Safe Contexts  and the  Global Lock .  The idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the  Global Lock  when it needs to access Redis data, operate on it, and release it.   We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.", 
            "title": "3. Enter the Redis GIL"
        }, 
        {
            "location": "/Threading/#4\"_\"making\"_\"search\"_\"concurrent", 
            "text": "Up until now, the flow of a search query was simple - the query would arrive at a  Command Handler  callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result.   To allow concurrency, we adapted the following design:    RediSearch has a thread pool for running concurrent search queries.     When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.    The thread pool runs a query processing function in its own thread.    The function locks the Redis Global lock, and starts executing the query.    Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).    If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.    When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state.     Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently.", 
            "title": "4. Making Search Concurrent"
        }, 
        {
            "location": "/Threading/#figure\"_\"1\"_\"serial\"_\"vs\"_\"concurrent\"_\"search", 
            "text": "On the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.   The same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.  As a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.", 
            "title": "Figure 1: Serial vs. Concurrent Search"
        }, 
        {
            "location": "/Threading/#5\"_\"the\"_\"effect\"_\"of\"_\"concurrency", 
            "text": "While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running  KEYS *  in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!  There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\".   This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation.   To enable safe mode and disable query concurrency, you can configure RediSearch at load time:  redis-server --loadmodule redisearch.so SAFEMODE  in command line, or by adding  loadmodule redisearch.so SAFEMODE  to your redis.conf - depending on how you load the module.", 
            "title": "5. The Effect of Concurrency"
        }, 
        {
            "location": "/Threading/#6\"_\"some\"_\"numbers", 
            "text": "I've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.   Benchmark Setup   The data-set consists of about 1,000,000 Reddit comments.  Two clients using Redis-benchmark were running  - first separately, then in parallel:  One client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.  One client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).  Both clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.", 
            "title": "6. Some Numbers!"
        }, 
        {
            "location": "/Threading/#the\"_\"results", 
            "text": "Note  While we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries.", 
            "title": "The Results:"
        }, 
        {
            "location": "/Threading/#7\"_\"parting\"_\"words", 
            "text": "This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.  For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "7. Parting Words"
        }, 
        {
            "location": "/Chinese/", 
            "text": "Chinese support in RediSearch\n\n\nSupport for adding documents in Chinese is available starting at version 0.99.0.\n\n\nChinese support allows Chinese documents to be added and tokenized using segmentation\nrather than simple tokenization using whitespace and/or punctuation.\n\n\nIndexing a Chinese document is different than indexing a document in most other\nlanguages because of how tokens are extracted. While most languages can have\ntheir tokens distinguished by separation characters and whitespace, this\nis not common in Chinese.\n\n\nChinese tokenization is done by scanning the input text and checking every\ncharacter or sequence of characters against a dictionary of predefined terms\nand determining the most likely (based on the surrounding terms and characters)\nmatch.\n\n\nRediSearch makes use of the \nFriso\n\nchinese tokenization library for this purpose. This is largely transparent to\nthe user and often no additional configuration is required.\n\n\nExample: Using Chinese in RediSearch\n\n\nIn pseudo-code:\n\n\nFT.CREATE idx SCHEMA txt TEXT\nFT.ADD idx docCn 1.0 LANGUAGE chinese FIELDS txt \nRedis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\uff0c\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8]\n\nFT.SEARCH idx \n\u6570\u636e\n LANGUAGE chinese HIGHLIGHT SUMMARIZE\n# Outputs:\n# \nb\n\u6570\u636e\n/b\n?... \nb\n\u6570\u636e\n/b\n\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... \nb\n\u6570\u636e\n/b\n\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8...\n\n\n\n\n\nUsing the Python Client:\n\n\n# -*- coding: utf-8 -*-\n\n\n\nfrom\n \nredisearch.client\n \nimport\n \nClient\n,\n \nQuery\n\n\nfrom\n \nredisearch\n \nimport\n \nTextField\n\n\n\nclient\n \n=\n \nClient\n(\nidx\n)\n\n\ntry\n:\n\n    \nclient\n.\ndrop_index\n()\n\n\nexcept\n:\n\n    \npass\n\n\n\nclient\n.\ncreate_index\n([\nTextField\n(\ntxt\n)])\n\n\n\n# Add a document\n\n\nclient\n.\nadd_document\n(\ndocCn1\n,\n\n                    \ntxt\n=\nRedis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8]\n,\n\n                    \nlanguage\n=\nchinese\n)\n\n\nprint\n \nclient\n.\nsearch\n(\nQuery\n(\n\u6570\u636e\n)\n.\nsummarize\n()\n.\nhighlight\n()\n.\nlanguage\n(\nchinese\n))\n.\ndocs\n[\n0\n]\n.\ntxt\n\n\n\n\n\n\nPrints:\n\n\nb\n\u6570\u636e\n/b\n?... \nb\n\u6570\u636e\n/b\n\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03... \nb\n\u6570\u636e\n/b\n\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8... \n\n\n\n\n\nUsing custom dictionaries\n\n\nIf you wish to use a custom dictionary, you can do so at the module level when\nloading the module. The \nFRISOINI\n setting can point to the location of a\n\nfriso.ini\n file which contains the relevant settings and paths to the dictionary\nfiles.\n\n\nNote that there is no \"default\" friso.ini file location. RediSearch comes with\nits own \nfriso.ini\n and dictionary files which are compiled into the module\nbinary at build-time.", 
            "title": "Chinese Support"
        }, 
        {
            "location": "/Chinese/#chinese\"_\"support\"_\"in\"_\"redisearch", 
            "text": "Support for adding documents in Chinese is available starting at version 0.99.0.  Chinese support allows Chinese documents to be added and tokenized using segmentation\nrather than simple tokenization using whitespace and/or punctuation.  Indexing a Chinese document is different than indexing a document in most other\nlanguages because of how tokens are extracted. While most languages can have\ntheir tokens distinguished by separation characters and whitespace, this\nis not common in Chinese.  Chinese tokenization is done by scanning the input text and checking every\ncharacter or sequence of characters against a dictionary of predefined terms\nand determining the most likely (based on the surrounding terms and characters)\nmatch.  RediSearch makes use of the  Friso \nchinese tokenization library for this purpose. This is largely transparent to\nthe user and often no additional configuration is required.", 
            "title": "Chinese support in RediSearch"
        }, 
        {
            "location": "/Chinese/#example\"_\"using\"_\"chinese\"_\"in\"_\"redisearch", 
            "text": "In pseudo-code:  FT.CREATE idx SCHEMA txt TEXT\nFT.ADD idx docCn 1.0 LANGUAGE chinese FIELDS txt  Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\uff0c\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] \nFT.SEARCH idx  \u6570\u636e  LANGUAGE chinese HIGHLIGHT SUMMARIZE\n# Outputs:\n#  b \u6570\u636e /b ?...  b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03...  b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8...  Using the Python Client:  # -*- coding: utf-8 -*-  from   redisearch.client   import   Client ,   Query  from   redisearch   import   TextField  client   =   Client ( idx )  try : \n     client . drop_index ()  except : \n     pass  client . create_index ([ TextField ( txt )])  # Add a document  client . add_document ( docCn1 , \n                     txt = Redis\u652f\u6301\u4e3b\u4ece\u540c\u6b65\u3002\u6570\u636e\u53ef\u4ee5\u4ece\u4e3b\u670d\u52a1\u5668\u5411\u4efb\u610f\u6570\u91cf\u7684\u4ece\u670d\u52a1\u5668\u4e0a\u540c\u6b65\u4ece\u670d\u52a1\u5668\u53ef\u4ee5\u662f\u5173\u8054\u5176\u4ed6\u4ece\u670d\u52a1\u5668\u7684\u4e3b\u670d\u52a1\u5668\u3002\u8fd9\u4f7f\u5f97Redis\u53ef\u6267\u884c\u5355\u5c42\u6811\u590d\u5236\u3002\u4ece\u76d8\u53ef\u4ee5\u6709\u610f\u65e0\u610f\u7684\u5bf9\u6570\u636e\u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03/\u8ba2\u9605\u673a\u5236\uff0c\u4f7f\u5f97\u4ece\u6570\u636e\u5e93\u5728\u4efb\u4f55\u5730\u65b9\u540c\u6b65\u6811\u65f6\uff0c\u53ef\u8ba2\u9605\u4e00\u4e2a\u9891\u9053\u5e76\u63a5\u6536\u4e3b\u670d\u52a1\u5668\u5b8c\u6574\u7684\u6d88\u606f\u53d1\u5e03\u8bb0\u5f55\u3002\u540c\u6b65\u5bf9\u8bfb\u53d6\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8] , \n                     language = chinese )  print   client . search ( Query ( \u6570\u636e ) . summarize () . highlight () . language ( chinese )) . docs [ 0 ] . txt   Prints:  b \u6570\u636e /b ?...  b \u6570\u636e /b \u8fdb\u884c\u5199\u64cd\u4f5c\u3002\u7531\u4e8e\u5b8c\u5168\u5b9e\u73b0\u4e86\u53d1\u5e03...  b \u6570\u636e /b \u5197\u4f59\u5f88\u6709\u5e2e\u52a9\u3002[8...", 
            "title": "Example: Using Chinese in RediSearch"
        }, 
        {
            "location": "/Chinese/#using\"_\"custom\"_\"dictionaries", 
            "text": "If you wish to use a custom dictionary, you can do so at the module level when\nloading the module. The  FRISOINI  setting can point to the location of a friso.ini  file which contains the relevant settings and paths to the dictionary\nfiles.  Note that there is no \"default\" friso.ini file location. RediSearch comes with\nits own  friso.ini  and dictionary files which are compiled into the module\nbinary at build-time.", 
            "title": "Using custom dictionaries"
        }, 
        {
            "location": "/contrib/", 
            "text": "Contributor agreement\n\n\nPlease refer to the following page for the agreement: \nRedis Labs Software Grant and Contributor License Agreement", 
            "title": "Contributor agreement"
        }, 
        {
            "location": "/contrib/#contributor\"_\"agreement", 
            "text": "Please refer to the following page for the agreement:  Redis Labs Software Grant and Contributor License Agreement", 
            "title": "Contributor agreement"
        }
    ]
}