{
    "docs": [
        {
            "location": "/", 
            "text": "RediSearch - Redis Powered Search Engine\n\n\nRediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by \nRedis Labs\n. \n\n\n\n\nQuick Links:\n\n\n\n\nSource Code at GitHub\n.\n\n\nLatest Release: 0.20.0\n\n\nDocker Image: redislabs/redisearch\n\n\nQuick Start Guide\n\n\n\n\n\n\n\n\nSupported Platforms\n\n\nRediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.\n\n\ni386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently. \n\n\n\n\nOverview\n\n\nRedisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.\n\n\nThis also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional redis search approache\n\n\nClient Libraries\n\n\nOfficial and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee \nClients Page\n\n\nPrimary Features:\n\n\n\n\nFull-Text indexing of multiple fields in documents.\n\n\nIncremental indexing without performance loss.\n\n\nDocument ranking (provided manually by the user at index time).\n\n\nComplex boolean queries with AND, OR, NOT operators between sub-queries.\n\n\nOptional query clauses.\n\n\nPrefix based searches.\n\n\nField weights.\n\n\nAuto-complete suggestions (with fuzzy prefix suggestions)\n\n\nExact Phrase Search, Slop based search.\n\n\nStemming based query expansion in \nmany languages\n (using \nSnowball\n).\n\n\nSupport for custom functions for query expansion and scoring (see \nExtensions\n).\n\n\nLimiting searches to specific document fields (\nup to 32 fields supported\n).\n\n\nNumeric filters and ranges.\n\n\nGeo filtering using Redis' own Geo-commands. \n\n\nUnicode support (UTF-8 input required)\n\n\nRetrieve full document content or just ids\n\n\nDocument deletion and updating with index garbage collection.", 
            "title": "Home"
        }, 
        {
            "location": "/#redisearch-redis-powered-search-engine", 
            "text": "RediSearch is a an open-source Full-Text and Secondary Index engine over Redis, developed by  Redis Labs .    Quick Links:   Source Code at GitHub .  Latest Release: 0.20.0  Docker Image: redislabs/redisearch  Quick Start Guide     Supported Platforms  RediSearch is developed and tested on Linux and Mac OS, on x86_64 CPUs.  i386 CPUs should work fine for small data-sets, but are not tested and not recommended. Atom CPUs are not supported currently.", 
            "title": "RediSearch - Redis Powered Search Engine"
        }, 
        {
            "location": "/#overview", 
            "text": "Redisearch implements a search engine on top of Redis, but unlike other Redis \nsearch libraries, it does not use internal data structures like sorted sets.  This also enables more advanced features, like exact phrase matching and numeric filtering for text queries, \nthat are not possible or efficient with traditional redis search approache", 
            "title": "Overview"
        }, 
        {
            "location": "/#client-libraries", 
            "text": "Official and community client libraries in Python, Java, JavaScript, Ruby, Go, C#, and PHP. \nSee  Clients Page", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/#primary-features", 
            "text": "Full-Text indexing of multiple fields in documents.  Incremental indexing without performance loss.  Document ranking (provided manually by the user at index time).  Complex boolean queries with AND, OR, NOT operators between sub-queries.  Optional query clauses.  Prefix based searches.  Field weights.  Auto-complete suggestions (with fuzzy prefix suggestions)  Exact Phrase Search, Slop based search.  Stemming based query expansion in  many languages  (using  Snowball ).  Support for custom functions for query expansion and scoring (see  Extensions ).  Limiting searches to specific document fields ( up to 32 fields supported ).  Numeric filters and ranges.  Geo filtering using Redis' own Geo-commands.   Unicode support (UTF-8 input required)  Retrieve full document content or just ids  Document deletion and updating with index garbage collection.", 
            "title": "Primary Features:"
        }, 
        {
            "location": "/Quick_Start/", 
            "text": "Quick Start Guide for RediSearch:\n\n\nRunning with Docker\n\n\ndocker run -p \n6379\n:6379 redislabs/redisearch:latest\n\n\n\n\n\nBuilding and running from source:\n\n\ngit clone https://github.com/RedisLabsModules/RediSearch.git\n\ncd\n RediSearch/src\nmake all\n\n\n# Assuming you have a redis build from the unstable branch:\n\n/path/to/redis-server --loadmodule ./redisearch.so\n\n\n\n\n\nCreating an index with fields and weights (default weight is 1.0):\n\n\n127.0.0.1:6379\n FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK \n\n\n\n\n\nAdding documents to the index:\n\n\n127.0.0.1:6379\n FT.ADD myIdx doc1 1.0 FIELDS title \nhello world\n body \nlorem ipsum\n url \nhttp://redis.io\n \nOK\n\n\n\n\n\nSearching the index:\n\n\n127.0.0.1:6379\n FT.SEARCH myIdx \nhello world\n LIMIT 0 10\n1) (integer) 1\n2) \ndoc1\n\n3) 1) \ntitle\n\n   2) \nhello world\n\n   3) \nbody\n\n   4) \nlorem ipsum\n\n   5) \nurl\n\n   6) \nhttp://redis.io\n\n\n\n\n\n\n\n\nNOTE\n: Input is expected to be valid utf-8 or ascii. The engine cannot handle wide character unicode at the moment. \n\n\n\n\nDropping the index:\n\n\n127.0.0.1:6379\n FT.DROP myIdx\nOK\n\n\n\n\n\nAdding and getting Auto-complete suggestions:\n\n\n127.0.0.1:6379\n FT.SUGADD autocomplete \nhello world\n 100\nOK\n\n127.0.0.1:6379\n FT.SUGGET autocomplete \nhe\n\n1) \nhello world", 
            "title": "Quick Start"
        }, 
        {
            "location": "/Quick_Start/#quick-start-guide-for-redisearch", 
            "text": "", 
            "title": "Quick Start Guide for RediSearch:"
        }, 
        {
            "location": "/Quick_Start/#running-with-docker", 
            "text": "docker run -p  6379 :6379 redislabs/redisearch:latest", 
            "title": "Running with Docker"
        }, 
        {
            "location": "/Quick_Start/#building-and-running-from-source", 
            "text": "git clone https://github.com/RedisLabsModules/RediSearch.git cd  RediSearch/src\nmake all # Assuming you have a redis build from the unstable branch: \n/path/to/redis-server --loadmodule ./redisearch.so", 
            "title": "Building and running from source:"
        }, 
        {
            "location": "/Quick_Start/#creating-an-index-with-fields-and-weights-default-weight-is-10", 
            "text": "127.0.0.1:6379  FT.CREATE myIdx SCHEMA title TEXT WEIGHT 5.0 body TEXT url TEXT\nOK", 
            "title": "Creating an index with fields and weights (default weight is 1.0):"
        }, 
        {
            "location": "/Quick_Start/#adding-documents-to-the-index", 
            "text": "127.0.0.1:6379  FT.ADD myIdx doc1 1.0 FIELDS title  hello world  body  lorem ipsum  url  http://redis.io  \nOK", 
            "title": "Adding documents to the index:"
        }, 
        {
            "location": "/Quick_Start/#searching-the-index", 
            "text": "127.0.0.1:6379  FT.SEARCH myIdx  hello world  LIMIT 0 10\n1) (integer) 1\n2)  doc1 \n3) 1)  title \n   2)  hello world \n   3)  body \n   4)  lorem ipsum \n   5)  url \n   6)  http://redis.io    NOTE : Input is expected to be valid utf-8 or ascii. The engine cannot handle wide character unicode at the moment.", 
            "title": "Searching the index:"
        }, 
        {
            "location": "/Quick_Start/#dropping-the-index", 
            "text": "127.0.0.1:6379  FT.DROP myIdx\nOK", 
            "title": "Dropping the index:"
        }, 
        {
            "location": "/Quick_Start/#adding-and-getting-auto-complete-suggestions", 
            "text": "127.0.0.1:6379  FT.SUGADD autocomplete  hello world  100\nOK\n\n127.0.0.1:6379  FT.SUGGET autocomplete  he \n1)  hello world", 
            "title": "Adding and getting Auto-complete suggestions:"
        }, 
        {
            "location": "/Clients/", 
            "text": "RediSearch Client Libraries\n\n\nRediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages. \n\n\nWhile it is possible and simple to use the raw redis commands API, in most cases it's easier to just use a client library abstracting it. \n\n\nCurrently available Libraries\n\n\n\n\n\n\n\n\nLanguage\n\n\nLibrary\n\n\nAuthor\n\n\nLicense\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nredisearch-py\n\n\nRedis Labs\n\n\nBSD\n\n\nUsually the most up-to-date client library\n\n\n\n\n\n\nJava\n\n\nJRediSearch\n\n\nRedis Labs\n\n\nBSD\n\n\n\n\n\n\n\n\nGo\n\n\nredisearch-go\n\n\nRedis Labs\n\n\nBSD\n\n\nIncomplete API\n\n\n\n\n\n\nJavaScript\n\n\nRedRediSearch\n\n\nKyle J. Davis\n\n\nMIT\n\n\nPartial API, compatible with \nReds\n\n\n\n\n\n\nC#\n\n\nNRediSearch\n\n\nMarc Gravell\n\n\nMIT\n\n\nPart of StackExchange.Redis\n\n\n\n\n\n\nPHP\n\n\nredisearch-php\n\n\nEthan Hann\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby on Rails\n\n\nredi_search_rails\n\n\nDmitry Polyakovsky\n\n\nMIT\n\n\n\n\n\n\n\n\nRuby\n\n\nredisearch-rb\n\n\nVictor Ruiz\n\n\nMIT", 
            "title": "Client Libraries"
        }, 
        {
            "location": "/Clients/#redisearch-client-libraries", 
            "text": "RediSearch has several client libraries, written by the module authors and community members - abstracting the API in different programming languages.   While it is possible and simple to use the raw redis commands API, in most cases it's easier to just use a client library abstracting it.", 
            "title": "RediSearch Client Libraries"
        }, 
        {
            "location": "/Clients/#currently-available-libraries", 
            "text": "Language  Library  Author  License  Comments      Python  redisearch-py  Redis Labs  BSD  Usually the most up-to-date client library    Java  JRediSearch  Redis Labs  BSD     Go  redisearch-go  Redis Labs  BSD  Incomplete API    JavaScript  RedRediSearch  Kyle J. Davis  MIT  Partial API, compatible with  Reds    C#  NRediSearch  Marc Gravell  MIT  Part of StackExchange.Redis    PHP  redisearch-php  Ethan Hann  MIT     Ruby on Rails  redi_search_rails  Dmitry Polyakovsky  MIT     Ruby  redisearch-rb  Victor Ruiz  MIT", 
            "title": "Currently available Libraries"
        }, 
        {
            "location": "/Commands/", 
            "text": "RediSeach Full Command Documentation\n\n\nFT.CREATE\n\n\nFormat:\n\n\n  FT.CREATE {index} \n    [NOOFFSETS] [NOFIELDS] [NOSCOREIDX]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] | NUMERIC | GEO] [SORTABLE] ...\n\n\n\n\n\nDescription:\n\n\nCreates an index with the given spec. The index name will be used in all the key names\nso keep it short!\n\n\nParameters:\n\n\n\n\n\n\nindex\n: the index name to create. If it exists the old spec will be overwritten\n\n\n\n\n\n\nNOOFFSETS\n: If set, we do not store term offsets for documents (saves memory, does not allow exact searches)\n\n\n\n\n\n\nNOFIELDS\n: If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields.\n\n\n\n\n\n\nNOSCOREIDX\n: If set, we avoid saving the top results for single words. Saves a lot of memory, slows down searches for common single word queries.\n\n\n\n\n\n\nNOFREQS\n: If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.\n\n\n\n\n\n\nSTOPWORDS\n: If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}. \n\n\nIf not set, we take the default list of stopwords. \n\n\nIf \n{num}\n is set to 0, the index will not have stopwords.\n\n\n\n\n\n\nSCHEMA {field} {options...}\n: After the SCHEMA keyword we define the index fields. \nThey can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0.\n\n\nNumeric or text field can have the optional SORTABLE argument that allows the user to later \nsort the results by the value of this field\n (this adds memory overhead so do not declare it on large text fields).\nText fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names\n\n\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns:\n\n\nOK or an error\n\n\n\n\nFT.ADD\n\n\nFormat:\n\n\nFT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  FIELDS {field} {value} [{field} {value}...]\n\n\n\n\n\nDescription\n\n\nAdd a documet to the index.\n\n\nParameters:\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id that will be returned from searches. \n  Note that the same docId cannot be added twice to the same index\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nNOSAVE\n: If set to true, we will not save the actual document in the index and only index it.\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.\n\n\n\n\n\n\nFIELDS\n: Following the FIELDS specifier, we are looking for pairs of  \n{field} {value}\n to be indexed.\n\n\n\n\n\n\nEach field will be scored based on the index spec given in FT.CREATE. \n  Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set \n\n\n\n\n\n\nPAYLOAD {payload}\n: Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.\n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\n\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\n\n\nFT.ADDHASH\n\n\nFormat\n\n\n \nFT\n.\nADDHASH\n \n{\nindex\n}\n \n{\ndocId\n}\n \n{\nscore\n}\n \n[\nLANGUAGE\n \nlanguage\n]\n \n[\nREPLACE\n]\n\n\n\n\n\n\nDescription\n\n\nAdd a documet to the index from an existing HASH key in Redis.\n\n\nParameters:\n\n\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\n\n\ndocId\n: The document's id. This has to be an existing HASH key in redis that will hold the fields \n    the index needs.\n\n\n\n\n\n\nscore\n: The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1\n\n\n\n\n\n\nREPLACE\n: If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.\n\n\n\n\n\n\nLANGUAGE language\n: If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:\n\n\n\n\n\n\n\n\n\"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"\n\n\n\n\nComplexity\n\n\nO(n), where n is the number of tokens in the document\n\n\nReturns\n\n\nOK on success, or an error if something went wrong.\n\n\n\n\nFT.INFO\n\n\nFormat\n\n\nFT.INFO {index} \n\n\n\n\n\nDescription\n\n\nReturn information and statistics on the index. Returned values include:\n\n\n\n\nNumber of documents.\n\n\nNumber of distinct terms.\n\n\nAverage bytes per record.\n\n\nSize and capacity of the index buffers.\n\n\n\n\nExample:\n\n\n127.0.0.1:6379\n ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5) \n1\n\n 5) num_docs\n 6) \n502694\n\n 7) num_terms\n 8) \n439158\n\n 9) num_records\n10) \n8098583\n\n11) inverted_sz_mb\n12) \n45.58\n13) inverted_cap_mb\n14) \n56.61\n15) inverted_cap_ovh\n16) \n0.19\n17) offset_vectors_sz_mb\n18) \n9.27\n19) skip_index_size_mb\n20) \n7.35\n21) score_index_size_mb\n22) \n30.8\n23) records_per_doc_avg\n24) \n16.1\n25) bytes_per_record_avg\n26) \n5.90\n27) offsets_per_term_avg\n28) \n1.20\n29) offset_bits_per_record_avg\n30) \n8.00\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nArray Response. A nested array of keys and values.\n\n\n\n\nFT.SEARCH\n\n\nFormat\n\n\nFT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]\n\n\n\n\n\nDescription\n\n\nSearch the index with a textual query, returning either documents or just ids.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nquery\n: the text query to search. If it's more than a single word, put it in quotes.\n  See below for documentation on query syntax. \n\n\nNOCONTENT\n: If it appears after the query, we only return the document ids and not \n  the content. This is useful if rediseach is only an index on an external document collection\n\n\nRETURN {num} {field} ...\n: Use this keyword to limit which fields from the document are returned.\n  \nnum\n is the number of fields following the keyword. If \nnum\n is 0, it acts like \nNOCONTENT\n.\n\n\nLIMIT first num\n: If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10\n\n\nINFIELDS {num} {field} ...\n: If set, filter the results to ones appearing only in specific\n  fields of the document, like title or url. num is the number of specified field arguments\n\n\nINKEYS {num} {field} ...\n: If set, we limit the result to a given set of keys specified in the list. \n  the first argument must be the length of the list, and greater than zero.\n  Non existent keys are ignored - unless all the keys are non existent.\n\n\nSLOP {slop}\n: If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0)\n\n\nINORDER\n: If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them. \n\n\nFILTER numeric_field min max\n: If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be \n-inf\n, \n+inf\n and use \n(\n for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.\n\n\nGEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft\n: If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See \nGEORADIUS\n for more details. \n\n\nNOSTOPWORDS\n: If set, we do not filter stopwords from the query. \n\n\nWITHSCORES\n: If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances\n\n\nWITHSORTKEYS\n: Only relevant in conjunction with \nSORTBY\n. Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes.\n\n\nVERBATIM\n: if set, we do not try to use stemming for query expansion but search the query terms verbatim.\n\n\nLANGUAGE {language}\n: If set, we use a stemmer for the supplied langauge during search for query expansion. \n  Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages.\n\n\nEXPANDER {expander}\n: If set, we will use a custom query expander instead of the stemmer. \nSee Extensions\n.\n\n\nSCORER {scorer}\n: If set, we will use a custom scoring function defined by the user. \nSee Extensions\n.\n\n\nPAYLOAD {payload}\n: Add an arbitrary, binary safe payload that will be exposed to custom scoring functions. \nSee Extensions\n.\n\n\nWITHPAYLOADS\n: If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if \nWITHSCORES\n was set, follow the scores.\n\n\nSORTBY {field} [ASC|DESC]\n: If specified, and field is a \nsortable field\n, the results are ordered by the value of this field. This applies to both text and numeric fields.\n\n\n\n\nComplexity\n\n\nO(n) for single word queries (though for popular words we save a cache of the top 50 results).\n\n\nComplexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.\n\n\nReturns\n\n\nArray reply,\n where the first element is the total number of results, and then pairs of document id, and a nested array of field/value. \n\n\nIf \nNOCONTENT\n was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.\n\n\n\n\nFT.EXPLAIN\n\n\nFormat\n\n\nFT.EXPLAIN {index} {query}\n\n\n\n\n\nDescription\n\n\nReturn the execution plan for a complex query\n\n\nExample:\n\n\n$ redis-cli --raw\n\n\n127\n.0.0.1:6379\n FT.EXPLAIN rd \n(foo bar)|(hello world) @date:[100 200]|@date:[500 +inf]\n\nINTERSECT \n{\n\n  UNION \n{\n\n    INTERSECT \n{\n\n      foo\n      bar\n    \n}\n\n    INTERSECT \n{\n\n      hello\n      world\n    \n}\n\n  \n}\n\n  UNION \n{\n\n    NUMERIC \n{\n100\n.000000 \n=\n x \n=\n \n200\n.000000\n}\n\n    NUMERIC \n{\n500\n.000000 \n=\n x \n=\n inf\n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\nquery\n: The query string, as if sent to FT.SEARCH\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nString Response. A string representing the execution plan (see above example). \n\n\nNote\n: You should use \nredis-cli --raw\n to properly read line-breaks in the returned response.\n\n\n\n\nFT.DEL\n\n\nFormat\n\n\nFT.DEL {index} {doc_id}\n\n\n\n\n\nDescription\n\n\nDelete a document from the index. Returns 1 if the document was in the index, or 0 if not. \n\n\nAfter deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.\n\n\nNOTE\n: This does not actually delete the document from the index, just marks it as deleted. \nThus, deleting and re-inserting the same document over and over will inflate the index size with each re-insertion.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\ndoc_id\n: the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed.\n\n\n\n\nComplexity\n\n\nO(1)\n\n\nReturns\n\n\nInteger Reply: 1 if the document was deleted, 0 if not.\n\n\n\n\nFT.DROP\n\n\nFormat\n\n\nFT.DROP {index}\n\n\n\n\n\nDescription\n\n\nDeletes all the keys associated with the index. \n\n\nIf no other data is on the redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nReturns\n\n\nStatus Reply: OK on success.\n\n\n\n\nFT.OPTIMIZE\n\n\nFormat\n\n\nFT.OPTIMIZE {index}\n\n\n\n\n\nDescription\n\n\nAfter the index is built (and doesn't need to be updated again withuot a complete rebuild)\nwe can optimize memory consumption by trimming all index buffers to their actual size.\n\n\nWarning 1\n: Do not run it if you intend to update your index afterward.\n\n\nWarning 2\n: This blocks redis for a long time. Do not run it on production instances\n\n\nParameters\n\n\n\n\nindex\n: The Fulltext index name. The index must be first created with FT.CREATE\n\n\n\n\nReturns:\n\n\nInteger Reply - the number of index entries optimized.\n\n\n\n\nFT.SUGGADD\n\n\nFormat\n\n\nFT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]\n\n\n\n\n\nDescription\n\n\nAdd a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestino dictionaries to the user.\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the suggestion string we index\n\n\nscore\n: a floating point number of the suggestion string's weight\n\n\nINCR\n: if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time\n\n\nPAYLOAD {payload}\n: If set, we save an extra payload with the suggestion, that can be fetched by adding the \nWITHPAYLOADS\n argument to \nFT.SUGGET\n.\n\n\n\n\nReturns:\n\n\nInteger Reply: the current size of the suggestion dictionary.\n\n\n\n\nFT.SUGGET\n\n\nFormat\n\n\nFT\n.\nSUGGET\n \n{\nkey\n}\n \n{\nprefix\n}\n \n[\nFUZZY\n]\n \n[\nWITHPAYLOADS\n]\n \n[\nMAX\n \nnum\n]\n\n\n\n\n\n\nDescription\n\n\nGet completion suggestions for a prefix\n\n\nParameters:\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nprefix\n: the prefix to complete on\n\n\nFUZZY\n: if set,we do a fuzzy prefix search, including prefixes at levenshtein distance of 1 from the prefix sent\n\n\nMAX num\n: If set, we limit the results to a maximum of \nnum\n. (\nNote\n: The default is 5, and the number cannot be greater than 10).\n\n\nWITHSCORES\n: If set, we also return the score of each suggestion. this can be used to merge results from multiple instances\n\n\nWITHPAYLOADS\n: If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply.\n\n\n\n\nReturns:\n\n\nArray Reply: a list of the top suggestions matching the prefix, optionally with score after each entry\n\n\n\n\nFT.SUGDEL\n\n\nFormat\n\n\nFT.SUGDEL {key} {string}\n\n\n\n\n\nDescription\n\n\nDelete a string from a suggestion index. \n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\nstring\n: the string to delete\n\n\n\n\nReturns:\n\n\nInteger Reply: 1 if the string was found and deleted, 0 otherwise.\n\n\n\n\nFT.SUGLEN\n\n\nFormat\n\n\nFT.SUGLEN {key}\n\n\n\n\n\nDescription\n\n\nGet the size of an autoc-complete suggestion dictionary\n\n\nParameters\n\n\n\n\nkey\n: the suggestion dictionary key.\n\n\n\n\nReturns:\n\n\nInteger Reply: the current size of the suggestion dictionary.", 
            "title": "Commands"
        }, 
        {
            "location": "/Commands/#rediseach-full-command-documentation", 
            "text": "", 
            "title": "RediSeach Full Command Documentation"
        }, 
        {
            "location": "/Commands/#ftcreate", 
            "text": "", 
            "title": "FT.CREATE"
        }, 
        {
            "location": "/Commands/#format", 
            "text": "FT.CREATE {index} \n    [NOOFFSETS] [NOFIELDS] [NOSCOREIDX]\n    [STOPWORDS {num} {stopword} ...]\n    SCHEMA {field} [TEXT [NOSTEM] [WEIGHT {weight}] | NUMERIC | GEO] [SORTABLE] ...", 
            "title": "Format:"
        }, 
        {
            "location": "/Commands/#description", 
            "text": "Creates an index with the given spec. The index name will be used in all the key names\nso keep it short!", 
            "title": "Description:"
        }, 
        {
            "location": "/Commands/#parameters", 
            "text": "index : the index name to create. If it exists the old spec will be overwritten    NOOFFSETS : If set, we do not store term offsets for documents (saves memory, does not allow exact searches)    NOFIELDS : If set, we do not store field bits for each term. Saves memory, does not allow filtering by specific fields.    NOSCOREIDX : If set, we avoid saving the top results for single words. Saves a lot of memory, slows down searches for common single word queries.    NOFREQS : If set, we avoid saving the term frequencies in the index. This saves\n  memory but does not allow sorting based on the frequencies of a given term within\n  the document.    STOPWORDS : If set, we set the index with a custom stopword list, to be ignored during indexing and search time. {num} is the number of stopwords, followed by a list of stopword arguments exactly the length of {num}.   If not set, we take the default list of stopwords.   If  {num}  is set to 0, the index will not have stopwords.    SCHEMA {field} {options...} : After the SCHEMA keyword we define the index fields. \nThey can be numeric, textual or geographical. For textual fields we optionally specify a weight. The default weight is 1.0.  Numeric or text field can have the optional SORTABLE argument that allows the user to later  sort the results by the value of this field  (this adds memory overhead so do not declare it on large text fields).\nText fields can have the NOSTEM argument which will disable stemming when indexing its values. \nThis may be ideal for things like proper names", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#complexity", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns", 
            "text": "OK or an error", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftadd", 
            "text": "", 
            "title": "FT.ADD"
        }, 
        {
            "location": "/Commands/#format_1", 
            "text": "FT.ADD {index} {docId} {score} \n  [NOSAVE]\n  [REPLACE]\n  [LANGUAGE {language}] \n  [PAYLOAD {payload}]\n  FIELDS {field} {value} [{field} {value}...]", 
            "title": "Format:"
        }, 
        {
            "location": "/Commands/#description_1", 
            "text": "Add a documet to the index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_1", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id that will be returned from searches. \n  Note that the same docId cannot be added twice to the same index    score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    NOSAVE : If set to true, we will not save the actual document in the index and only index it.    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.    FIELDS : Following the FIELDS specifier, we are looking for pairs of   {field} {value}  to be indexed.    Each field will be scored based on the index spec given in FT.CREATE. \n  Passing fields that are not in the index spec will make them be stored as part of the document, or ignored if NOSAVE is set     PAYLOAD {payload} : Optionally set a binary safe payload string to the document, \n  that can be evaluated at query time by a custom scoring function, or retrieved to the client.    LANGUAGE language : If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:     \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#complexity_1", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_1", 
            "text": "OK on success, or an error if something went wrong.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftaddhash", 
            "text": "", 
            "title": "FT.ADDHASH"
        }, 
        {
            "location": "/Commands/#format_2", 
            "text": "FT . ADDHASH   { index }   { docId }   { score }   [ LANGUAGE   language ]   [ REPLACE ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_2", 
            "text": "Add a documet to the index from an existing HASH key in Redis.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_2", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE    docId : The document's id. This has to be an existing HASH key in redis that will hold the fields \n    the index needs.    score : The document's rank based on the user's ranking. This must be between 0.0 and 1.0. \n  If you don't have a score just set it to 1    REPLACE : If set, we will do an UPSERT style insertion - and delete an older version of the document if it exists.    LANGUAGE language : If set, we use a stemmer for the supplied langauge during indexing. Defaults to English. \n  If an unsupported language is sent, the command returns an error. \n  The supported languages are:     \"arabic\",  \"danish\",    \"dutch\",   \"english\",   \"finnish\",    \"french\",\n\"german\",  \"hungarian\", \"italian\", \"norwegian\", \"portuguese\", \"romanian\",\n\"russian\", \"spanish\",   \"swedish\", \"tamil\",     \"turkish\"", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#complexity_2", 
            "text": "O(n), where n is the number of tokens in the document", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_2", 
            "text": "OK on success, or an error if something went wrong.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftinfo", 
            "text": "", 
            "title": "FT.INFO"
        }, 
        {
            "location": "/Commands/#format_3", 
            "text": "FT.INFO {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_3", 
            "text": "Return information and statistics on the index. Returned values include:   Number of documents.  Number of distinct terms.  Average bytes per record.  Size and capacity of the index buffers.   Example:  127.0.0.1:6379  ft.info wik{0}\n 1) index_name\n 2) wikipedia\n 3) fields\n 4) 1) 1) title\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n    2) 1) body\n       2) type\n       3) FULLTEXT\n       4) weight\n       5)  1 \n 5) num_docs\n 6)  502694 \n 7) num_terms\n 8)  439158 \n 9) num_records\n10)  8098583 \n11) inverted_sz_mb\n12)  45.58\n13) inverted_cap_mb\n14)  56.61\n15) inverted_cap_ovh\n16)  0.19\n17) offset_vectors_sz_mb\n18)  9.27\n19) skip_index_size_mb\n20)  7.35\n21) score_index_size_mb\n22)  30.8\n23) records_per_doc_avg\n24)  16.1\n25) bytes_per_record_avg\n26)  5.90\n27) offsets_per_term_avg\n28)  1.20\n29) offset_bits_per_record_avg\n30)  8.00", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_3", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_3", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_3", 
            "text": "Array Response. A nested array of keys and values.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftsearch", 
            "text": "", 
            "title": "FT.SEARCH"
        }, 
        {
            "location": "/Commands/#format_4", 
            "text": "FT.SEARCH {index} {query} [NOCONTENT] [VERBATIM] [NOSTOPWORDS] [WITHSCORES] [WITHPAYLOADS] [WITHSORTKEYS]\n  [FILTER {numeric_field} {min} {max}] ...\n  [GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft]\n  [INKEYS {num} {key} ... ]\n  [INFIELDS {num} {field} ... ]\n  [RETURN {num} {field} ... ]\n  [SLOP {slop}] [INORDER]\n  [LANGUAGE {language}]\n  [EXPANDER {expander}]\n  [SCORER {scorer}]\n  [PAYLOAD {payload}]\n  [SORTBY {field} [ASC|DESC]]\n  [LIMIT offset num]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_4", 
            "text": "Search the index with a textual query, returning either documents or just ids.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_4", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  query : the text query to search. If it's more than a single word, put it in quotes.\n  See below for documentation on query syntax.   NOCONTENT : If it appears after the query, we only return the document ids and not \n  the content. This is useful if rediseach is only an index on an external document collection  RETURN {num} {field} ... : Use this keyword to limit which fields from the document are returned.\n   num  is the number of fields following the keyword. If  num  is 0, it acts like  NOCONTENT .  LIMIT first num : If the parameters appear after the query, we limit the results to \n  the offset and number of results given. The default is 0 10  INFIELDS {num} {field} ... : If set, filter the results to ones appearing only in specific\n  fields of the document, like title or url. num is the number of specified field arguments  INKEYS {num} {field} ... : If set, we limit the result to a given set of keys specified in the list. \n  the first argument must be the length of the list, and greater than zero.\n  Non existent keys are ignored - unless all the keys are non existent.  SLOP {slop} : If set, we allow a maximum of N intervening number of unmatched offsets between phrase terms. (i.e the slop for exact phrases is 0)  INORDER : If set, and usually used in conjunction with SLOP, we make sure the query terms appear in the same order in the document as in the query, regardless of the offsets between them.   FILTER numeric_field min max : If set, and numeric_field is defined as a numeric field in \n  FT.CREATE, we will limit results to those having numeric values ranging between min and max.\n  min and max follow ZRANGE syntax, and can be  -inf ,  +inf  and use  (  for exclusive ranges. \n  Multiple numeric filters for different fields are supported in one query.  GEOFILTER {geo_field} {lon} {lat} {raius} m|km|mi|ft : If set, we filter the results to a given radius \n  from lon and lat. Radius is given as a number and units. See  GEORADIUS  for more details.   NOSTOPWORDS : If set, we do not filter stopwords from the query.   WITHSCORES : If set, we also return the relative internal score of each document. this can be\n  used to merge results from multiple instances  WITHSORTKEYS : Only relevant in conjunction with  SORTBY . Returns the value of the sorting key, right after the id and score and /or payload if requested. This is usually not needed by users, and exists for distributed search coordination purposes.  VERBATIM : if set, we do not try to use stemming for query expansion but search the query terms verbatim.  LANGUAGE {language} : If set, we use a stemmer for the supplied langauge during search for query expansion. \n  Defaults to English. If an unsupported language is sent, the command returns an error. See FT.ADD for the list of languages.  EXPANDER {expander} : If set, we will use a custom query expander instead of the stemmer.  See Extensions .  SCORER {scorer} : If set, we will use a custom scoring function defined by the user.  See Extensions .  PAYLOAD {payload} : Add an arbitrary, binary safe payload that will be exposed to custom scoring functions.  See Extensions .  WITHPAYLOADS : If set, we retrieve optional document payloads (see FT.ADD). \n  the payloads follow the document id, and if  WITHSCORES  was set, follow the scores.  SORTBY {field} [ASC|DESC] : If specified, and field is a  sortable field , the results are ordered by the value of this field. This applies to both text and numeric fields.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_4", 
            "text": "O(n) for single word queries (though for popular words we save a cache of the top 50 results).  Complexity for complex queries changes, but in general it's proportional to the number of words and the number of intersection points between them.", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_4", 
            "text": "Array reply,  where the first element is the total number of results, and then pairs of document id, and a nested array of field/value.   If  NOCONTENT  was given, we return an array where the first element is the total number of results, and the rest of the members are document ids.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftexplain", 
            "text": "", 
            "title": "FT.EXPLAIN"
        }, 
        {
            "location": "/Commands/#format_5", 
            "text": "FT.EXPLAIN {index} {query}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_5", 
            "text": "Return the execution plan for a complex query  Example:  $ redis-cli --raw 127 .0.0.1:6379  FT.EXPLAIN rd  (foo bar)|(hello world) @date:[100 200]|@date:[500 +inf] \nINTERSECT  { \n  UNION  { \n    INTERSECT  { \n      foo\n      bar\n     } \n    INTERSECT  { \n      hello\n      world\n     } \n   } \n  UNION  { \n    NUMERIC  { 100 .000000  =  x  =   200 .000000 } \n    NUMERIC  { 500 .000000  =  x  =  inf } \n   }  }", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_5", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  query : The query string, as if sent to FT.SEARCH", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_5", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_5", 
            "text": "String Response. A string representing the execution plan (see above example).   Note : You should use  redis-cli --raw  to properly read line-breaks in the returned response.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdel", 
            "text": "", 
            "title": "FT.DEL"
        }, 
        {
            "location": "/Commands/#format_6", 
            "text": "FT.DEL {index} {doc_id}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_6", 
            "text": "Delete a document from the index. Returns 1 if the document was in the index, or 0 if not.   After deletion, the document can be re-added to the index. It will get a different internal id and will be a new document from the index's POV.  NOTE : This does not actually delete the document from the index, just marks it as deleted. \nThus, deleting and re-inserting the same document over and over will inflate the index size with each re-insertion.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_6", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE  doc_id : the id of the document to be deleted. It does not actually delete the HASH key in which the document is stored. Use DEL to do that manually if needed.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#complexity_6", 
            "text": "O(1)", 
            "title": "Complexity"
        }, 
        {
            "location": "/Commands/#returns_6", 
            "text": "Integer Reply: 1 if the document was deleted, 0 if not.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftdrop", 
            "text": "", 
            "title": "FT.DROP"
        }, 
        {
            "location": "/Commands/#format_7", 
            "text": "FT.DROP {index}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_7", 
            "text": "Deletes all the keys associated with the index.   If no other data is on the redis instance, this is equivalent to FLUSHDB, apart from the fact\nthat the index specification is not deleted.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_7", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_7", 
            "text": "Status Reply: OK on success.", 
            "title": "Returns"
        }, 
        {
            "location": "/Commands/#ftoptimize", 
            "text": "Format  FT.OPTIMIZE {index}  Description  After the index is built (and doesn't need to be updated again withuot a complete rebuild)\nwe can optimize memory consumption by trimming all index buffers to their actual size.  Warning 1 : Do not run it if you intend to update your index afterward.  Warning 2 : This blocks redis for a long time. Do not run it on production instances", 
            "title": "FT.OPTIMIZE"
        }, 
        {
            "location": "/Commands/#parameters_8", 
            "text": "index : The Fulltext index name. The index must be first created with FT.CREATE", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_8", 
            "text": "Integer Reply - the number of index entries optimized.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsuggadd", 
            "text": "", 
            "title": "FT.SUGGADD"
        }, 
        {
            "location": "/Commands/#format_8", 
            "text": "FT.SUGADD {key} {string} {score} [INCR] [PAYLOAD {payload}]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_8", 
            "text": "Add a suggestion string to an auto-complete suggestion dictionary. This is disconnected from the\nindex definitions, and leaves creating and updating suggestino dictionaries to the user.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_9", 
            "text": "key : the suggestion dictionary key.  string : the suggestion string we index  score : a floating point number of the suggestion string's weight  INCR : if set, we increment the existing entry of the suggestion by the given score, instead of replacing the score. This is useful for updating the dictionary based on user queries in real time  PAYLOAD {payload} : If set, we save an extra payload with the suggestion, that can be fetched by adding the  WITHPAYLOADS  argument to  FT.SUGGET .", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_9", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsugget", 
            "text": "", 
            "title": "FT.SUGGET"
        }, 
        {
            "location": "/Commands/#format_9", 
            "text": "FT . SUGGET   { key }   { prefix }   [ FUZZY ]   [ WITHPAYLOADS ]   [ MAX   num ]", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_9", 
            "text": "Get completion suggestions for a prefix", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_10", 
            "text": "key : the suggestion dictionary key.  prefix : the prefix to complete on  FUZZY : if set,we do a fuzzy prefix search, including prefixes at levenshtein distance of 1 from the prefix sent  MAX num : If set, we limit the results to a maximum of  num . ( Note : The default is 5, and the number cannot be greater than 10).  WITHSCORES : If set, we also return the score of each suggestion. this can be used to merge results from multiple instances  WITHPAYLOADS : If set, we return optional payloads saved along with the suggestions. If no payload is present for an entry, we return a Null Reply.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/Commands/#returns_10", 
            "text": "Array Reply: a list of the top suggestions matching the prefix, optionally with score after each entry", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsugdel", 
            "text": "", 
            "title": "FT.SUGDEL"
        }, 
        {
            "location": "/Commands/#format_10", 
            "text": "FT.SUGDEL {key} {string}", 
            "title": "Format"
        }, 
        {
            "location": "/Commands/#description_10", 
            "text": "Delete a string from a suggestion index.", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_11", 
            "text": "key : the suggestion dictionary key.  string : the string to delete", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_11", 
            "text": "Integer Reply: 1 if the string was found and deleted, 0 otherwise.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Commands/#ftsuglen", 
            "text": "Format  FT.SUGLEN {key}", 
            "title": "FT.SUGLEN"
        }, 
        {
            "location": "/Commands/#description_11", 
            "text": "Get the size of an autoc-complete suggestion dictionary", 
            "title": "Description"
        }, 
        {
            "location": "/Commands/#parameters_12", 
            "text": "key : the suggestion dictionary key.", 
            "title": "Parameters"
        }, 
        {
            "location": "/Commands/#returns_12", 
            "text": "Integer Reply: the current size of the suggestion dictionary.", 
            "title": "Returns:"
        }, 
        {
            "location": "/Configuring/", 
            "text": "Run-Time Configuration\n\n\nRediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added. \n\n\n\n\nPassing Configuration Options\n\n\nIn general, passing configuration options is done by appending arguments after the \n--loadmodule\n argument in command line, \nloadmodule\n configurtion directive in a redis config file, or \nMODULE LOAD\n when loading modules in command line. For example:\n\n\nIn redis.conf:\n\n\nloadmodule redisearch.so OPT1 OPT2\n\n\nIn redis-cli:\n\n\n127.0.0.6379\n MODULE load redisearch.so OPT1 OPT2\n\n\nIn command-line:\n\n\n$ redis-server --loadmodule ./redisearch.so OPT1 OPT2\n\n\n\n\nRediSearch Configuration Options\n\n\nSAFEMODE\n\n\nIf present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.\n\n\nThis is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily incosistent results (i.e. documents that were valid during the the invokation of the query are not returned because they were deleted durin query processing).\n\n\nDefault:\n\n\nOff (not present)\n\n\nExample\n\n\n$ redis-server --loadmoulde ./redisearch.so SAFEMODE\n\n\n\n\n\n\n\nEXTLOAD {file_name}\n\n\nIf present, we try to load a redisearch extension dynamic library from the specified file path. See \nExtensions\n for details.\n\n\nDefault:\n\n\nNone\n\n\nExample:\n\n\n$ redis-server --loadmoulde ./redisearch.so EXTLOAD ./ext/my_extension.so\n\n\n\n\n\n\n\nNOGC\n\n\nIf set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.\n\n\nDefault:\n\n\nNot set\n\n\nExample:\n\n\n$ redis-server --loadmoulde ./redisearch.so NOGC", 
            "title": "Configuration"
        }, 
        {
            "location": "/Configuring/#run-time-configuration", 
            "text": "RediSearch supports a few run-time configuration options that should be determined when loading the module. In time more options will be added.    Passing Configuration Options  In general, passing configuration options is done by appending arguments after the  --loadmodule  argument in command line,  loadmodule  configurtion directive in a redis config file, or  MODULE LOAD  when loading modules in command line. For example:  In redis.conf:  loadmodule redisearch.so OPT1 OPT2  In redis-cli:  127.0.0.6379  MODULE load redisearch.so OPT1 OPT2  In command-line:  $ redis-server --loadmodule ./redisearch.so OPT1 OPT2", 
            "title": "Run-Time Configuration"
        }, 
        {
            "location": "/Configuring/#redisearch-configuration-options", 
            "text": "", 
            "title": "RediSearch Configuration Options"
        }, 
        {
            "location": "/Configuring/#safemode", 
            "text": "If present in the argument list, RediSearch will turn off concurrency for query processing, and work in a single thread.  This is useful if data consistency is extremely important, and avoids a situation where deletion of documents while querying them can cause momentarily incosistent results (i.e. documents that were valid during the the invokation of the query are not returned because they were deleted durin query processing).", 
            "title": "SAFEMODE"
        }, 
        {
            "location": "/Configuring/#default", 
            "text": "Off (not present)", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example", 
            "text": "$ redis-server --loadmoulde ./redisearch.so SAFEMODE", 
            "title": "Example"
        }, 
        {
            "location": "/Configuring/#extload-file_name", 
            "text": "If present, we try to load a redisearch extension dynamic library from the specified file path. See  Extensions  for details.", 
            "title": "EXTLOAD {file_name}"
        }, 
        {
            "location": "/Configuring/#default_1", 
            "text": "None", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_1", 
            "text": "$ redis-server --loadmoulde ./redisearch.so EXTLOAD ./ext/my_extension.so", 
            "title": "Example:"
        }, 
        {
            "location": "/Configuring/#nogc", 
            "text": "If set, we turn off Garbage Collection for all indexes. This is used mainly for debugging and testing, and should not be set by users.", 
            "title": "NOGC"
        }, 
        {
            "location": "/Configuring/#default_2", 
            "text": "Not set", 
            "title": "Default:"
        }, 
        {
            "location": "/Configuring/#example_2", 
            "text": "$ redis-server --loadmoulde ./redisearch.so NOGC", 
            "title": "Example:"
        }, 
        {
            "location": "/Query_Syntax/", 
            "text": "Search Query Syntax:\n\n\nWe support a simple syntax for complex queries with the following rules:\n\n\n\n\nMulti-word phrases simply a list of tokens, e.g. \nfoo bar baz\n, and imply intersection (AND) of the terms.\n\n\nExact phrases are wrapped in quotes, e.g \n\"hello world\"\n.\n\n\nOR Unions (i.e \nword1 OR word2\n), are expressed with a pipe (\n|\n), e.g. \nhello|hallo|shalom|hola\n.\n\n\nNOT negation (i.e. \nword1 NOT word2\n) of expressions or sub-queries. e.g. \nhello -world\n. As of version 0.19.3, purely negative queries (i.e. \n-foo\n or \n-@title:(foo|bar)\n) are supported. \n\n\nPrefix matches (all terms starting with a prefix) are expressed with a \n*\n following a 3-letter or longer prefix.\n\n\nSelection of specific fields using the syntax \n@field:hello world\n.\n\n\nNumeric Range matches on numeric fields with the syntax \n@field:[{min} {max}]\n.\n\n\nOptional terms or clauses: \nfoo ~bar\n means bar is optional but documents with bar in them will rank higher. \n\n\nAn expression in a query can be wrapped in parentheses to resolve disambiguity, e.g. \n(hello|hella) (world|werld)\n.\n\n\nCombinations of the above can be used together, e.g \nhello (world|foo) \"bar baz\" bbbb\n\n\n\n\nPure Negative Queries\n\n\nAs of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g. \n-hello\n or \n-(@title:foo|bar)\n. The results will be all the documents \nNOT\n containing the query terms.\n\n\nWarning\n: Any complex expression can be negated this way, however caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.\n\n\nField modifiers\n\n\nAs of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword. \n\n\nPer query expression or sub expression, it is possible to specify which fields it matches, by prepending the experssion with the \n@\n symbol, the field name and a \n:\n (colon) symbol. \n\n\nIf a field modifier precedes multiple words, they are considered to be a phrase with the same modifier. \n\n\nIf a field modifier preceds an expression in parentheses, it applies only to the expression inside the parentheses.\n\n\nMultiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:\n\n\nFT.SEARCH cars \n@country:korea @engine:(diesel|hybrid) @class:suv\n\n\n\n\n\n\nMultiple modifiers can be applied to the same term or grouped terms. e.g.:\n\n\nFT.SEARCH idx \n@title|body:(hello world) @url|image:mydomain\n\n\n\n\n\n\nThis will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.\n\n\nNumeric Filters in Query (Since v0.16)\n\n\nIf a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the redis request, or filter with it by specifying filtering rules in the query. The syntax is \n@field:[{min} {max}]\n - e.g. \n@price:[100 200]\n.\n\n\nA few notes on numeric predicates:\n\n\n\n\n\n\nIt is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.\n\n\n\n\n\n\nIt is possible to interesect or union multiple numeric filters in the same query, be it for the same field or different ones.\n\n\n\n\n\n\n-inf\n, \ninf\n and \n+inf\n are acceptable numbers in range. Thus greater-than 100 is expressed as \n[(100 inf]\n.\n\n\n\n\n\n\nNumeric filters are inclusive. Exclusive min or max are expressed with \n(\n prepended to the number, e.g. \n[(100 (200]\n.\n\n\n\n\n\n\nIt is possible to negate a numeric filter by prepending a \n-\n sign to the filter, e.g. returnig a result where price differs from 100 is expressed as: \n@title:foo -@price:[100 100]\n. \n\n\n\n\n\n\nPrefix Matching\n\n\nOn index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending \n*\n to a prefix token. For example:\n\n\nhel* world\n\n\n\n\n\nWill be expanded to cover \n(hello|help|helm|...) world\n. \n\n\nA few notes on prefix searches:\n\n\n\n\n\n\nAs prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffxies.\n\n\n\n\n\n\nAs a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:\n\n\n\n\n\n\nPrefixes are limited to 3 letters or more. \n\n\n\n\n\n\nExpansion is limited to 200 terms or less. \n\n\n\n\n\n\nPrefix matching fully supports unicode and is case insensitive.\n\n\n\n\n\n\nCurrently there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap. \n\n\n\n\n\n\nA Few Query Examples\n\n\n\n\n\n\nSimple phrase query - hello AND world\n\n\nhello world\n\n\n\n\n\n\n\n\n\nExact phrase query - \nhello\n FOLLOWED BY \nworld\n\n\nhello world\n\n\n\n\n\n\n\n\n\n\nUnion: documents containing either \nhello\n OR \nworld\n\n\nhello|world\n\n\n\n\n\n\n\n\n\nNot: documents containing \nhello\n but not \nworld\n\n\nhello -world\n\n\n\n\n\n\n\n\n\nIntersection of unions\n\n\n(hello|halo) (world|werld)\n\n\n\n\n\n\n\n\n\nNegation of union\n\n\nhello -(world|werld)\n\n\n\n\n\n\n\n\n\nUnion inside phrase\n\n\n(barack|barrack) obama\n\n\n\n\n\n\n\n\n\nOptional terms with higher priority to ones containing more matches:\n\n\nobama ~barack ~michelle\n\n\n\n\n\n\n\n\n\nExact phrase in one field, one word in aonther field:\n\n\n@title:\nbarack obama\n @job:president\n\n\n\n\n\n\n\n\n\nCombined AND, OR with field specifiers:\n\n\n@title:hello world @body:(foo bar) @category:(articles|biographies)\n\n\n\n\n\n\n\n\n\nPrefix Queries:\n\n\nhello worl*\n\nhel* worl*\n\nhello -worl*\n\n\n\n\n\n\n\n\n\nNumeric Filtering - products named \"tv\" with a price range of 200-500:\n\n\n@name:tv @price:[200 500]\n\n\n\n\n\n\n\n\n\nNumeric Filtering - users with age greater than 18:\n\n\n@age:[(18 +inf]\n\n\n\n\n\n\n\n\n\nTechnical Note\n\n\nThe query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition \nat the git repo.", 
            "title": "Query Syntax"
        }, 
        {
            "location": "/Query_Syntax/#search-query-syntax", 
            "text": "We support a simple syntax for complex queries with the following rules:   Multi-word phrases simply a list of tokens, e.g.  foo bar baz , and imply intersection (AND) of the terms.  Exact phrases are wrapped in quotes, e.g  \"hello world\" .  OR Unions (i.e  word1 OR word2 ), are expressed with a pipe ( | ), e.g.  hello|hallo|shalom|hola .  NOT negation (i.e.  word1 NOT word2 ) of expressions or sub-queries. e.g.  hello -world . As of version 0.19.3, purely negative queries (i.e.  -foo  or  -@title:(foo|bar) ) are supported.   Prefix matches (all terms starting with a prefix) are expressed with a  *  following a 3-letter or longer prefix.  Selection of specific fields using the syntax  @field:hello world .  Numeric Range matches on numeric fields with the syntax  @field:[{min} {max}] .  Optional terms or clauses:  foo ~bar  means bar is optional but documents with bar in them will rank higher.   An expression in a query can be wrapped in parentheses to resolve disambiguity, e.g.  (hello|hella) (world|werld) .  Combinations of the above can be used together, e.g  hello (world|foo) \"bar baz\" bbbb", 
            "title": "Search Query Syntax:"
        }, 
        {
            "location": "/Query_Syntax/#pure-negative-queries", 
            "text": "As of version 0.19.3 it is possible to have a query consisting of just a negative expression, e.g.  -hello  or  -(@title:foo|bar) . The results will be all the documents  NOT  containing the query terms.  Warning : Any complex expression can be negated this way, however caution should be taken here: if a negative expression has little or no results, this is equivalent to traversing and ranking all the documents in the index, which can be slow and cause high CPU consumption.", 
            "title": "Pure Negative Queries"
        }, 
        {
            "location": "/Query_Syntax/#field-modifiers", 
            "text": "As of version 0.12 it is possible to specify field modifiers in the query and not just using the INFIELDS global keyword.   Per query expression or sub expression, it is possible to specify which fields it matches, by prepending the experssion with the  @  symbol, the field name and a  :  (colon) symbol.   If a field modifier precedes multiple words, they are considered to be a phrase with the same modifier.   If a field modifier preceds an expression in parentheses, it applies only to the expression inside the parentheses.  Multiple modifiers can be combined to create complex filtering on several fields. For example, if we have an index of car models, with a vehicle class, country of origin and engine type, we can search for SUVs made in Korea with hybrid or diesel engines - with the following query:  FT.SEARCH cars  @country:korea @engine:(diesel|hybrid) @class:suv   Multiple modifiers can be applied to the same term or grouped terms. e.g.:  FT.SEARCH idx  @title|body:(hello world) @url|image:mydomain   This will search for documents that have \"hello world\" either in the body or the title, and the term \"mydomain\" in their url or image fields.", 
            "title": "Field modifiers"
        }, 
        {
            "location": "/Query_Syntax/#numeric-filters-in-query-since-v016", 
            "text": "If a field in the schema is defined as NUMERIC, it is possible to either use the FILTER argument in the redis request, or filter with it by specifying filtering rules in the query. The syntax is  @field:[{min} {max}]  - e.g.  @price:[100 200] .", 
            "title": "Numeric Filters in Query (Since v0.16)"
        }, 
        {
            "location": "/Query_Syntax/#a-few-notes-on-numeric-predicates", 
            "text": "It is possible to specify a numeric predicate as the entire query, whereas it is impossible to do it with the FILTER argument.    It is possible to interesect or union multiple numeric filters in the same query, be it for the same field or different ones.    -inf ,  inf  and  +inf  are acceptable numbers in range. Thus greater-than 100 is expressed as  [(100 inf] .    Numeric filters are inclusive. Exclusive min or max are expressed with  (  prepended to the number, e.g.  [(100 (200] .    It is possible to negate a numeric filter by prepending a  -  sign to the filter, e.g. returnig a result where price differs from 100 is expressed as:  @title:foo -@price:[100 100] .", 
            "title": "A few notes on numeric predicates:"
        }, 
        {
            "location": "/Query_Syntax/#prefix-matching", 
            "text": "On index updating, we maintain a dictionary of all terms in the index. This can be used to match all terms starting with a given prefix. Selecting prefix matches is done by appending  *  to a prefix token. For example:  hel* world  Will be expanded to cover  (hello|help|helm|...) world .", 
            "title": "Prefix Matching"
        }, 
        {
            "location": "/Query_Syntax/#a-few-notes-on-prefix-searches", 
            "text": "As prefixes can be expanded into many many terms, use them with caution. There is no magic going on, the expansion will create a Union operation of all suffxies.    As a protective measure to avoid selecting too many terms, and block redis, which is single threaded, there are two limitations on prefix matching:    Prefixes are limited to 3 letters or more.     Expansion is limited to 200 terms or less.     Prefix matching fully supports unicode and is case insensitive.    Currently there is no sorting or bias based on suffix popularity, but this is on the near-term roadmap.", 
            "title": "A few notes on prefix searches:"
        }, 
        {
            "location": "/Query_Syntax/#a-few-query-examples", 
            "text": "Simple phrase query - hello AND world  hello world    Exact phrase query -  hello  FOLLOWED BY  world  hello world     Union: documents containing either  hello  OR  world  hello|world    Not: documents containing  hello  but not  world  hello -world    Intersection of unions  (hello|halo) (world|werld)    Negation of union  hello -(world|werld)    Union inside phrase  (barack|barrack) obama    Optional terms with higher priority to ones containing more matches:  obama ~barack ~michelle    Exact phrase in one field, one word in aonther field:  @title: barack obama  @job:president    Combined AND, OR with field specifiers:  @title:hello world @body:(foo bar) @category:(articles|biographies)    Prefix Queries:  hello worl*\n\nhel* worl*\n\nhello -worl*    Numeric Filtering - products named \"tv\" with a price range of 200-500:  @name:tv @price:[200 500]    Numeric Filtering - users with age greater than 18:  @age:[(18 +inf]", 
            "title": "A Few Query Examples"
        }, 
        {
            "location": "/Query_Syntax/#technical-note", 
            "text": "The query parser is built using the Lemon Parser Generator and a Ragel based lexer. You can see the grammar definition  at the git repo.", 
            "title": "Technical Note"
        }, 
        {
            "location": "/Sorting/", 
            "text": "Sorting By Indexed Fields\n\n\nAs of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name. \n\n\nDeclaring Sortable Fields\n\n\nWhen creating the index with FT.CREATE, you can declare \nTEXT\n and \nNUMERIC\n properties to be \nSORTABLE\n. When a property is sortable, we can later decide to order the results by its values. For example, in the followig schema:\n\n\n FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE\n\n\n\n\n\nThe fields \nlast_name\n and \nage\n are sortable, but \nfirst_name\n isn't. This means we can search by either first and/or last name, and sort by last name or age. \n\n\nNote on sortable TEXT fields\n\n\nIn the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.\n\n\nAlso note that text fields get normalized and lowercased in a unicode-safe way when stored for sorting , and currently there is no way to change this behavior. This means that \nAmerica\n and \namerica\n are considered equal in terms of sorting.\n\n\nSpecifying SORTBY\n\n\nIf an index includes sortable fields, you can add the \nSORTBY\n parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If \nWITHSCORES\n is specified along with \nSORTBY\n, the scores returned are simply the relative position of each result in the result set.\n\n\nThe syntax for SORTBY is:\n\n\nSORTBY {field_name} [ASC|DESC]\n\n\n\n\n\n\n\n\n\nfield_name must be a sortabl field defined in the schema.\n\n\n\n\n\n\nASC means the order will be ascending, DESC that it will be descending.\n\n\n\n\n\n\nThe default ordering is ASC if not specified otherwise.\n\n\n\n\n\n\nQuick Example\n\n\n FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users\n\n FT.ADD users user1 1.0 FIELDS first_name \nalice\n last_name \njones\n age 35\n\n FT.ADD users user2 1.0 FIELDS first_name \nbob\n last_name \njones\n age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name\n\n FT.SEARCH users \n@last_name:jones\n SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age\n\n FT.SEARCH users \nalice jones\n SORTBY age ASC", 
            "title": "Sortable Values"
        }, 
        {
            "location": "/Sorting/#sorting-by-indexed-fields", 
            "text": "As of RediSearch 0.15, it is possible to bypass the scoring function mechanism, and order search results by the value of different document properties (fields) directly - even if the sorting field is not used by the query. For example, you can search for first name and sort by last name.", 
            "title": "Sorting By Indexed Fields"
        }, 
        {
            "location": "/Sorting/#declaring-sortable-fields", 
            "text": "When creating the index with FT.CREATE, you can declare  TEXT  and  NUMERIC  properties to be  SORTABLE . When a property is sortable, we can later decide to order the results by its values. For example, in the followig schema:   FT.CREATE users SCHEMA first_name TEXT last_name TEXT SORTABLE age NUMERIC SORTABLE  The fields  last_name  and  age  are sortable, but  first_name  isn't. This means we can search by either first and/or last name, and sort by last name or age.", 
            "title": "Declaring Sortable Fields"
        }, 
        {
            "location": "/Sorting/#note-on-sortable-text-fields", 
            "text": "In the current implementation, when declaring a sortable field, its content gets copied into a special location in the index, for fast access on sorting. This means that making long text fields sortable is very expensive, and you should be careful with it.  Also note that text fields get normalized and lowercased in a unicode-safe way when stored for sorting , and currently there is no way to change this behavior. This means that  America  and  america  are considered equal in terms of sorting.", 
            "title": "Note on sortable TEXT fields"
        }, 
        {
            "location": "/Sorting/#specifying-sortby", 
            "text": "If an index includes sortable fields, you can add the  SORTBY  parameter to the search request (outside the query body), and order the results by it. This overrides the scoring function mechanism, and the two cannot be combined. If  WITHSCORES  is specified along with  SORTBY , the scores returned are simply the relative position of each result in the result set.  The syntax for SORTBY is:  SORTBY {field_name} [ASC|DESC]    field_name must be a sortabl field defined in the schema.    ASC means the order will be ascending, DESC that it will be descending.    The default ordering is ASC if not specified otherwise.", 
            "title": "Specifying SORTBY"
        }, 
        {
            "location": "/Sorting/#quick-example", 
            "text": "FT.CREATE users SCHEMA first_name TEXT SORTABLE last_name TEXT age NUMERIC SORTABLE\n\n# Add some users  FT.ADD users user1 1.0 FIELDS first_name  alice  last_name  jones  age 35  FT.ADD users user2 1.0 FIELDS first_name  bob  last_name  jones  age 36\n\n# Searching while sorting\n\n# Searching by last name and sorting by first name  FT.SEARCH users  @last_name:jones  SORTBY first_name DESC\n\n# Searching by both first and last name, and sorting by age  FT.SEARCH users  alice jones  SORTBY age ASC", 
            "title": "Quick Example"
        }, 
        {
            "location": "/Extensions/", 
            "text": "Extending RediSearch\n\n\nRediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C and compiled into the engine when building it.\n\n\nThere are two kinds of extension APIs at the moment: \n\n\n\n\nQuery Expanders\n, whose role is to expand query tokens (i.e. stemmers).\n\n\nScoring Funtions\n, whose role is to rank search results in query time.\n\n\n\n\nRegistering Extensions\n\n\nCurrently there is no dynamic linking of extensions and they need to be compiled into the engine. However, the API is already geared for easy registration of run-time extensions. \n\n\nThe entry point is a function receiving an \nRSExtensionCtx\n object, that contains functions for registering the expanders/scorers. \n\n\nRight now, it is necessary to call these init functions explicitly in \nmodule.c\n, but in the future this will be automated.\n\n\nHere is an example of an extension initialization function:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nint\n \nMyExtensionInit\n(\nRSExtensionCtx\n \n*\nctx\n)\n \n{\n\n\n  \n/* Register  a scoring function with an alias my_scorer and no special private data and free function */\n\n  \nif\n \n(\nctx\n-\nRegisterScoringFunction\n(\nmy_scorer\n,\n \nMyCustomScorer\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \n/* Register a query expander  */\n\n  \nif\n \n(\nctx\n-\nRegisterQueryExpander\n(\nmy_expander\n,\n \nMyExpander\n,\n \nNULL\n,\n \nNULL\n)\n \n==\n\n      \nREDISEARCH_ERR\n)\n \n{\n\n    \nreturn\n \nREDISEARCH_ERR\n;\n\n  \n}\n\n\n  \nreturn\n \nREDISEARCH_OK\n;\n\n\n}\n\n\n\n\n\n\nCalling your custom functions\n\n\nWhen performing a query, you can tell RediSearch to use your scorers or expanders by specifing the SCORER or EXPANDER arguments, with the given alias.\ne.g.:\n\n\nFT.SEARCH my_index \nfoo bar\n EXPANDER my_expander SCORER my_scorer\n\n\n\n\n\nNOTE\n: Expander and scorer aliases are \ncase sensitive\n.\n\n\nThe Query Expander API\n\n\nAt the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.\n\n\nThe API for an expander is the following:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nMyQueryExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \n...\n\n\n}\n\n\n\n\n\n\nRSQueryExpanderCtx\n\n\nRSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:\n\n\ntypedef\n \nstruct\n \nRSQueryExpanderCtx\n \n{\n\n\n  \n/* Opaque query object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQuery\n \n*\nquery\n;\n\n\n  \n/* Opaque query node object used internally by the engine, and should not be accessed */\n\n  \nstruct\n \nRSQueryNode\n \n**\ncurrentNode\n;\n\n\n  \n/* Private data of the extension, set on extension initialization */\n\n  \nvoid\n \n*\nprivdata\n;\n\n\n  \n/* The language of the query, defaults to \nenglish\n */\n\n  \nconst\n \nchar\n \n*\nlanguage\n;\n\n\n  \n/* ExpandToken allows the user to add an expansion of the token in the query, that will be\n\n\n   * union-merged with the given token in query time. str is the expanded string, len is its length,\n\n\n   * and flags is a 32 bit flag mask that can be used by the extension to set private information on\n\n\n   * the token */\n\n  \nvoid\n \n(\n*\nExpandToken\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nconst\n \nchar\n \n*\nstr\n,\n \nsize_t\n \nlen\n,\n\n                      \nRSTokenFlags\n \nflags\n);\n\n\n  \n/* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)\n\n\n   */\n\n  \nvoid\n \n(\n*\nSetPayload\n)(\nstruct\n \nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSPayload\n \npayload\n);\n\n\n\n}\n \nRSQueryExpanderCtx\n;\n\n\n\n\n\n\nRSToken\n\n\nRSToken represents a single query token to be expanded, and is defined as:\n\n\n/* A token in the query. The expanders receive query tokens and can expand the query with more query\n\n\n * tokens */\n\n\ntypedef\n \nstruct\n \n{\n\n  \n/* The token string - which may or may not be NULL terminated */\n\n  \nconst\n \nchar\n \n*\nstr\n;\n\n  \n/* The token length */\n\n  \nsize_t\n \nlen\n;\n\n\n  \n/* 1 if the token is the result of query expansion */\n\n  \nuint8_t\n \nexpanded\n:\n1\n;\n\n\n  \n/* Extension specific token flags that can be examined later by the scoring function */\n\n  \nRSTokenFlags\n \nflags\n;\n\n\n}\n \nRSToken\n;\n\n\n\n\n\n\nThe Scoring Function API\n\n\nA scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.\n\n\nSince the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized. \n\n\nA scoring function is applied to each potential result (per document) and is implemented with the following signature:\n\n\ndouble\n \nMyScoringFunction\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nres\n,\n\n                                    \nRSDocumentMetadata\n \n*\ndmd\n,\n \ndouble\n \nminScore\n);\n\n\n\n\n\n\nRSScoringFunctionCtx is a context that implements some helper methods. \n\n\nRSIndexResult is the result information - containing the document id, frequency, terms and offsets. \n\n\nRSDocumentMetadata is an object holding global information about the document, such as its a-priory score. \n\n\nminSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.\n\n\nThe return value of the function is double representing the final score of the result. Returning 0 filters the result out automatically, thus a scoring function can act as a filter function as well.\n\n\nRSScoringFunctionCtx\n\n\nThis is an object containing the following members:\n\n\n\n\nvoid *privdata\n: a pointer to an object set by the extension on initialization time.\n\n\nRSPayload payload\n: A Payload object set either by the query expander or the client.\n\n\nint GetSlop(RSIndexResult *res)\n: A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.\n\n\n\n\nRSIndexResult\n\n\nThis is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.\n\n\nSee redisearch.h for details\n\n\nRSDocumentMetadata\n\n\nThis is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function. \n\n\nExample Query Expander\n\n\nThis example query expander expands each token with the the term foo:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\nvoid\n \nDummyExpander\n(\nRSQueryExpanderCtx\n \n*\nctx\n,\n \nRSToken\n \n*\ntoken\n)\n \n{\n\n    \nctx\n-\nExpandToken\n(\nctx\n,\n \nstrdup\n(\nfoo\n),\n \nstrlen\n(\nfoo\n),\n \n0x1337\n);\n  \n\n}\n\n\n\n\n\n\nExample Scoring Function\n\n\nThis is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:\n\n\n#include\n \nredisearch.h\n //must be in the include path\n\n\n\ndouble\n \nTFIDFScorer\n(\nRSScoringFunctionCtx\n \n*\nctx\n,\n \nRSIndexResult\n \n*\nh\n,\n \nRSDocumentMetadata\n \n*\ndmd\n,\n\n                   \ndouble\n \nminScore\n)\n \n{\n\n  \n// no need to evaluate documents with score 0 \n\n  \nif\n \n(\ndmd\n-\nscore\n \n==\n \n0\n)\n \nreturn\n \n0\n;\n\n\n  \n// calculate sum(tf-idf) for each term in the result\n\n  \ndouble\n \ntfidf\n \n=\n \n0\n;\n\n  \nfor\n \n(\nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nh\n-\nnumRecords\n;\n \ni\n++\n)\n \n{\n\n    \n// take the term frequency and multiply by the term IDF, add that to the total\n\n    \ntfidf\n \n+=\n \n(\nfloat\n)\nh\n-\nrecords\n[\ni\n].\nfreq\n \n*\n \n(\nh\n-\nrecords\n[\ni\n].\nterm\n \n?\n \nh\n-\nrecords\n[\ni\n].\nterm\n-\nidf\n \n:\n \n0\n);\n\n  \n}\n\n  \n// normalize by the maximal frequency of any term in the document   \n\n  \ntfidf\n \n/=\n  \n(\ndouble\n)\ndmd\n-\nmaxFreq\n;\n\n\n  \n// multiply by the document score (between 0 and 1)\n\n  \ntfidf\n \n*=\n \ndmd\n-\nscore\n;\n\n\n  \n// no need to factor the slop if tfidf is already below minimal score\n\n  \nif\n \n(\ntfidf\n \n \nminScore\n)\n \n{\n\n    \nreturn\n \n0\n;\n\n  \n}\n\n\n  \n// get the slop and divide the result by it, making sure we prefer results with closer terms\n\n  \ntfidf\n \n/=\n \n(\ndouble\n)\nctx\n-\nGetSlop\n(\nh\n);\n\n\n  \nreturn\n \ntfidf\n;\n\n\n}", 
            "title": "Extension API"
        }, 
        {
            "location": "/Extensions/#extending-redisearch", 
            "text": "RediSearch supports an extension mechanism, much like Redis supports modules. The API is very minimal at the moment, and it does not yet support dynamic loading of extensions in run-time. Instead, extensions must be written in C and compiled into the engine when building it.  There are two kinds of extension APIs at the moment:    Query Expanders , whose role is to expand query tokens (i.e. stemmers).  Scoring Funtions , whose role is to rank search results in query time.", 
            "title": "Extending RediSearch"
        }, 
        {
            "location": "/Extensions/#registering-extensions", 
            "text": "Currently there is no dynamic linking of extensions and they need to be compiled into the engine. However, the API is already geared for easy registration of run-time extensions.   The entry point is a function receiving an  RSExtensionCtx  object, that contains functions for registering the expanders/scorers.   Right now, it is necessary to call these init functions explicitly in  module.c , but in the future this will be automated.  Here is an example of an extension initialization function:  #include   redisearch.h  //must be in the include path  int   MyExtensionInit ( RSExtensionCtx   * ctx )   { \n\n   /* Register  a scoring function with an alias my_scorer and no special private data and free function */ \n   if   ( ctx - RegisterScoringFunction ( my_scorer ,   MyCustomScorer ,   NULL ,   NULL )   ==   REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   /* Register a query expander  */ \n   if   ( ctx - RegisterQueryExpander ( my_expander ,   MyExpander ,   NULL ,   NULL )   == \n       REDISEARCH_ERR )   { \n     return   REDISEARCH_ERR ; \n   } \n\n   return   REDISEARCH_OK ;  }", 
            "title": "Registering Extensions"
        }, 
        {
            "location": "/Extensions/#calling-your-custom-functions", 
            "text": "When performing a query, you can tell RediSearch to use your scorers or expanders by specifing the SCORER or EXPANDER arguments, with the given alias.\ne.g.:  FT.SEARCH my_index  foo bar  EXPANDER my_expander SCORER my_scorer  NOTE : Expander and scorer aliases are  case sensitive .", 
            "title": "Calling your custom functions"
        }, 
        {
            "location": "/Extensions/#the-query-expander-api", 
            "text": "At the moment, we only support basic query expansion, one token at a time. An expander can decide to expand any given token with as many tokens it wishes, that will be Union-merged in query time.  The API for an expander is the following:  #include   redisearch.h  //must be in the include path  void   MyQueryExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ...  }", 
            "title": "The Query Expander API"
        }, 
        {
            "location": "/Extensions/#rsqueryexpanderctx", 
            "text": "RSQueryExpanderCtx is a context that contains private data of the extension, and a callback method to expand the query. It is defined as:  typedef   struct   RSQueryExpanderCtx   { \n\n   /* Opaque query object used internally by the engine, and should not be accessed */ \n   struct   RSQuery   * query ; \n\n   /* Opaque query node object used internally by the engine, and should not be accessed */ \n   struct   RSQueryNode   ** currentNode ; \n\n   /* Private data of the extension, set on extension initialization */ \n   void   * privdata ; \n\n   /* The language of the query, defaults to  english  */ \n   const   char   * language ; \n\n   /* ExpandToken allows the user to add an expansion of the token in the query, that will be     * union-merged with the given token in query time. str is the expanded string, len is its length,     * and flags is a 32 bit flag mask that can be used by the extension to set private information on     * the token */ \n   void   ( * ExpandToken )( struct   RSQueryExpanderCtx   * ctx ,   const   char   * str ,   size_t   len , \n                       RSTokenFlags   flags ); \n\n   /* SetPayload allows the query expander to set GLOBAL payload on the query (not unique per token)     */ \n   void   ( * SetPayload )( struct   RSQueryExpanderCtx   * ctx ,   RSPayload   payload );  }   RSQueryExpanderCtx ;", 
            "title": "RSQueryExpanderCtx"
        }, 
        {
            "location": "/Extensions/#rstoken", 
            "text": "RSToken represents a single query token to be expanded, and is defined as:  /* A token in the query. The expanders receive query tokens and can expand the query with more query   * tokens */  typedef   struct   { \n   /* The token string - which may or may not be NULL terminated */ \n   const   char   * str ; \n   /* The token length */ \n   size_t   len ; \n\n   /* 1 if the token is the result of query expansion */ \n   uint8_t   expanded : 1 ; \n\n   /* Extension specific token flags that can be examined later by the scoring function */ \n   RSTokenFlags   flags ;  }   RSToken ;", 
            "title": "RSToken"
        }, 
        {
            "location": "/Extensions/#the-scoring-function-api", 
            "text": "A scoring function receives each document being evaluated by the query, for final ranking. \nIt has access to all the query terms that brought up the document,and to metadata about the\ndocument such as its a-priory score, length, etc.  Since the scoring function is evaluated per each document, potentially millions of times, and since\nredis is single threaded - it is important that it works as fast as possible and be heavily optimized.   A scoring function is applied to each potential result (per document) and is implemented with the following signature:  double   MyScoringFunction ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * res , \n                                     RSDocumentMetadata   * dmd ,   double   minScore );   RSScoringFunctionCtx is a context that implements some helper methods.   RSIndexResult is the result information - containing the document id, frequency, terms and offsets.   RSDocumentMetadata is an object holding global information about the document, such as its a-priory score.   minSocre is the minimal score that will yield a result that will be relevant to the search. It can be used to stop processing mid-way of before we even start.  The return value of the function is double representing the final score of the result. Returning 0 filters the result out automatically, thus a scoring function can act as a filter function as well.", 
            "title": "The Scoring Function API"
        }, 
        {
            "location": "/Extensions/#rsscoringfunctionctx", 
            "text": "This is an object containing the following members:   void *privdata : a pointer to an object set by the extension on initialization time.  RSPayload payload : A Payload object set either by the query expander or the client.  int GetSlop(RSIndexResult *res) : A callback method that yields the total minimal distance between the query terms. This can be used to prefer results where the \"slop\" is smaller and the terms are nearer to each other.", 
            "title": "RSScoringFunctionCtx"
        }, 
        {
            "location": "/Extensions/#rsindexresult", 
            "text": "This is an object holding the information about the current result in the index, which is an aggregate of all the terms that resulted in the current document being considered a valid result.  See redisearch.h for details", 
            "title": "RSIndexResult"
        }, 
        {
            "location": "/Extensions/#rsdocumentmetadata", 
            "text": "This is an object describing global information, unrelated to the current query, about the document being evaluated by the scoring function.", 
            "title": "RSDocumentMetadata"
        }, 
        {
            "location": "/Extensions/#example-query-expander", 
            "text": "This example query expander expands each token with the the term foo:  #include   redisearch.h  //must be in the include path  void   DummyExpander ( RSQueryExpanderCtx   * ctx ,   RSToken   * token )   { \n     ctx - ExpandToken ( ctx ,   strdup ( foo ),   strlen ( foo ),   0x1337 );    }", 
            "title": "Example Query Expander"
        }, 
        {
            "location": "/Extensions/#example-scoring-function", 
            "text": "This is an actual scoring function, calculating TF-IDF for the document, multiplying that by the document score, and dividing that by the slop:  #include   redisearch.h  //must be in the include path  double   TFIDFScorer ( RSScoringFunctionCtx   * ctx ,   RSIndexResult   * h ,   RSDocumentMetadata   * dmd , \n                    double   minScore )   { \n   // no need to evaluate documents with score 0  \n   if   ( dmd - score   ==   0 )   return   0 ; \n\n   // calculate sum(tf-idf) for each term in the result \n   double   tfidf   =   0 ; \n   for   ( int   i   =   0 ;   i     h - numRecords ;   i ++ )   { \n     // take the term frequency and multiply by the term IDF, add that to the total \n     tfidf   +=   ( float ) h - records [ i ]. freq   *   ( h - records [ i ]. term   ?   h - records [ i ]. term - idf   :   0 ); \n   } \n   // normalize by the maximal frequency of any term in the document    \n   tfidf   /=    ( double ) dmd - maxFreq ; \n\n   // multiply by the document score (between 0 and 1) \n   tfidf   *=   dmd - score ; \n\n   // no need to factor the slop if tfidf is already below minimal score \n   if   ( tfidf     minScore )   { \n     return   0 ; \n   } \n\n   // get the slop and divide the result by it, making sure we prefer results with closer terms \n   tfidf   /=   ( double ) ctx - GetSlop ( h ); \n\n   return   tfidf ;  }", 
            "title": "Example Scoring Function"
        }, 
        {
            "location": "/Stemming/", 
            "text": "Stemming Support\n\n\nRediSearch supports stemming - that is adding the base form of a word to the index. This allows \nthe query for \"going\" to also return results for \"go\" and \"gone\", for example. \n\n\nThe current stemming support is based on the Snowball stemmer library, which supports most European\nlanguages, as well as Arabic and other. We hope to include more languages soon (if you need a specicif\nlangauge support, please open an issue). \n\n\nFor further details see the \nSnowball Stemmer website\n.\n\n\nSupported languages:\n\n\nThe following languages are supported, and can be passed to the engine \nwhen indexing or querying, with lowercase letters:\n\n\n\n\narabic\n\n\ndanish\n\n\ndutch\n\n\nenglish\n\n\nfinnish\n\n\nfrench\n\n\ngerman\n\n\nhungarian\n\n\nitalian\n\n\nnorwegian\n\n\nportuguese\n\n\nromanian\n\n\nrussian\n\n\nspanish\n\n\nswedish\n\n\ntamil\n\n\nturkish", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#stemming-support", 
            "text": "RediSearch supports stemming - that is adding the base form of a word to the index. This allows \nthe query for \"going\" to also return results for \"go\" and \"gone\", for example.   The current stemming support is based on the Snowball stemmer library, which supports most European\nlanguages, as well as Arabic and other. We hope to include more languages soon (if you need a specicif\nlangauge support, please open an issue).   For further details see the  Snowball Stemmer website .", 
            "title": "Stemming Support"
        }, 
        {
            "location": "/Stemming/#supported-languages", 
            "text": "The following languages are supported, and can be passed to the engine \nwhen indexing or querying, with lowercase letters:   arabic  danish  dutch  english  finnish  french  german  hungarian  italian  norwegian  portuguese  romanian  russian  spanish  swedish  tamil  turkish", 
            "title": "Supported languages:"
        }, 
        {
            "location": "/python_client/", 
            "text": "Package redisearch Documentation\n\n\nOverview\n\n\nredisearch-py\n is a python search engine library that utilizes the RediSearch Redis Module API.\n\n\nIt is the \"official\" client of redisearch, and should be regarded as its canonical client implementation.\n\n\nThe source code can be found at \nhttp://github.com/RedisLabs/redisearch-py\n\n\nExample: Using the Python Client\n\n\nfrom\n \nredisearch\n \nimport\n \nClient\n,\n \nTextField\n,\n \nNumericField\n\n\n\n# Creating a client with a given index name\n\n\nclient\n \n=\n \nClient\n(\nmyIndex\n)\n\n\n\n# Creating the index definition and schema\n\n\nclient\n.\ncreate_index\n([\nTextField\n(\ntitle\n,\n \nweight\n=\n5.0\n),\n \nTextField\n(\nbody\n)])\n\n\n\n# Indexing a document\n\n\nclient\n.\nadd_document\n(\ndoc1\n,\n \ntitle\n \n=\n \nRediSearch\n,\n \nbody\n \n=\n \nRedisearch impements a search engine on top of redis\n)\n\n\n\n# Simple search\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n)\n\n\n\n# the result has the total number of results, and a list of documents\n\n\nprint\n \nres\n.\ntotal\n \n# \n1\n\n\nprint\n \nres\n.\ndocs\n[\n0\n]\n.\ntitle\n \n\n\n# Searching with snippets\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nsearch engine\n,\n \nsnippet_sizes\n \n=\n \n{\nbody\n:\n \n50\n})\n\n\n\n# Searching with complext parameters:\n\n\nq\n \n=\n \nQuery\n(\nsearch engine\n)\n.\nverbatim\n()\n.\nno_content\n()\n.\npaging\n(\n0\n,\n5\n)\n\n\nres\n \n=\n \nclient\n.\nsearch\n(\nq\n)\n\n\n\n\n\n\nExample: Using the Auto Completer Client:\n\n\n# Using the auto-completer\n\n\nac\n \n=\n \nAutoCompleter\n(\nac\n)\n\n\n\n# Adding some terms\n\n\nac\n.\nadd_suggestions\n(\nSuggestion\n(\nfoo\n,\n \n5.0\n),\n \nSuggestion\n(\nbar\n,\n \n1.0\n))\n\n\n\n# Getting suggestions\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n)\n \n# returns nothing\n\n\n\nsuggs\n \n=\n \nac\n.\nget_suggestions\n(\ngoo\n,\n \nfuzzy\n \n=\n \nTrue\n)\n \n# returns [\nfoo\n]\n\n\n\n\n\n\nInstalling\n\n\n\n\n\n\nInstall redis 4.0 RC2 or above\n\n\n\n\n\n\nInstall RediSearch\n\n\n\n\n\n\nInstall the python client\n\n\n\n\n\n\n$ pip install redisearch\n\n\n\n\n\nClass AutoCompleter\n\n\nA client to RediSearch's AutoCompleter API\n\n\nIt provides prefix searches with optionally fuzzy matching of prefixes    \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nkey\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new AutoCompleter client for the given key, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_suggestions\n\n\ndef\n \nadd_suggestions\n(\nself\n,\n \n*\nsuggestions\n,\n \n**\nkwargs\n)\n\n\n\n\n\n\nAdd suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.\n\n\nIf kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores\n\n\ndelete\n\n\ndef\n \ndelete\n(\nself\n,\n \nstring\n)\n\n\n\n\n\n\nDelete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise\n\n\nget_suggestions\n\n\ndef\n \nget_suggestions\n(\nself\n,\n \nprefix\n,\n \nfuzzy\n=\nFalse\n,\n \nnum\n=\n10\n,\n \nwith_scores\n=\nFalse\n,\n \nwith_payloads\n=\nFalse\n)\n\n\n\n\n\n\nGet a list of suggestions from the AutoCompleter, for a given prefix\n\n\nParameters:\n\n\n\n\nprefix\n: the prefix we are searching. \nMust be valid ascii or utf-8\n\n\nfuzzy\n: If set to true, the prefix search is done in fuzzy mode. \n    \nNOTE\n: Running fuzzy searches on short (\n3 letters) prefixes can be very slow, and even scan the entire index.\n\n\nwith_scores\n: if set to true, we also return the (refactored) score of each suggestion. \n  This is normally not needed, and is NOT the original score inserted into the index\n\n\nwith_payloads\n: Return suggestion payloads\n\n\nnum\n: The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.\n\n\n\n\nReturns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.\n\n\nlen\n\n\ndef\n \nlen\n(\nself\n)\n\n\n\n\n\n\nReturn the number of entries in the AutoCompleter index\n\n\nClass Client\n\n\nA client for the RediSearch module. \nIt abstracts the API of the module and lets you just use the engine \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nindex_name\n,\n \nhost\n=\nlocalhost\n,\n \nport\n=\n6379\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nCreate a new Client for the given index_name, and optional host and port\n\n\nIf conn is not None, we employ an already existing redis connection\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a single document to the index.\n\n\nParameters\n\n\n\n\ndoc_id\n: the id of the saved document.\n\n\nnosave\n: if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.\n\n\nscore\n: the document ranking, between 0.0 and 1.0 \n\n\npayload\n: optional inner-index payload we can save for fast access in scoring functions\n\n\nreplace\n: if True, and the document already is in the index, we perform an update and reindex the document\n\n\nfields\n kwargs dictionary of the document fields to be saved and/or indexed. \n             NOTE: Geo points shoule be encoded as strings of \"lon,lat\"\n\n\n\n\nbatch_indexer\n\n\ndef\n \nbatch_indexer\n(\nself\n,\n \nchunk_size\n=\n100\n)\n\n\n\n\n\n\nCreate a new batch indexer from the client with a given chunk size\n\n\ncreate_index\n\n\ndef\n \ncreate_index\n(\nself\n,\n \nfields\n,\n \nno_term_offsets\n=\nFalse\n,\n \nno_field_flags\n=\nFalse\n,\n \nno_score_indexes\n=\nFalse\n,\n \nstopwords\n=\nNone\n)\n\n\n\n\n\n\nCreate the search index. Creating an existing index juts updates its properties\n\n\nParameters:\n\n\n\n\nfields\n: a list of TextField or NumericField objects\n\n\nno_term_offsets\n: If true, we will not save term offsets in the index\n\n\nno_field_flags\n: If true, we will not save field flags that allow searching in specific fields\n\n\nno_score_indexes\n: If true, we will not save optimized top score indexes for single word queries\n\n\nstopwords\n: If not None, we create the index with this custom stopword list. The list can be empty\n\n\n\n\ndelete_document\n\n\ndef\n \ndelete_document\n(\nself\n,\n \ndoc_id\n,\n \nconn\n=\nNone\n)\n\n\n\n\n\n\nDelete a document from index\nReturns 1 if the document was deleted, 0 if not\n\n\ndrop_index\n\n\ndef\n \ndrop_index\n(\nself\n)\n\n\n\n\n\n\nDrop the index if it exists\n\n\ninfo\n\n\ndef\n \ninfo\n(\nself\n)\n\n\n\n\n\n\nGet info an stats about the the current index, including the number of documents, memory consumption, etc\n\n\nload_document\n\n\ndef\n \nload_document\n(\nself\n,\n \nid\n)\n\n\n\n\n\n\nLoad a single document by id\n\n\nsearch\n\n\ndef\n \nsearch\n(\nself\n,\n \nquery\n,\n \nsnippet_sizes\n=\nNone\n)\n\n\n\n\n\n\nSearch the index for a given query, and return a result of documents\n\n\nParameters\n\n\n\n\nquery\n: the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format\n\n\nsnippet_sizes\n: A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}\n\n\n\n\nClass BatchIndexer\n\n\nA batch indexer allows you to automatically batch \ndocument indexeing in pipelines, flushing it every N documents. \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nclient\n,\n \nchunk_size\n=\n1000\n)\n\n\n\n\n\n\nadd_document\n\n\ndef\n \nadd_document\n(\nself\n,\n \ndoc_id\n,\n \nnosave\n=\nFalse\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n,\n \nreplace\n=\nFalse\n,\n \n**\nfields\n)\n\n\n\n\n\n\nAdd a document to the batch query\n\n\ncommit\n\n\ndef\n \ncommit\n(\nself\n)\n\n\n\n\n\n\nManually commit and flush the batch indexing query\n\n\nClass Document\n\n\nRepresents a single document in a result set \n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nid\n,\n \npayload\n=\nNone\n,\n \n**\nfields\n)\n\n\n\n\n\n\nsnippetize\n\n\ndef\n \nsnippetize\n(\nself\n,\n \nfield\n,\n \nsize\n=\n500\n,\n \nbold_tokens\n=\n())\n\n\n\n\n\n\nCreate a shortened snippet from the document's content \n:param size: the szie of the snippet in characters. It might be a bit longer or shorter\n:param boldTokens: a list of tokens we want to make bold (basically the query terms)\n\n\nClass GeoField\n\n\nGeoField is used to define a geo-indexing field in a schema defintion\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass GeoFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nlon\n,\n \nlat\n,\n \nradius\n,\n \nunit\n=\nkm\n)\n\n\n\n\n\n\nClass NumericField\n\n\nNumericField is used to define a numeric field in a schema defintion\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nsortable\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)\n\n\n\n\n\n\nClass NumericFilter\n\n\nNone\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nfield\n,\n \nminval\n,\n \nmaxval\n,\n \nminExclusive\n=\nFalse\n,\n \nmaxExclusive\n=\nFalse\n)\n\n\n\n\n\n\nClass Query\n\n\nQuery is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.\n\n\nThe setter functions return the query object, so they can be chained, \ni.e. \nQuery(\"foo\").verbatim().filter(...)\n etc.\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nquery_string\n)\n\n\n\n\n\n\nCreate a new query object. \n\n\nThe query string is set in the constructor, and other options have setter functions.\n\n\nadd_filter\n\n\ndef\n \nadd_filter\n(\nself\n,\n \nflt\n)\n\n\n\n\n\n\nAdd a numeric or geo filter to the query. \n\nCurrently only one of each filter is supported by the engine\n\n\n\n\nflt\n: A NumericFilter or GeoFilter object, used on a corresponding field\n\n\n\n\nget_args\n\n\ndef\n \nget_args\n(\nself\n)\n\n\n\n\n\n\nFormat the redis arguments for this query and return them\n\n\nin_order\n\n\ndef\n \nin_order\n(\nself\n)\n\n\n\n\n\n\nMatch only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'\n\n\nlimit_fields\n\n\ndef\n \nlimit_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nLimit the search to specific TEXT fields only\n\n\n\n\nfields\n: A list of strings, case sensitive field names from the defined schema\n\n\n\n\nlimit_ids\n\n\ndef\n \nlimit_ids\n(\nself\n,\n \n*\nids\n)\n\n\n\n\n\n\nLimit the results to a specific set of pre-known document ids of any length\n\n\nno_content\n\n\ndef\n \nno_content\n(\nself\n)\n\n\n\n\n\n\nSet the query to only return ids and not the document content\n\n\nno_stopwords\n\n\ndef\n \nno_stopwords\n(\nself\n)\n\n\n\n\n\n\nPrevent the query from being filtered for stopwords. \nOnly useful in very big queries that you are certain contain no stopwords.\n\n\npaging\n\n\ndef\n \npaging\n(\nself\n,\n \noffset\n,\n \nnum\n)\n\n\n\n\n\n\nSet the paging for the query (defaults to 0..10).\n\n\n\n\noffset\n: Paging offset for the results. Defaults to 0\n\n\nnum\n: How many results do we want\n\n\n\n\nquery_string\n\n\ndef\n \nquery_string\n(\nself\n)\n\n\n\n\n\n\nReturn the query string of this query only\n\n\nreturn_fields\n\n\ndef\n \nreturn_fields\n(\nself\n,\n \n*\nfields\n)\n\n\n\n\n\n\nOnly return values from these fields\n\n\nslop\n\n\ndef\n \nslop\n(\nself\n,\n \nslop\n)\n\n\n\n\n\n\nAllow a masimum of N intervening non matched terms between phrase terms (0 means exact phrase)\n\n\nverbatim\n\n\ndef\n \nverbatim\n(\nself\n)\n\n\n\n\n\n\nSet the query to be verbatim, i.e. use no query expansion or stemming\n\n\nwith_payloads\n\n\ndef\n \nwith_payloads\n(\nself\n)\n\n\n\n\n\n\nAsk the engine to return document payloads\n\n\nClass Result\n\n\nRepresents the result of a search query, and has an array of Document objects\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nres\n,\n \nhascontent\n,\n \nquery_text\n,\n \nduration\n=\n0\n,\n \nsnippets\n=\nNone\n,\n \nhas_payload\n=\nFalse\n)\n\n\n\n\n\n\n\n\nsnippets\n: An optional dictionary of the form {field: snippet_size} for snippet formatting\n\n\n\n\nClass Suggestion\n\n\nRepresents a single suggestion being sent or returned from the auto complete server\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nstring\n,\n \nscore\n=\n1.0\n,\n \npayload\n=\nNone\n)\n\n\n\n\n\n\nClass TextField\n\n\nTextField is used to define a text field in a schema definition\n\n\n__init__\n\n\ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nweight\n=\n1.0\n,\n \nsortable\n=\nFalse\n)\n\n\n\n\n\n\nredis_args\n\n\ndef\n \nredis_args\n(\nself\n)", 
            "title": "Python API"
        }, 
        {
            "location": "/python_client/#package-redisearch-documentation", 
            "text": "", 
            "title": "Package redisearch Documentation"
        }, 
        {
            "location": "/python_client/#overview", 
            "text": "redisearch-py  is a python search engine library that utilizes the RediSearch Redis Module API.  It is the \"official\" client of redisearch, and should be regarded as its canonical client implementation.  The source code can be found at  http://github.com/RedisLabs/redisearch-py", 
            "title": "Overview"
        }, 
        {
            "location": "/python_client/#example-using-the-python-client", 
            "text": "from   redisearch   import   Client ,   TextField ,   NumericField  # Creating a client with a given index name  client   =   Client ( myIndex )  # Creating the index definition and schema  client . create_index ([ TextField ( title ,   weight = 5.0 ),   TextField ( body )])  # Indexing a document  client . add_document ( doc1 ,   title   =   RediSearch ,   body   =   Redisearch impements a search engine on top of redis )  # Simple search  res   =   client . search ( search engine )  # the result has the total number of results, and a list of documents  print   res . total   #  1  print   res . docs [ 0 ] . title   # Searching with snippets  res   =   client . search ( search engine ,   snippet_sizes   =   { body :   50 })  # Searching with complext parameters:  q   =   Query ( search engine ) . verbatim () . no_content () . paging ( 0 , 5 )  res   =   client . search ( q )", 
            "title": "Example: Using the Python Client"
        }, 
        {
            "location": "/python_client/#example-using-the-auto-completer-client", 
            "text": "# Using the auto-completer  ac   =   AutoCompleter ( ac )  # Adding some terms  ac . add_suggestions ( Suggestion ( foo ,   5.0 ),   Suggestion ( bar ,   1.0 ))  # Getting suggestions  suggs   =   ac . get_suggestions ( goo )   # returns nothing  suggs   =   ac . get_suggestions ( goo ,   fuzzy   =   True )   # returns [ foo ]", 
            "title": "Example: Using the Auto Completer Client:"
        }, 
        {
            "location": "/python_client/#installing", 
            "text": "Install redis 4.0 RC2 or above    Install RediSearch    Install the python client    $ pip install redisearch", 
            "title": "Installing"
        }, 
        {
            "location": "/python_client/#class-autocompleter", 
            "text": "A client to RediSearch's AutoCompleter API  It provides prefix searches with optionally fuzzy matching of prefixes", 
            "title": "Class AutoCompleter"
        }, 
        {
            "location": "/python_client/#9595init9595", 
            "text": "def   __init__ ( self ,   key ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new AutoCompleter client for the given key, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95suggestions", 
            "text": "def   add_suggestions ( self ,   * suggestions ,   ** kwargs )   Add suggestion terms to the AutoCompleter engine. Each suggestion has a score and string.  If kwargs['increment'] is true and the terms are already in the server's dictionary, we increment their scores", 
            "title": "add_suggestions"
        }, 
        {
            "location": "/python_client/#delete", 
            "text": "def   delete ( self ,   string )   Delete a string from the AutoCompleter index.\nReturns 1 if the string was found and deleted, 0 otherwise", 
            "title": "delete"
        }, 
        {
            "location": "/python_client/#get95suggestions", 
            "text": "def   get_suggestions ( self ,   prefix ,   fuzzy = False ,   num = 10 ,   with_scores = False ,   with_payloads = False )   Get a list of suggestions from the AutoCompleter, for a given prefix", 
            "title": "get_suggestions"
        }, 
        {
            "location": "/python_client/#parameters", 
            "text": "prefix : the prefix we are searching.  Must be valid ascii or utf-8  fuzzy : If set to true, the prefix search is done in fuzzy mode. \n     NOTE : Running fuzzy searches on short ( 3 letters) prefixes can be very slow, and even scan the entire index.  with_scores : if set to true, we also return the (refactored) score of each suggestion. \n  This is normally not needed, and is NOT the original score inserted into the index  with_payloads : Return suggestion payloads  num : The maximum number of results we return. Note that we might return less. The algorithm trims irrelevant suggestions.   Returns a list of Suggestion objects. If with_scores was False, the score of all suggestions is 1.", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#len", 
            "text": "def   len ( self )   Return the number of entries in the AutoCompleter index", 
            "title": "len"
        }, 
        {
            "location": "/python_client/#class-client", 
            "text": "A client for the RediSearch module. \nIt abstracts the API of the module and lets you just use the engine", 
            "title": "Class Client"
        }, 
        {
            "location": "/python_client/#9595init9595_1", 
            "text": "def   __init__ ( self ,   index_name ,   host = localhost ,   port = 6379 ,   conn = None )   Create a new Client for the given index_name, and optional host and port  If conn is not None, we employ an already existing redis connection", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   ** fields )   Add a single document to the index.", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#parameters_1", 
            "text": "doc_id : the id of the saved document.  nosave : if set to true, we just index the document, and don't save a copy of it. This means that searches will just return ids.  score : the document ranking, between 0.0 and 1.0   payload : optional inner-index payload we can save for fast access in scoring functions  replace : if True, and the document already is in the index, we perform an update and reindex the document  fields  kwargs dictionary of the document fields to be saved and/or indexed. \n             NOTE: Geo points shoule be encoded as strings of \"lon,lat\"", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#batch95indexer", 
            "text": "def   batch_indexer ( self ,   chunk_size = 100 )   Create a new batch indexer from the client with a given chunk size", 
            "title": "batch_indexer"
        }, 
        {
            "location": "/python_client/#create95index", 
            "text": "def   create_index ( self ,   fields ,   no_term_offsets = False ,   no_field_flags = False ,   no_score_indexes = False ,   stopwords = None )   Create the search index. Creating an existing index juts updates its properties", 
            "title": "create_index"
        }, 
        {
            "location": "/python_client/#parameters_2", 
            "text": "fields : a list of TextField or NumericField objects  no_term_offsets : If true, we will not save term offsets in the index  no_field_flags : If true, we will not save field flags that allow searching in specific fields  no_score_indexes : If true, we will not save optimized top score indexes for single word queries  stopwords : If not None, we create the index with this custom stopword list. The list can be empty", 
            "title": "Parameters:"
        }, 
        {
            "location": "/python_client/#delete95document", 
            "text": "def   delete_document ( self ,   doc_id ,   conn = None )   Delete a document from index\nReturns 1 if the document was deleted, 0 if not", 
            "title": "delete_document"
        }, 
        {
            "location": "/python_client/#drop95index", 
            "text": "def   drop_index ( self )   Drop the index if it exists", 
            "title": "drop_index"
        }, 
        {
            "location": "/python_client/#info", 
            "text": "def   info ( self )   Get info an stats about the the current index, including the number of documents, memory consumption, etc", 
            "title": "info"
        }, 
        {
            "location": "/python_client/#load95document", 
            "text": "def   load_document ( self ,   id )   Load a single document by id", 
            "title": "load_document"
        }, 
        {
            "location": "/python_client/#search", 
            "text": "def   search ( self ,   query ,   snippet_sizes = None )   Search the index for a given query, and return a result of documents", 
            "title": "search"
        }, 
        {
            "location": "/python_client/#parameters_3", 
            "text": "query : the search query. Either a text for simple queries with default parameters, or a Query object for complex queries.\n             See RediSearch's documentation on query format  snippet_sizes : A dictionary of {field: snippet_size} used to trim and format the result. e.g.e {'body': 500}", 
            "title": "Parameters"
        }, 
        {
            "location": "/python_client/#class-batchindexer", 
            "text": "A batch indexer allows you to automatically batch \ndocument indexeing in pipelines, flushing it every N documents.", 
            "title": "Class BatchIndexer"
        }, 
        {
            "location": "/python_client/#9595init9595_2", 
            "text": "def   __init__ ( self ,   client ,   chunk_size = 1000 )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95document_1", 
            "text": "def   add_document ( self ,   doc_id ,   nosave = False ,   score = 1.0 ,   payload = None ,   replace = False ,   ** fields )   Add a document to the batch query", 
            "title": "add_document"
        }, 
        {
            "location": "/python_client/#commit", 
            "text": "def   commit ( self )   Manually commit and flush the batch indexing query", 
            "title": "commit"
        }, 
        {
            "location": "/python_client/#class-document", 
            "text": "Represents a single document in a result set", 
            "title": "Class Document"
        }, 
        {
            "location": "/python_client/#9595init9595_3", 
            "text": "def   __init__ ( self ,   id ,   payload = None ,   ** fields )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#snippetize", 
            "text": "def   snippetize ( self ,   field ,   size = 500 ,   bold_tokens = ())   Create a shortened snippet from the document's content \n:param size: the szie of the snippet in characters. It might be a bit longer or shorter\n:param boldTokens: a list of tokens we want to make bold (basically the query terms)", 
            "title": "snippetize"
        }, 
        {
            "location": "/python_client/#class-geofield", 
            "text": "GeoField is used to define a geo-indexing field in a schema defintion", 
            "title": "Class GeoField"
        }, 
        {
            "location": "/python_client/#9595init9595_4", 
            "text": "def   __init__ ( self ,   name )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class-geofilter", 
            "text": "None", 
            "title": "Class GeoFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_5", 
            "text": "def   __init__ ( self ,   field ,   lon ,   lat ,   radius ,   unit = km )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class-numericfield", 
            "text": "NumericField is used to define a numeric field in a schema defintion", 
            "title": "Class NumericField"
        }, 
        {
            "location": "/python_client/#9595init9595_6", 
            "text": "def   __init__ ( self ,   name ,   sortable = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_1", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/python_client/#class-numericfilter", 
            "text": "None", 
            "title": "Class NumericFilter"
        }, 
        {
            "location": "/python_client/#9595init9595_7", 
            "text": "def   __init__ ( self ,   field ,   minval ,   maxval ,   minExclusive = False ,   maxExclusive = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class-query", 
            "text": "Query is used to build complex queries that have more parameters than just the query string.\nThe query string is set in the constructor, and other options have setter functions.  The setter functions return the query object, so they can be chained, \ni.e.  Query(\"foo\").verbatim().filter(...)  etc.", 
            "title": "Class Query"
        }, 
        {
            "location": "/python_client/#9595init9595_8", 
            "text": "def   __init__ ( self ,   query_string )   Create a new query object.   The query string is set in the constructor, and other options have setter functions.", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#add95filter", 
            "text": "def   add_filter ( self ,   flt )   Add a numeric or geo filter to the query.  Currently only one of each filter is supported by the engine   flt : A NumericFilter or GeoFilter object, used on a corresponding field", 
            "title": "add_filter"
        }, 
        {
            "location": "/python_client/#get95args", 
            "text": "def   get_args ( self )   Format the redis arguments for this query and return them", 
            "title": "get_args"
        }, 
        {
            "location": "/python_client/#in95order", 
            "text": "def   in_order ( self )   Match only documents where the query terms appear in the same order in the document.\ni.e. for the query 'hello world', we do not match 'world hello'", 
            "title": "in_order"
        }, 
        {
            "location": "/python_client/#limit95fields", 
            "text": "def   limit_fields ( self ,   * fields )   Limit the search to specific TEXT fields only   fields : A list of strings, case sensitive field names from the defined schema", 
            "title": "limit_fields"
        }, 
        {
            "location": "/python_client/#limit95ids", 
            "text": "def   limit_ids ( self ,   * ids )   Limit the results to a specific set of pre-known document ids of any length", 
            "title": "limit_ids"
        }, 
        {
            "location": "/python_client/#no95content", 
            "text": "def   no_content ( self )   Set the query to only return ids and not the document content", 
            "title": "no_content"
        }, 
        {
            "location": "/python_client/#no95stopwords", 
            "text": "def   no_stopwords ( self )   Prevent the query from being filtered for stopwords. \nOnly useful in very big queries that you are certain contain no stopwords.", 
            "title": "no_stopwords"
        }, 
        {
            "location": "/python_client/#paging", 
            "text": "def   paging ( self ,   offset ,   num )   Set the paging for the query (defaults to 0..10).   offset : Paging offset for the results. Defaults to 0  num : How many results do we want", 
            "title": "paging"
        }, 
        {
            "location": "/python_client/#query95string", 
            "text": "def   query_string ( self )   Return the query string of this query only", 
            "title": "query_string"
        }, 
        {
            "location": "/python_client/#return95fields", 
            "text": "def   return_fields ( self ,   * fields )   Only return values from these fields", 
            "title": "return_fields"
        }, 
        {
            "location": "/python_client/#slop", 
            "text": "def   slop ( self ,   slop )   Allow a masimum of N intervening non matched terms between phrase terms (0 means exact phrase)", 
            "title": "slop"
        }, 
        {
            "location": "/python_client/#verbatim", 
            "text": "def   verbatim ( self )   Set the query to be verbatim, i.e. use no query expansion or stemming", 
            "title": "verbatim"
        }, 
        {
            "location": "/python_client/#with95payloads", 
            "text": "def   with_payloads ( self )   Ask the engine to return document payloads", 
            "title": "with_payloads"
        }, 
        {
            "location": "/python_client/#class-result", 
            "text": "Represents the result of a search query, and has an array of Document objects", 
            "title": "Class Result"
        }, 
        {
            "location": "/python_client/#9595init9595_9", 
            "text": "def   __init__ ( self ,   res ,   hascontent ,   query_text ,   duration = 0 ,   snippets = None ,   has_payload = False )    snippets : An optional dictionary of the form {field: snippet_size} for snippet formatting", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class-suggestion", 
            "text": "Represents a single suggestion being sent or returned from the auto complete server", 
            "title": "Class Suggestion"
        }, 
        {
            "location": "/python_client/#9595init9595_10", 
            "text": "def   __init__ ( self ,   string ,   score = 1.0 ,   payload = None )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#class-textfield", 
            "text": "TextField is used to define a text field in a schema definition", 
            "title": "Class TextField"
        }, 
        {
            "location": "/python_client/#9595init9595_11", 
            "text": "def   __init__ ( self ,   name ,   weight = 1.0 ,   sortable = False )", 
            "title": "__init__"
        }, 
        {
            "location": "/python_client/#redis95args_2", 
            "text": "def   redis_args ( self )", 
            "title": "redis_args"
        }, 
        {
            "location": "/java_client/", 
            "text": "JRediSearch - RediSearch Java Client\n\n\nhttps://github.com/RedisLabs/JRediSearch\n\n\nOverview\n\n\nJRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis. \n\n\nSee full documentation at \nhttps://github.com/RedisLabs/JRediSearch\n.\n\n\nUsage example\n\n\nInitializing the client:\n\n\nimport\n \nio.redisearch.client.Client\n;\n\n\nimport\n \nio.redisearch.Document\n;\n\n\nimport\n \nio.redisearch.SearchResult\n;\n\n\nimport\n \nio.redisearch.Query\n;\n\n\nimport\n \nio.redisearch.Schema\n;\n\n\n\n...\n\n\n\nClient\n \nclient\n \n=\n \nnew\n \nClient\n(\ntestung\n,\n \nlocalhost\n,\n \n6379\n);\n\n\n\n\n\n\nDefining a schema for an index and creating it:\n\n\nSchema\n \nsc\n \n=\n \nnew\n \nSchema\n()\n\n                \n.\naddTextField\n(\ntitle\n,\n \n5.0\n)\n\n                \n.\naddTextField\n(\nbody\n,\n \n1.0\n)\n\n                \n.\naddNumericField\n(\nprice\n);\n\n\n\nclient\n.\ncreateIndex\n(\nsc\n,\n \nClient\n.\nIndexOptions\n.\nDefault\n());\n\n\n\n\n\n\nAdding documents to the index:\n\n\nMap\nString\n,\n \nObject\n \nfields\n \n=\n \nnew\n \nHashMap\n();\n\n\nfields\n.\nput\n(\ntitle\n,\n \nhello world\n);\n\n\nfields\n.\nput\n(\nbody\n,\n \nlorem ipsum\n);\n\n\nfields\n.\nput\n(\nprice\n,\n \n1337\n);\n\n\n\nclient\n.\naddDocument\n(\ndoc1\n,\n \nfields\n);\n\n\n\n\n\n\nSearching the index:\n\n\n// Creating a complex query\n\n\nQuery\n \nq\n \n=\n \nnew\n \nQuery\n(\nhello world\n)\n\n                    \n.\naddFilter\n(\nnew\n \nQuery\n.\nNumericFilter\n(\nprice\n,\n \n0\n,\n \n1000\n))\n\n                    \n.\nlimit\n(\n0\n,\n5\n);\n\n\n\n// actual search\n\n\nSearchResult\n \nres\n \n=\n \nclient\n.\nsearch\n(\nq\n);", 
            "title": "Java API"
        }, 
        {
            "location": "/java_client/#jredisearch-redisearch-java-client", 
            "text": "https://github.com/RedisLabs/JRediSearch", 
            "title": "JRediSearch - RediSearch Java Client"
        }, 
        {
            "location": "/java_client/#overview", 
            "text": "JRediSearch is a Java library abstracting the API of the RediSearch Redis module, that implements a powerful in-memory search engine inside Redis.   See full documentation at  https://github.com/RedisLabs/JRediSearch .", 
            "title": "Overview"
        }, 
        {
            "location": "/java_client/#usage-example", 
            "text": "Initializing the client:  import   io.redisearch.client.Client ;  import   io.redisearch.Document ;  import   io.redisearch.SearchResult ;  import   io.redisearch.Query ;  import   io.redisearch.Schema ;  ...  Client   client   =   new   Client ( testung ,   localhost ,   6379 );   Defining a schema for an index and creating it:  Schema   sc   =   new   Schema () \n                 . addTextField ( title ,   5.0 ) \n                 . addTextField ( body ,   1.0 ) \n                 . addNumericField ( price );  client . createIndex ( sc ,   Client . IndexOptions . Default ());   Adding documents to the index:  Map String ,   Object   fields   =   new   HashMap ();  fields . put ( title ,   hello world );  fields . put ( body ,   lorem ipsum );  fields . put ( price ,   1337 );  client . addDocument ( doc1 ,   fields );   Searching the index:  // Creating a complex query  Query   q   =   new   Query ( hello world ) \n                     . addFilter ( new   Query . NumericFilter ( price ,   0 ,   1000 )) \n                     . limit ( 0 , 5 );  // actual search  SearchResult   res   =   client . search ( q );", 
            "title": "Usage example"
        }, 
        {
            "location": "/payloads/", 
            "text": "Document Payloads\n\n\nUsually, redisearch stores documents as HASH keys. But if you want to access some data for \naggregation or scoring functions, we might want to store that data as an inline payload. \nThis will allow us to evaluate properties of a document for scoring purposes at very low cost.\n\n\nSince the scoring functions already have access to the DocumentMetaData, which contains document flags and score,\nWe can add custom payloads that can be evaluated in run-time.\n\n\nPayloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose \nof evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, \nif you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.\n\n\nAdding payloads for documents:\n\n\nWhen inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload.\nThis is done with the PAYLOAD keyword:\n\n\nFT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...\n\n\n\n\n\nEvaluating Payloads in Query Time\n\n\nWhen imlplementing a scoring function, the signature of the function exposed is:\n\n\ndouble\n \n(\n*\nScoringFunction\n)(\nDocumentMetadata\n \n*\ndmd\n,\n \nIndexResult\n \n*\nh\n);\n\n\n\n\n\n\n\n\nNOTE: currently scoring functions cannot be dynamically added, and forking the engine and replacing them is required.\n\n\n\n\nDocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with\narbitrary length:\n\n\ntypedef\n \nstruct\n  \n{\n\n    \nchar\n \n*\ndata\n,\n\n    \nuint32_t\n \nlen\n;\n\n\n}\n \nDocumentPayload\n;\n\n\n\n\n\n\nIf no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it.\nIt is recommended to encode some metadata about the payload inside it, like a leading version number, etc.\n\n\nRetrieving payloads from documents\n\n\nWhen searching, it is possible to request the document payloads from the engine. \n\n\nThis is done by adding the keyword \nWITHPAYLOADS\n to \nFT.SEARCH\n. \n\n\nIf \nWITHPAYLOADS\n is set, the payloads follow the document id in the returned result. \nIf \nWITHSCORES\n is set as well, the payloads follow the scores. e.g.:\n\n\n127.0.0.1:6379\n FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379\n FT.ADD foo doc2 1.0 PAYLOAD \nhi there!\n FIELDS bar \nhello\n\nOK\n127.0.0.1:6379\n FT.SEARCH foo \nhello\n WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2) \ndoc2\n           # id\n3) \n1\n              # score\n4) \nhi there!\n      # payload\n5) 1) \nbar\n         # fields\n   2) \nhello", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#document-payloads", 
            "text": "Usually, redisearch stores documents as HASH keys. But if you want to access some data for \naggregation or scoring functions, we might want to store that data as an inline payload. \nThis will allow us to evaluate properties of a document for scoring purposes at very low cost.  Since the scoring functions already have access to the DocumentMetaData, which contains document flags and score,\nWe can add custom payloads that can be evaluated in run-time.  Payloads are NOT indexed and are not treated by the engine in any way. They are simply there for the purpose \nof evaluating them in query time, and optionally retrieving them. They can be JSON objects, strings, or preferably, \nif you are interested in fast evaluation, some sort of binary encoded data which is fast to decode.", 
            "title": "Document Payloads"
        }, 
        {
            "location": "/payloads/#adding-payloads-for-documents", 
            "text": "When inserting a document using FT.ADD, you can ask RediSearch to store an arbitrary binary safe string as the document payload.\nThis is done with the PAYLOAD keyword:  FT.ADD {index_name} {doc_id} {score} PAYLOAD {payload} FIELDS {field} {data}...", 
            "title": "Adding payloads for documents:"
        }, 
        {
            "location": "/payloads/#evaluating-payloads-in-query-time", 
            "text": "When imlplementing a scoring function, the signature of the function exposed is:  double   ( * ScoringFunction )( DocumentMetadata   * dmd ,   IndexResult   * h );    NOTE: currently scoring functions cannot be dynamically added, and forking the engine and replacing them is required.   DocumentMetaData includes a few fields, one of them being the payload. It wraps a simple byte array with\narbitrary length:  typedef   struct    { \n     char   * data , \n     uint32_t   len ;  }   DocumentPayload ;   If no payload was set to the document, it is simply NULL. If it is not, you can go ahead and decode it.\nIt is recommended to encode some metadata about the payload inside it, like a leading version number, etc.", 
            "title": "Evaluating Payloads in Query Time"
        }, 
        {
            "location": "/payloads/#retrieving-payloads-from-documents", 
            "text": "When searching, it is possible to request the document payloads from the engine.   This is done by adding the keyword  WITHPAYLOADS  to  FT.SEARCH .   If  WITHPAYLOADS  is set, the payloads follow the document id in the returned result. \nIf  WITHSCORES  is set as well, the payloads follow the scores. e.g.:  127.0.0.1:6379  FT.CREATE foo SCHEMA bar TEXT\nOK\n127.0.0.1:6379  FT.ADD foo doc2 1.0 PAYLOAD  hi there!  FIELDS bar  hello \nOK\n127.0.0.1:6379  FT.SEARCH foo  hello  WITHPAYLOADS WITHSCORES\n1) (integer) 1\n2)  doc2            # id\n3)  1               # score\n4)  hi there!       # payload\n5) 1)  bar          # fields\n   2)  hello", 
            "title": "Retrieving payloads from documents"
        }, 
        {
            "location": "/design/gc/", 
            "text": "Garbage Collection in RediSearch\n\n\n1. The Need For GC\n\n\n\n\nDeleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.\n\n\nThis means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.\n\n\nThus all inverted index entries belonging to this document id are just garbage. \n\n\nWe do not want to go and explicitly delete them when deleting a document because it will make this operation very long an depending on the length of the document.\n\n\nOn top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.\n\n\n\n\nAll of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory. \n\n\nThus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.\n\n\n2. Garbage Collecting a Single Term Index\n\n\nA single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage. \n\n\nThe algorithm is pretty simple: \n\n\n\n\nCreate a reader and writer for each block\n\n\nRead each block's records one by one\n\n\nIf no record is invalid, do nothing\n\n\nOnce we found a garbage record, we advance the reader but not the writer.\n\n\nOnce we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.\n\n\n\n\nPseudo code:\n\n\nforeach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer\ns tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length\n\n\n\n\n\nNOTE\n: Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.\n\n\n2.1 Garbage Collection on Numeric Indexes\n\n\nNumeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.\n\n\n3. GC And Concurrency\n\n\nSince RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us). \n\n\nIt GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.\n\n\nThis means, however, that we need to consider a few things:\n\n\n\n\n\n\nFrom the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.\n\n\n\n\n\n\nFrom the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.\n\n\n\n\n\n\nTo solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:\n\n Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed. \n\n Before starting an index iterator, we copy the index's gc marker to the iterator's context.\n\n After waking up from sleep in the iterator, we check the gc markers in both objects.\n\n If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower. \n\n\nTo solve 2 is simpler: \n\n The GC will of course operate only while the GIL is locked.\n\n The GC will never yield execution while in the middle of a block.\n\n The GC will check whether the key has been delted while it slept.\n\n The GC will get a new pointer to the next block on each read, assuring the pointer is safe.\n\n\n4. Scheduling Garbage Collection\n\n\nWhile the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively. \n\n\nSo the GC will use sampling of random terms and collect them. \n\n\nThis leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage. \n\n\nSolving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.\n\n\nThus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.\n\n\nSolving 1 can be done in the following way:\n\n\nWe start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.\n\n\nThe frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "Garbage Collection"
        }, 
        {
            "location": "/design/gc/#garbage-collection-in-redisearch", 
            "text": "", 
            "title": "Garbage Collection in RediSearch"
        }, 
        {
            "location": "/design/gc/#1-the-need-for-gc", 
            "text": "Deleting documents is not really deleting them. It marks the document as deleted in the global document table, to make it fast.  This means that basically an internal numeric id is no longer assigned to a document. When we traverse the index we check for deletion.  Thus all inverted index entries belonging to this document id are just garbage.   We do not want to go and explicitly delete them when deleting a document because it will make this operation very long an depending on the length of the document.  On top of that, updating a document is basically deleting it, and then adding it again with a new incremental internal id. We do not do any diffing, and only append to the indexes, so the ids remain incremental, and the updates fast.   All of the above means that if we have a lot of updates and deletes, a large portion of our inverted index will become garbage - both slowing things down and consuming unnecessary memory.   Thus we want to optimize the index. But we also do not want to disturb the normal operation. This means that optimization or garbage collection should be a background process, that is non intrusive. It only needs to be faster than the deletion rate over a long enough period of time so that we don't create more garbage than we can collect.", 
            "title": "1. The Need For GC"
        }, 
        {
            "location": "/design/gc/#2-garbage-collecting-a-single-term-index", 
            "text": "A single term inverted index is consisted of an array of \"blocks\" each containing an encoded list of records - document id delta plus other data depending on the index encoding scheme. When some of these records refer to deleted documents, this is called garbage.   The algorithm is pretty simple:    Create a reader and writer for each block  Read each block's records one by one  If no record is invalid, do nothing  Once we found a garbage record, we advance the reader but not the writer.  Once we found at least one garbage record, we encode the next records to the writer, recalculating the deltas.   Pseudo code:  foreach index_block as block:\n\n   reader = new_reader(block)\n   writer = new_write(block)\n   garbage = 0\n   while not reader.end():\n        record = reader.decode_next()\n        if record.is_valid():\n            if garbage != 0:\n                # Write the record at the writer s tip with a newly calculated delta\n                writer.write_record(record)\n            else:\n                writer.advance(record.length)\n        else:\n            garbage += record.length  NOTE : Currently the algorithm does not delete empty blocks and does not merge small ones, this is a future improvement that shouldn't be very hard to do.", 
            "title": "2. Garbage Collecting a Single Term Index"
        }, 
        {
            "location": "/design/gc/#21-garbage-collection-on-numeric-indexes", 
            "text": "Numeric indexes are now a tree of inverted indexes with a special encoding of (docId delta,value). This means the same algorithm can be applied to them, only traversing each inverted index object in the tree.", 
            "title": "2.1 Garbage Collection on Numeric Indexes"
        }, 
        {
            "location": "/design/gc/#3-gc-and-concurrency", 
            "text": "Since RediSearch 0.17, we are using a multi threaded concurrent query execution model. This means that index iterators yield execution to other threads every given time (currently configured at 50us).   It GC should also take advantage of this ability, and perform its task from a side thread, blocking the global redis lock for a short periods of time, incrementally processing indexes without interrupting execution and indexing.  This means, however, that we need to consider a few things:    From the Index Iterator's POV - a GC sweep might be performed while we are sleeping during iterating the index. This means that the offset of the reader in its current block might not be correct. In that case we need to do a slow search in the index for the last id we've read. This is an operation we don't want to do unless we have to.    From the GC thread's POC - an index might be written to or deleted while we are sleeping during a sweep.    To solve 1 we need to detect this in the reader, and adapt to this situation. Detection of a GC sweep while sleeping is simple:  Each inverted index key has a \"gc marker\" variable that increments each time it has been GC'ed.   Before starting an index iterator, we copy the index's gc marker to the iterator's context.  After waking up from sleep in the iterator, we check the gc markers in both objects.  If they are the same we can simply trust the byte offset of the reader in the current block.\n* IF not, we seek the reader to the previously read docId, which is slower.   To solve 2 is simpler:   The GC will of course operate only while the GIL is locked.  The GC will never yield execution while in the middle of a block.  The GC will check whether the key has been delted while it slept.  The GC will get a new pointer to the next block on each read, assuring the pointer is safe.", 
            "title": "3. GC And Concurrency"
        }, 
        {
            "location": "/design/gc/#4-scheduling-garbage-collection", 
            "text": "While the  GC process will run on a side thread and not interfere with normal operations, we do want to avoid running it when not necessary. \nThe problem is that we do not know, when a document has been deleted, which terms now hold garbage, becuase for that we need a forward index or to re-tokenize the document, which are expensive in RAM and CPU respectively.   So the GC will use sampling of random terms and collect them.   This leaves two problems:\n1. How to avoid GC when it's not needed (the user is not deleting any documents or doing it very little).\n2. How to make sure we hit terms that are more likely to contain garbage.   Solving 2 for now will be done by trying some sort of weighted random. We already have a dictionary with the frequency of terms in the index. However running a real weighted random on it is an expensive operation.  Thus, an approximation of a weighted random can be easily done by blindly selecting N terms in each round, and then applying a weighted random function to this small set of keys. This assures that even the least frequent terms will get visited.  Solving 1 can be done in the following way:  We start with a frequency F for the random sampler of the garbage collector. At first it is relatively infrequent or even 0. It can be configured.\nThen, we do the following:\n    * Each time a document is deleted or updated we increase the frequency a bit.\n    * Each time we find a key with garbage we increase the frequency a bit.\n    * Each time we sample a key with NO garbage found, we decrease the frequency a bit. This can be related to the frequency of the key, which can be seen as a hint to the probability of finding garbage in it. So if we don't find garbage in the most frequent term, it is a very strong indicator that the index contains little to no garbage.  The frequency is of course bounded to a maximum which we will never surpass. Thus when a lot of garbage is created, the frequency will be at its maximum, and will eventually decay to 0 or a pre-configured minimum - until more documents will be deleted.", 
            "title": "4. Scheduling Garbage Collection"
        }, 
        {
            "location": "/Threading/", 
            "text": "Multi-Threading in RediSearch\n\n\nBy Dvir Volk, July 2017\n\n\n1. One Thread To Rule Them All\n\n\nRedis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with. \n\n\nWhile keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like \nZUNIONSTORE\n, \nLRANGE\n, \nSINTER\n and of course the infamous \nKEYS\n, can block Redis for seconds or minutes, depending on the size of data they are handling. \n\n\n2. RediSearch and the Single Thread Issue\n\n\nRediSearch\n is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine. \n\n\nWhile it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked. \n\n\nThink, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond, \nwhich is impossible with current hardware\n. The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.\n\n\nSo taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.\n\n\n3. Enter the Redis GIL\n\n\nLuckily, Redis BDFL \nSalvatore Sanfilippo\n has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API - \nThread Safe Contexts\n and the \nGlobal Lock\n.\n\n\nThe idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the \nGlobal Lock\n when it needs to access Redis data, operate on it, and release it. \n\n\nWe still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.\n\n\n4. Making Search Concurrent\n\n\nUp until now, the flow of a search query was simple - the query would arrive at a \nCommand Handler\n callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result. \n\n\nTo allow concurrency, we adapted the following design:\n\n\n\n\n\n\nRediSearch has a thread pool for running concurrent search queries. \n\n\n\n\n\n\nWhen a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.\n\n\n\n\n\n\nThe thread pool runs a query processing function in its own thread.\n\n\n\n\n\n\nThe function locks the Redis Global lock, and starts executing the query.\n\n\n\n\n\n\nSince the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).\n\n\n\n\n\n\nIf enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.\n\n\n\n\n\n\nWhen the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state. \n\n\n\n\n\n\nThus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently. \n\n\n\n\nFigure 1: Serial vs. Concurrent Search\n\n\n\n\nOn the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.\n\n\n\n\nThe same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.\n\n\nAs a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.\n\n\n5. The Effect of Concurrency\n\n\nWhile this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running \nKEYS *\n in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!\n\n\nThere is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\". \n\n\nThis is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation. \n\n\nTo enable safe mode and disable query concurrency, you can configure RediSearch at load time: \nredis-server --loadmodule redisearch.so SAFEMODE\n in command line, or by adding \nloadmodule redisearch.so SAFEMODE\n to your redis.conf - depending on how you load the module.\n\n\n6. Some Numbers!\n\n\nI've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.\n\n\n\n\nBenchmark Setup\n\n\n\n\nThe data-set consists of about 1,000,000 Reddit comments.\n\n\nTwo clients using Redis-benchmark were running  - first separately, then in parallel:\n\n\nOne client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.\n\n\nOne client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).\n\n\nBoth clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.\n\n\n\n\n\n\nThe Results:\n\n\n\n\n\n\n\n\nNote\n\n\nWhile we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries. \n\n\n\n\n7. Parting Words\n\n\nThis little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.\n\n\nFor RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#multi-threading-in-redisearch", 
            "text": "By Dvir Volk, July 2017", 
            "title": "Multi-Threading in RediSearch"
        }, 
        {
            "location": "/Threading/#1-one-thread-to-rule-them-all", 
            "text": "Redis has been, from its inception, single threaded - and will remain so at least in 4.0. I'm not going to get into the reasons of why that is - but up until now it has been a reality that Redis apps, and more recently Redis Modules such as RediSearch - have to deal with.   While keeping things single-threaded makes Redis simple and fast - the down-side is that long running commands block the entire server for the duration of the query's execution. Most Redis commands are fast so that is not a problem, but commands like  ZUNIONSTORE ,  LRANGE ,  SINTER  and of course the infamous  KEYS , can block Redis for seconds or minutes, depending on the size of data they are handling.", 
            "title": "1. One Thread To Rule Them All"
        }, 
        {
            "location": "/Threading/#2-redisearch-and-the-single-thread-issue", 
            "text": "RediSearch  is a new search engine module written at Redis Labs. It leverages Redis' powerful infrastructure with efficient data structures, to create a fast and feature rich, real-time search engine.   While it is extremely fast and uses highly optimized data structures and algorithms, it was facing the same problem with regards to concurrency: Depending on the size of your data-set and the cardinality of search queries, they can take internally anywhere between a few microseconds, to hundreds of milliseconds to seconds in extreme cases. And when that happens - the entire Redis server that the engine is running on - is blocked.   Think, for example, on the a full-text query intersecting the terms \"hello\" and \"world\", each with, let's say, a million entries, and half a million common intersection points. To do that in a millisecond, you would have to scan, intersect and rank each result in one nanosecond,  which is impossible with current hardware . The same goes for indexing a 1000 word document. It blocks Redis entirely for that duration.  So taking into account that in the real world, search queries may not behave like your average Redis O(1) command, and block the entire server for long periods of time. Of course, you could and should split your search index into a cluster, and a cluster version of RediSearch will soon be available as part of Redis Labs Enterprise cluster - but even if we distribute the data across cluster nodes, some queries will be slow.", 
            "title": "2. RediSearch and the Single Thread Issue"
        }, 
        {
            "location": "/Threading/#3-enter-the-redis-gil", 
            "text": "Luckily, Redis BDFL  Salvatore Sanfilippo  has added a revolutionary change just near the finish line of Redis 4.0 and the release of the modules API -  Thread Safe Contexts  and the  Global Lock .  The idea is simple - while Redis in itself still remains single threaded, a module can run many threads - and any one of them can acquire the  Global Lock  when it needs to access Redis data, operate on it, and release it.   We still cannot really query Redis in parallel - only one thread can acquire the lock, including the Redis main thread - but we can make sure that a long running query will give other queries time to properly run by yielding this lock from time to time. Note that this limitation applies to our use case only - in other use cases such as training machine learning models, actual parallel processing the background is achievable and easy.", 
            "title": "3. Enter the Redis GIL"
        }, 
        {
            "location": "/Threading/#4-making-search-concurrent", 
            "text": "Up until now, the flow of a search query was simple - the query would arrive at a  Command Handler  callback in the Redis Module, and it would be the only thing running inside Redis right now. Then it would parse the query, execute it, taking as long as it takes - and return the result.   To allow concurrency, we adapted the following design:    RediSearch has a thread pool for running concurrent search queries.     When a search request arrives, it gets to the handler, gets parsed on the main thread, and a request object is passed to the thread pool via a queue.    The thread pool runs a query processing function in its own thread.    The function locks the Redis Global lock, and starts executing the query.    Since the search execution is basically an iterator running in a cycle, we simply sample the elapsed time every several iterations (sampling on each iteration would slow things down as it has a cost of its own).    If enough time has elapsed, the query processor releases the Global Lock, and immediately tries to acquire it again. When the lock is released, the kernel will schedule another thread to run - be it Redis' main thread, or another query thread.    When the lock is acquired again - we reopen all Redis resources we were holding before releasing the lock (keys might have been deleted while the thread has been \"sleeping\"), and continue work from the previous state.     Thus the operating system's scheduler makes sure all query threads get CPU time to run. While one is running the rest wait idly, but since execution is yielded about 5,000 times a second, it creates the effect of concurrency. Fast queries will finish in one go without yielding execution, slow ones will take many iteration to finish, but will allow other queries to run concurrently.", 
            "title": "4. Making Search Concurrent"
        }, 
        {
            "location": "/Threading/#figure-1-serial-vs-concurrent-search", 
            "text": "On the left-hand side, all queries are handled one after the other. On the right side, each query is given it time-slice to run. Notice that while the total time for all queries remain the same, queries 3 and 4 finish much faster.   The same approach is applied to indexing. If a document is big and tokenizing and indexing it will block Redis for a long time - we break that into many smaller iterations and allow Redis to do other things instead of blocking for a very long time. In fact, in the case of indexing there is enough work to be done in parallel using multiple cores - namely tokenizing and normalizing the document. This is especially effective for very big documents.  As a side note - this could have been implemented with a single thread switching between all the query execution loops, but the code refactoring required for that was much larger, and the effect with reasonable load would have remained similar, so we opted to keep this for a future release.", 
            "title": "Figure 1: Serial vs. Concurrent Search"
        }, 
        {
            "location": "/Threading/#5-the-effect-of-concurrency", 
            "text": "While this is not magic, and if all your queries are slow they will remain slow, and no real parallel processing is done here - this is revolutionary in Redis terms. Think about the old problem of running  KEYS *  in a busy Redis instance. In single threaded operation, this will cause the instance to hang for seconds if not minutes. No it is possible to implement a concurrent version of KEYS in a module, that will hardly affect performance. In fact, Salvatore has already implemented one!  There is, however, a negative effect as well: we sacrifice atomicity of reads and writes for concurrency to a degree. Consider the following situation: One thread is processing a query that should retrieve document A, then yields the execution context; At the same time, another thread deletes or changes document A. The result - the query run by the first thread will not be able to retrieve the document, as it has already been changed or deleted while the thread was \"asleep\".   This is of course only relevant to high update/delete loads, and relatively slow and complex queries. In our view, for most use cases this is a sacrifice worth making, and usually query processing is fast enough that the probability of this happening is very low. However, this can be overcome easily: if strong atomicity of the operations is important, it is possible to have RediSearch operate in \"safe mode\", making all searches and updates atomic, thus making sure that each query refers to the sate of the index at the moment of its invocation.   To enable safe mode and disable query concurrency, you can configure RediSearch at load time:  redis-server --loadmodule redisearch.so SAFEMODE  in command line, or by adding  loadmodule redisearch.so SAFEMODE  to your redis.conf - depending on how you load the module.", 
            "title": "5. The Effect of Concurrency"
        }, 
        {
            "location": "/Threading/#6-some-numbers", 
            "text": "I've benchmarked both versions of the module - simple single threaded, and concurrent multi threaded, over the same set up.   Benchmark Setup   The data-set consists of about 1,000,000 Reddit comments.  Two clients using Redis-benchmark were running  - first separately, then in parallel:  One client doing a very intensive query - \"i\" which has 200,000 results with 5 concurrent connections.  One client is doing a very light query - \"Obama\", which has about 500 results - with 10 concurrent connections (we assume in a normal situation there will be more lightweight queries than heavy queries).  Both clients and the server running on my personal laptop - MacBook Pro with an Intel Quad Core i7 @ 2.2Ghz.", 
            "title": "6. Some Numbers!"
        }, 
        {
            "location": "/Threading/#the-results", 
            "text": "Note  While we can see that light queries are significantly slower when running in concurrent mode without contention, they are still very fast. But in contention, we see that lightweight queries run X40 faster in concurrent mode, since they are not blocked by the slow queries, as in single thread mode. In single thread mode we are only as fast as the slowest queries.", 
            "title": "The Results:"
        }, 
        {
            "location": "/Threading/#7-parting-words", 
            "text": "This little Global Lock feature and Thread Safe Contexts, is perhaps the most powerful thing that the Modules API offers. We touched only the problem of concurrency here, but it also enables background tasks, real parallel processing of data that does not touch the Redis keyspace, and more.  For RediSearch, it makes the difference between being a nice engine for small-ish use cases, to being a real beast that can handle huge data-sets at high loads. Combined with the up-and-coming distributed version of RediSearch (that also leverages the threading API, but that's a story for another post), it will make RediSearch a very powerful search and indexing engine.", 
            "title": "7. Parting Words"
        }
    ]
}